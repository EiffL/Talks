<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>A Brief Review of ML in WL</title>

	<meta name="description" content="KICP Workshop, August 2nd 2023">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>A Brief Review of Deep Learning Applications for Weak Lensing</h1>
						<h3> Lensing at different scales KICP Workshop, August 2nd 2023</h3>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>Fran√ßois Lanusse</h2>
								<br>
								<img src="/talks/assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="/talks/assets/logo_cnrs.png" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="/talks/assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a href="https://eiffl.github.io/talks/Chicago2023">eiffl.github.io/talks/Chicago2023</a> </div>
				</div>
			</section>

		<section>
			<section>
				<h3 class="slide-title">A few motivating examples</h3>

					<div class="r-stack">
						<div class="fragment current-visible" data-fragment-index="0">
							<img data-src="/talks/assets/hsc_shredded.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="0" style="float:right; font-size: 20px">Bosch et al. 2017</div>
						</div>

						<div class="fragment current-visible" data-fragment-index="1">
							<img data-src="/talks/assets/deepmass_sims_clean.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">Jeffrey, <b>Lanusse</b>, et al. 2020</div>
						</div>

						<div class="fragment" data-fragment-index="2">
							<img data-src="/talks/assets/scattering_ps.png" style="height:300px;"/>
							<img class="fragment"  data-src="/talks/assets/ScatteringTransform.png" style="height:400px;"/> <br>
							<div class="fragment" data-fragment-index="2" style="float:right; font-size: 20px">Cheng et al. 2020</div>
						</div>
					</div>
					<ul>
						<li class="fragment" data-fragment-index="0">Galaxies are no longer blobs.</li>
						<li class="fragment" data-fragment-index="1">Signals are no longer Gaussian.</li>
						<li class="fragment" data-fragment-index="2">Cosmological likelihoods are no longer tractable.</li>
					</ul>
					<br>
					<br>
					<div class="fragment">$\Longrightarrow$ This is the <b class="alert">end of the analytic era</b>...</div>
			</section>			

			<section>
				<h3 class="slide-title">... but the <b class="alert">beginning of the data-driven era</b></h3>
					<br>
					 <div class="container">
						 <div class="col fragment">
								 <b>Case I</b>: Examples from data, no accurate physical model<br>
								 <img data-src="/talks/assets/real_gal-inv-small.png" style="height:400px;"/><br>
									<div style="float:right; font-size: 20px">Mandelbaum et al. 2014</div>
									<br>
						 </div>

						 <div class="col fragment">
							 <b>Case II</b>: Physical model only available as a simulator<br>
							 <img data-src='/talks/assets/convergence.png' style="height:400px;"/><br>
								 <div style="float:right; font-size: 20px">Osato et al. 2020</div>
								 <br>
						 </div>
					 </div>
					 <br>
					 <div class="fragment">$\Longrightarrow$ Examples of <b class="alert">implicit distributions</b>: we have access to samples $\{x_0, x_1, \ldots, x_n \}$
						 but <b>we cannot evaluate $p(x)$</b>.
					 </div>
			</section>
		</section>


		<section class="inverted" data-background="#000">
			<h2>How can we leverage implicit distributions <br> for Bayesian inference?</h2>
		</section>

				  <section>
					  <section>
						  <h3 class="slide-title"> The answer is: Deep Generative Modeling</h3>
						  <br>
						  <ul>
							  <li>The goal of generative modeling is to <b>learn an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
								  from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
							  </li>
							  <br>
							  <li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
								  that tries to be close to $\mathbb{P}$.
							  </li>
						  </ul>

						  <br>
						  <div class="container">
							  <div class="col fragment fade-up">
								  <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
								  <br>
								  True $\mathbb{P}$
							  </div>

							  <div class="col  fragment fade-up">
								  <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
								  <br>
								  Samples $x_i \sim \mathbb{P}$
							  </div>

							  <div class="col  fragment fade-up">
								  <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
								  <br>
								  Model $\mathbb{P}_\theta$
							  </div>
						  </div>
						  <br>
						  <br>
						  <ul>
							  <li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
							  </li>
						  </ul>

					  </section>

					  <section>
						  <h3 class="slide-title">Why isn't it easy?</h3>
						  <br>
						  <ul>
							  <li> The <b class="alert">curse of dimensionality</b> put all points far apart in high dimension
							  </li>
						  </ul>
						  <div class="container">
							  <div class="col fragment fade-up">
								  <img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756597/pasted-from-clipboard.png" class="plain"></img>
							  </div>

							  <div class="col fragment fade-up">
								  <img style="height:350px;" data-src="https://developers.google.com/machine-learning/clustering/images/CurseofDimensionality.svg" class="plain"></img>
								  <br>Distance between pairs of points drawn from a Gaussian distribution.
							  </div>
						  </div>

						  <br>
						  <ul>
							  <li class="fragment"><b>Classical methods</b> for estimating probability densities, i.e. Kernel Density Estimation (KDE) start to <b>fail in high dimension</b> because of all the gaps
							  </li>
						  </ul>
					  </section>
					  <section>
						<h3 class="slide-title"> The evolution of generative models </h3>
		
						<br> 
						<div class='container'>
							<div class='col'>
								<div style="position:relative; width:500px; height:600px; margin:0 auto;">
									<img class="fragment current-visible plain" data-src="/talks/assets/DBN.png"
										style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="0" />
									<img class="fragment current-visible plain" data-src="/talks/assets/vae_faces.jpg"
										style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
									<img class="fragment current-visible plain" data-src="/talks/assets/gan-samples-1.png"
										style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="2" />
									<img class="fragment plain" data-src="/talks/assets/karras2017.png"
										style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
									<img class="fragment plain" data-src="https://preview.redd.it/de6cf3omoqpa1.jpg?width=960&format=pjpg&auto=webp&v=enabled&s=0f4641ba72ad4c7ef106efafc539804cf7247410"
									style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="4" />
								</div>
							</div>
		
							<div class='col'>
								<ul>
									<li class="fragment" data-fragment-index="0"> Deep Belief Network <br> (Hinton et al. 2006)
									</li>
									<br>
									<li class="fragment" data-fragment-index="1"> Variational AutoEncoder <br> (Kingma & Welling
										2014) </li>
									<br>
									<li class="fragment" data-fragment-index="2"> Generative Adversarial Network <br>
										(Goodfellow et al. 2014)</li>
									<br>
									<li class="fragment" data-fragment-index="3"> Wasserstein GAN <br> (Arjovsky et al. 2017)
									</li>
									<br>
									<li class="fragment" data-fragment-index="4"> Midjourney v5 Guided Diffusion (2023)
									</li>
								</ul>
							</div>
						</div>
						<br> <br> <br>
					</section>
		
				  </section>


			<section>
				<h3 class="slide-title">Structure of this talk</h3>

				<br>			
				<q>Deep Learning does not create new information, but <br>
					allows you to manipulate existing knowledge to answer new questions.</q>
				<br>
				<br>
				<br>
				<br>
				<br>
				<div class="fragment">
				<b>Let's go from pixels to cosmology</b>

				<br>
				<br>

				<ul>
					<li class="fragment grow">ML for image processing and shape measurement
					</li>
					<br>

					<li class="fragment grow">ML for convergence map reconstruction
					</li>

					<br>
					<li class="fragment grow">ML for cosmological inference
					</li>
				</ul>
				</div>
				<br>
				<br>
				<br>
				<br>
				<br>
				<br>
			</section>

			<section>
				<h1>Image Processing and Shape Measurement</h1>
				<hr>
				<h3><b>Disclaimer</b>: Shear is difficult, none of this has been deployed on real data yet.</h3>
			</section>


			<section>
				<section>
				  <h3 class="slide-title"> The weak lensing shape measurement problem</h3>
				   <div>
								   <img class="plain" data-src="/talks/assets/great.jpg"  style="width:1100px"/>
								   </div>
				   <div class="block fragment" >
				   <div class="block-title">
					 Shape measurement biases
				   </div>
				   <div class="block-content">
					 The  <b class="alert">measured ellipticity $e$</b> is typically a biased tracer of the <b class="alert">underlying shear $\gamma$</b >
					 $$ < e >  = \ (1 + m) \ \gamma \ + \ c $$
				 </div>
				 </div>
				</section>
   
				<section>
				  <h3 class="slide-title"> Simulation and calibration strategy</h3>
				  <br>
				  <img class="plain" data-src="/talks/assets/simu.png"/>
				  <br>
				  <br>
				  <div class="block">
				  <div class="block-title">
				   The GREAT3 approach
				  </div>
				  <div class="block-content">
					<ul>
					  <li> Input galaxies from deep HST/ACS COSMOS images (25.2 imag)
					  </li>
					  <br>
					  <li> Apply a range of PSFs and noise levels sampled from the survey
					  </li>
					  <br>
					  <li>  Measure response of shape measurement to a known shear and estimate $m$ and $c$
					  </li>
					</ul>
				</div>
				</div>
				</section>
   
				<section>
				  <h3 class="slide-title">The Bayesian hierarchical modeling strategy</h3>
   
				  <div class='container'>
					<div class='col'>
   
				  <div class='container'>
					<div class='col'>
					  <img data-src="/talks/assets/forward_G3-crop_img21.png" class="plain" style="height:200px; image-rendering: pixelated; image-rendering: -moz-crisp-edges; image-rendering: crisp-edges;">
					  <br>
					  $\mathbf{x}$
					</div>
					<div class='col'>
					  <img data-src="/talks/assets/shear-crop-lr.png" class="plain" style="height:200px; image-rendering: pixelated; image-rendering: -moz-crisp-edges; image-rendering: crisp-edges;">
					  <br>
					  $\mathbf{\gamma}$
					</div>
				  </div>
				  <br>
					  <br>
					  <ul>
					   <li>The root of the problem is that the likelihood $p(x | \gamma)$ is a
						 complicated beast.
					   </li>
					   <br>
					   <li class="fragment fade-up" data-fragment-index="0">Provides an explicit description of
						pixel level data, in terms of simple, tractable distributions
					   </li>
					  </ul>
   
					</div>
					<div class='col fragment fade-up' data-fragment-index="0">
					  <img data-src="/talks/assets/pgm_lensing.png" class="plain" style="height:550px;"/>
   
					  <div style="float:right; font-size: 20px">(Schneider et al. 2015)</div>
					</div>
				  </div>
				</section>
			  </section>
   
   
   <!--
			   <section>
   
   
   
   
				<section>
   
				  <h3 class="slide-title"> From simple to complex simulations... </h3>
				  <br>
   
			   <div class='container'>
						  <div class='col'>
			  <img class="plain" data-src="/talks/assets/HSC_sims.png"/>
				</div>
   
			   <div class='col'>
				  <img class="plain fragment fade-left" data-src="/talks/assets/HSC_sims_diff.png"/>
				</div>
			  </div>
				</section> -->
   
			 <section>
			   <h3 class="slide-title"> Impact of galaxy morphology</h3>
   
				 <br>
				 In both cases, we are building a <b class="alert">forward model</b> of the data, how
				 accurate does this model need to be?
				 <br>
				 <br>
						  <div class='container'>
							  <div class='col fragment fade-up'>
					 <img class="plain " data-src="/talks/assets/real_gal-inv.png" style="height: 350px;"/>
					 <br>
					 <div style="float:left; font-size: 20px">Mandelbaum, et al. (2013), Mandelbaum, et al. (2014)</div>
				   </div>
   
				   <div class='col'>
					 <img class="plain fragment fade-up" data-src="/talks/assets/great3_calib2-inv.png" style="height: 425px;"/>
				   </div>
				 </div>
				 <br>
   
				 <div class="fragment fade-up">
				   $\Longrightarrow$ We cannot measure shear without an <b>accurate model of galaxy morphology</b>
				 </div>
			 </section>
   
			   <section class="inverted" data-background="#000">
				   <h2>
				   Can we learn a model for galaxy morphologies from the data itself?</h2>
			   </section>

			  <section>
				<section>
				   <h3 class="slide-title" style="position:absolute;top:0;">A Physicist's approach: let's build a model</h3>
				   
				<div class="container">
					<div class="col">
						<div style="float: right; font-size: 20px">
							<b>Lanusse</b> et al. (2020)
							<a href="https://arxiv.org/abs/2008.03833"
								><img
									src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg"
									class="plain"
									style="height: 25px; vertical-align: middle"
							/></a>
						</div>
					</div>
				</div> 
				   <br>
				   <br>
				   <div class="container">
					 <div class="col">
						 <img class="plain fragment" data-src="/talks/assets/rand_z_square.png" style="height: 150px" data-fragment-index="4"/>
					 </div>
					 <div class="col">
						 <img class="plain fragment" data-src="/talks/assets/cosmos_gal.png" style="width: 200px" data-fragment-index="3"/>
					 </div>
					 <div class="col">
					   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_psf.png" style="width: 200px" data-fragment-index="2"/>
					 </div>
		
					 <div class="col">
					   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_pix.png" style="width: 200px" data-fragment-index="1"/>
					 </div>
		
					 <div class="col">
					   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_ground.png" style="width: 200px" data-fragment-index="0"/>
					 </div>
				   </div>
		
				 <div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
				   <div class='col fragment' data-fragment-index='4'> <font size="10"> $\longrightarrow$ </font> <br> $g_\theta$ </div>
				   <div class='col fragment' data-fragment-index='3'> <font size="10"> $\longrightarrow$ </font> <br> PSF </div>
				   <div class='col fragment' data-fragment-index='2'> <font size="10"> $\longrightarrow$ </font> <br> Pixelation</div>
				   <div class='col fragment' data-fragment-index='1'> <font size="10"> $\longrightarrow$ </font> <br> Noise </div>
				 </div>
		
				 <div class="container">
					 <div class="col">
					   <div style="position:relative; width:400px; height:300px; margin:0 auto;">
					   <img data-src="/talks/assets/pgm_0.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="0"/>
					   <img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="1"/>
					   <img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="2"/>
					   <img data-src="/talks/assets/pgm_2.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="3"/>
					   <img data-src="/talks/assets/pgm_3.png" class="plain fragment " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="4"/>
					   </div>
					 </div>
					 <div class=" col">
					   <div class="block fragment" data-fragment-index="0">
					   <div class="block-title">
						Probabilistic model
					   </div>
					   <div class="block-content">
					   <div style="position:relative; width:400px; height:100px; margin:0 auto;">
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="0"> $$ x \sim ? $$ </div>
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="1"> $$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br>latent $z$ is a denoised galaxy image</div>
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="2"> $$ x \sim \mathcal{N}( \mathbf{P} z, \Sigma) \quad z \sim ?$$<br>latent $z$ is a super-resolved and denoised galaxy image</div>
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="3"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast z), \Sigma) \quad z \sim ? $$<br>latent $z$ is a deconvolved, super-resolved, and denoised galaxy image </div>
						 <div class="plain fragment " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="4"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast g_\theta(z)), \Sigma) \quad z \sim \mathcal{N}(0, \mathbf{I}) $$ <br>latent $z$ is a Gaussian sample<br> <b class="alert"> $\theta$ are parameters of the model</b> </div>
					   </div>
					   <br>
					   <br>
					   <br>
					 </div>
					 </div>
					 </div>
				 </div>
				 <div class="fragment"> $\Longrightarrow$ <b class="alert"> Decouples the morphology model from the observing conditions</b>.</div>
				</section>
		
		<section>
		<h3 class="slide-title">How to train your <s>dragon</s> model</h3>
		<div class="container">
		   <div class="col">
			 <img data-src="/talks/assets/pgm.png" class="plain" style="height: 300px;" ></img>
		   </div>
		   <div class="col">
			 <ul>
			   <li> Training the generative amounts to finding $\theta_\star$ that
				 <b>maximizes the marginal likelihood</b> of the model:
				   $$p_\theta(x | \Sigma, \Pi) = \int \mathcal{N}( \Pi \ast g_\theta(z), \Sigma) \ p(z) \ dz$$
				   <div> $\Longrightarrow$ This is <b class="alert">generally intractable</b></div>
			   </li>
			   <br>
			   <li class="fragment fade-up"> Efficient training of parameter $\theta$ is made possible by <b class="alert">Amortized Variational Inference</b>.
			   </li>
			 </ul>
		   </div>
		</div>
		
		<div class="block fragment fade-up">
		<div class="block-title">
		Auto-Encoding Variational Bayes (Kingma & Welling, 2014)
		</div>
		<div class="block-content">
		 <ul>
		   <li class=" fade-up"> We introduce a <b>parametric distribution</b> $q_\phi(z | x, \Pi, \Sigma)$ which aims to model the
		   posterior $p_{\theta}(z | x, \Pi, \Sigma)$.
		   </li>
		   <br>
		   <li class=" fade-up"> Working out the KL divergence between these two distributions leads to:
		
			 $$\log p_\theta(x | \Sigma, \Pi) \quad \geq \quad - \mathbb{D}_{KL}\left( q_\phi(z | x, \Sigma, \Pi) \parallel p(z) \right) \quad + \quad \mathbb{E}_{z \sim q_{\phi}(. | x, \Sigma, \Pi)} \left[ \log p_\theta(x | z, \Sigma, \Pi)  \right]$$
		
			 $\Longrightarrow$ This is the <b>Evidence Lower-Bound</b>, which is differentiable with respect to $\theta$ and $\phi$.
		   </li>
		 </ul>
		</div>
		</div>
		
		</section>
		<section>
		<h3 class="slide-title">The famous Variational Auto-Encoder</h3>
		<img data-src="/talks/assets/vae.png" class="plain" style="height: 450px;"> </img>
		<br>
		<br>
		$$\log p_\theta(x| \Sigma, \Pi ) \geq - \underbrace{\mathbb{D}_{KL}\left( q_\phi(z | x, \Sigma, \Pi) \parallel p(z) \right)}_{\mbox{code regularization}} + \underbrace{\mathbb{E}_{z \sim q_{\phi}(. | x, \Sigma, \Pi)} \left[ \log p_\theta(x | z, \Sigma, \Pi)  \right]}_{\mbox{reconstruction error}} $$
		</section>
		
		   <section>
					 <h3 class="slide-title"> Sampling from the model</h3>
			   <div class="container">
			   <div class="col fragment fade-up">
				 <img data-src="/talks/assets/vae_samples_bad.png" class="plain" ></img>
				 Woups... what's going on?
			   </div>
			   <div class="col">
				 <img data-src="/talks/assets/latent_space.png" class="plain fragment fade-up" ></img>
			   </div>
			 </div>
		</section>
		
		   <section>
			   <h3 class="slide-title"> Tradeoff between code regularization and image quality</h3>
		
		 <br>
		 $$\log p_\theta(x| \Sigma, \Pi ) \geq - \underbrace{\mathbb{D}_{KL}\left( q_\phi(z | x, \Sigma, \Pi) \parallel p(z) \right)}_{\mbox{code regularization}} + \underbrace{\mathbb{E}_{z \sim q_{\phi}(. | x, \Sigma, \Pi)} \left[ \log p_\theta(x | z, \Sigma, \Pi)  \right]}_{\mbox{reconstruction error}} $$
		
		 <img data-src="/talks/assets/sdss_ae_kl.png" class="plain" ></img>
		
		</section>
		
		<section data-background-image=https://media.giphy.com/media/3o85xIO33l7RlmLR4I/source.gif>
		</section>
		
		   <section>
			   <h3 class="slide-title"> Latent space modeling with Normalizing Flows</h3>
		 <br>
		 $\Longrightarrow$ All we need to do is <b class="alert">sample from the aggregate posterior</b> of the data instead of sampling from the prior.
		
		<br>
		<br>
		
		<div class="container">
		<div class="col">
		 <img data-src="/talks/assets/flow_dinh_1.png" class="plain fragment fade-up" data-fragment-index="1"></img>
		 <img data-src="/talks/assets/flow_dinh_2.png" class="plain fragment fade-up" data-fragment-index="3"></img>
		
		 <br>
				<div class="fragment fade-up" style="float:right; font-size: 20px" data-fragment-index="1">Dinh et al. 2016</div>
		</div>
		<div class="col">
				 <div class="block fragment fade-up" data-fragment-index="1">
				 <div class="block-title">
				  Normalizing Flows
				 </div>
				 <div class="block-content">
				   <ul>
					 <li> Assumes a <b class="alert">bijective</b> mapping between
					   data space $x$ and latent space $z$ with prior $p(z)$:
					   $$ z = f_{\theta} ( x ) \qquad \mbox{and} \qquad x = f^{-1}_{\theta}(z)$$
					 </li>
					 <li class="fragment" data-fragment-index="2"> Admits an explicit marginal likelihood:
					   $$ \log p_\theta(x) = \log p(z) + \log \left| \frac{\partial f_\theta}{\partial x}  \right|(x)    $$
					 </li>
				   </ul>
			   </div>
			   </div>
			   <br>
				 <br>
				 <br>
				 <br>
		</div>
		</div>
		
		</section>
		</section>
		
		
		
		<section>
			<h3 class="slide-title"> Flow-VAE samples</h3>
		<br>
		<br>
		<img class="current-visible plain" data-src="/talks/assets/lanusse2020_figure1.png"/>
		</section>
		
		
		<section>
			<!-- <section>
				<h3 class="slide-title">Bayesian modeling of cosmic shear</h3>
				<div align="left">
				We aim to model the posterior distribution $p(\gamma|\mathcal{D})$ <br><br>
		
				<div class="fragment">
				$\begin{align}
				p(\gamma|\mathcal{D}) &= \int p(\gamma, z, \Pi|\mathcal{D}) ~dz~d\Pi \\
				\end{align}$
			</div>
			<div class="fragment">
				$\begin{align}
				~~~~~~~~~~~&= \int \color{orange}{\underbrace{p(\mathcal{D}|\gamma, z, \Pi)}_{\text{likelihood}}} \underbrace{p(\gamma)p(z)p(\Pi)}_{\text{priors}} ~dz~d\Pi
				\end{align}$
			</div>
				<br>
				
				<div class="fragment">
				The likelihood $\color{orange}{p(\mathcal{D}|\gamma, z, \Pi)}$ is naturally built from the forward model <br>
					<div align="center">
				<img class="plain" style="height:300px" data-src="/talks/assets/great.jpg" />
			</div>
			</div>
			</div>
		
			</section> -->
		
			<section>
				<h3 class="slide-title">
					Let's again think as physicists and extend the model to shear
				</h3>
				<!-- <div class="container">
					<div class="col">
						<div style="float: right; font-size: 20px">
							<b>Lanusse</b> et al. (2020)
							<a href="https://arxiv.org/abs/2008.03833"
								><img
									src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg"
									class="plain"
									style="height: 25px; vertical-align: middle"
							/></a>
						</div>
					</div>
				</div> -->
				<div class="container">
					<div class="col">
						<img
							class="plain "
							data-src="/talks/assets/rand_z_square.png"
							style="height: 150px"
							data-fragment-index="4"
						/>
					</div>
					<div class="col">
						<img
							class="plain "
							data-src="/talks/assets/cosmos_gal_shear.png"
							style="width: 200px"
							data-fragment-index="3"
						/>
					</div>
					<div class="col">
						<img
							class="plain "
							data-src="/talks/assets/cosmos_gal.png"
							style="width: 200px"
							data-fragment-index="2"
						/>
					</div>
		
					<div class="col">
						<img
							class="plain "
							data-src="/talks/assets/cosmos_gal_psf.png"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     osmos_gal_psf.png"
							style="width: 200px"
							data-fragment-index="1"
						/>
					</div>
		
					<div class="col">
						<img
							class="plain "
							data-src="/talks/assets/cosmos_gal_ground.png"
							style="width: 200px"
							data-fragment-index="0"
						/>
					</div>
				</div>
		
				<div
					class="container"
					style="
						position: relative;
						width: 1000px;
						height: 50px;
						margin: 0 auto;
					"
				>
					<div class="col " data-fragment-index="4">
						<font size="10"> $\longrightarrow$ </font> <br />
						$g_\theta$
					</div>
					<div class="col " data-fragment-index="3">
						<font size="10"> $\longrightarrow$ </font> <br />
						<b class="alert">shear $\gamma$</b>
					</div>
					<div class="col " data-fragment-index="2">
						<font size="10"> $\longrightarrow$ </font> <br />
						PSF
					</div>
					<div class="col " data-fragment-index="1">
						<font size="10"> $\longrightarrow$ </font> <br />
						Noise
					</div>
				</div>
		
				<div class="container">
					<div class="col">
						<div
							style="
								position: relative;
								width: 400px;
								height: 300px;
								margin: 0 auto;
							"
						>
							<!-- <img
								data-src="/talks/assets/pgm_x.png"
								class="plain  current-visible"
								style="position: absolute; top: 0; left: 0; height: 300px"
								data-fragment-index="0"
							/>
							<img
								data-src="/talks/assets/pgm_xzs.png"
								class="plain  current-visible"
								style="position: absolute; top: 0; left: 0; height: 300px"
								data-fragment-index="1"
							/>
							<img
								data-src="/talks/assets/pgm_xzsp.png"
								class="plain  current-visible"
								style="position: absolute; top: 0; left: 0; height: 350px"
								data-fragment-index="2"
							/>
							<img
								data-src="/talks/assets/pgm_xzsp_shear.png"
								class="plain  current-visible"
								style="position: absolute; top: 0; left: 0; height: 350px"
								data-fragment-index="3"
							/> -->
							<img
								data-src="/talks/assets/pgm_full.png"
								class="plain "
								style="position: absolute; top: 0; left: 0; height: 300px"
								data-fragment-index="4"
							/>
						</div>
					</div>
					<div class="col">
						<div class="block " data-fragment-index="0">
							<div class="block-title">Probabilistic model</div>
							<div class="block-content">
								<div
									style="
										position: relative;
										width: 400px;
										height: 100px;
										margin: 0 auto;
									"
								>
									<!-- <div
										class="plain fragment current-visible"
										style="
											position: absolute;
											top: 0;
											left: 0;
											width: 400px;
										"
										data-fragment-index="0"
									>
										$$ x \sim ? $$
									</div>
									<div
										class="plain fragment current-visible"
										style="
											position: absolute;
											top: 0;
											left: 0;
											width: 400px;
										"
										data-fragment-index="1"
									>
										$$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br />latent
										$z$ is a denoised galaxy image
									</div>
									<div
										class="plain fragment current-visible"
										style="
											position: absolute;
											top: 0;
											left: 0;
											width: 400px;
										"
										data-fragment-index="2"
									>
									$$ x \sim \mathcal{N}(\Pi \ast z, \Sigma)
									\quad z \sim ? $$<br />latent $z$ is a deconvolved,
									and denoised galaxy image
									</div>
									<div
										class="plain fragment current-visible"
										style="
											position: absolute;
											top: 0;
											left: 0;
											width: 450px;
										"
										data-fragment-index="3"
									>
									$$ x \sim \mathcal{N}(\Pi \ast (z \otimes \gamma), \Sigma)
									\quad z \sim ? $$<br />latent $z$ is a unsheared deconvolved,
									and denoised galaxy image
									</div> -->
									<div
										class="plain "
										style="
											position: absolute;
											top: 0;
											left: 0;
											width: 500px;
										"
										data-fragment-index="4"
									>
										$$ x \sim \mathcal{N}(\Pi \ast
										(g_\theta(z) \otimes \gamma), \Sigma) \quad z \sim \mathcal{N}(0,
										\mathbf{I}) $$ <br />latent $z$ are morphological parameters<br />
										
											$\theta$ are global parameters of the model
										<br >
										<b class="alert" >
											$\gamma$ are shear parameters</b>
									</div>
								</div>
								<br />
								<br />
								<br />
							</div>
						</div>
					</div>
				</div>
				<div class="fragment">
					$\Longrightarrow$ We have a hybrid probabilistic model, with the <b>known physics of lensing and of the instrument</b>, and
					<b>learned morphology model</b>.
				</div>
			</section>
		
			<section>
				<h3 class="slide-title">Joint inference using a parametric model for the morphology</h3>
				<div align="left">
				Let's assume that $g(z)$ is a <b class="alert">sersic model</b>, i.e. $z = \{n, r_\text{hlr}, F, e_1, e_2, s_x, s_y\}$ and 
				$$g(z) = F \times I_0 \exp \left( -b_n \left[\left( \frac{r}{r_\text{hlr}}\right)^{\frac{1}{n}} -1\right] \right)$$
				
				<div class="fragment">
					The joint inference of $p(z, \gamma | \mathcal{D})$ leads to a <b class="alert">biased posterior</b>...
				
					<div class="container">
						<div class="col" align="center">
							<img src="/talks/assets/shear_estimate_bias.png" class="plain" height="300"></img>
							<br>
							Marginal shear posterior $p(\gamma|\mathcal{D})$
						</div>
		
						<div class="col fragment">
							<img src="/talks/assets/sersic_fit.png" class="plain" height="300"></img>
							<br>
							Maximum a posteriori fit and residuals
						</div>
					</div>
				</div>
		
				<div class="fragment" align="center"> <br>
					<b class="alert">We need a more realistic model of galaxy morphology</b>
				</div>
			</section>
		
					<section>
						<h3 class="slide-title">Joint inference using a generative model for the morpholgy</h3>
						<div class="container">
							<div class="col">
								<div style="float: right; font-size: 20px">
									Remy, <b>Lanusse</b>, Starck (2022)
									<a href="https://arxiv.org/abs/2210.16243"
										><img
											src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2210.16243-B31B1B.svg"
											class="plain"
											style="height: 25px; vertical-align: middle"
									/></a>
								</div>
							</div>
						</div>
						<div align="left">
						Let's use a learned $g_\theta(z)$ <br><br>
						
						<div class="fragment">
							The joint inference of $p(z, \gamma | \mathcal{D})$ leads to an <b class="alert">unbiased posterior</b>!<br><br>
						
							<div class="container">
								<div class="col" align="center">
									<img src="/talks/assets/shear_estimate_nobias.png" class="plain" height="300"></img>
									<br>
									Marginal shear posterior $p(\gamma|\mathcal{D})$
								</div>
		
								<div class="col fragment">
									<img src="/talks/assets/deepgal1.png" class="plain" height="300"></img>
									<br>
									Maximum a posteriori fit and residuals
								</div>
							</div>
						</div>
					</section>
		</section>
		

		

<!-- 
			<section>
      <section data-background="/talks/assets/gal_hsc.png">
					<h3 class="slide-title">Example of application to deblending</h3>
					<br>
				 	<ul>
						<li><em>Hybrid Physical-Deep Learning Model for Astronomical Inverse Problems</em> <b> F. Lanusse</b>, P. Melchior, F. Moolekamp<br>
							<a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
							<a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
						</li>
					</ul>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
					<br>
      </section>



	  

						<section>
							<h3 class="slide-title"> The Scarlet algorithm: deblending as an optimization problem</h3>
									<div style="float:right; font-size: 20px">Melchior et al. 2018</div>

									$$ \mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i) + \sum_{i=1}^K g_i(A_i) +  \sum_{i=1}^K f_i(S_i)$$

							<div class="container">
							<div class="col">
									<img data-src="/talks/assets/scarlet_data.png" height=450 class="plain"></img>
							</div>

							<div class="col">

								Where for a $K$ component blend:
								<br>
									<ul>
									<li>$P$ is the convolution with the instrumental response</li>
									<br>
									<li>$A_i$ are channel-wise galaxy SEDs, $S_i$ are the morphology models</li>
									<br>
									<li>$\mathbf{\Sigma}$ is the noise covariance</li>
									<br>
									<li>$\log p_\theta$ is a PixelCNN prior</li>
									<br>
									<li>$f_i$ and $g_i$ are arbitrary additional non-smooth consraints, e.g. positivity, monotonicity...</li>
									</ul>
							</div>
						</div>

						<span class="fragment fade-up">$\Longrightarrow$ Explicit physical modeling of the observed sky</span>
						</section>

						 <section>
							<h3  class="slide-title">Training the morphology prior</h3>

							<div class="container">
								<div class="col">
									<img data-src="/talks/assets/cosmos_training.png" height=450 class="plain"></img>
									<div> Postage stamps of isolated COSMOS galaxies used for training, at WFIRST resolution and fixed fiducial PSF</div>
							</div>

							<div class="col">
							<div class="container fragment fade-in">
								<div class="col">
									isolated galaxy
								<img data-src="/talks/assets/gal_1.png" class="plain"></img>
								<span> $\log p_\theta(x) = 3293.7$ </span>
							</div>

								<div class="col">
									artificial blend
								<img data-src="/talks/assets/gal_2.png" class="plain"></img>
								<span> $\log p_\theta(x) = 3100.5 $ </span>
							</div>
								</div>
							</div>
						</section>

						<section>
						<h3 class="slide-title">Scarlet in action</h3>

						<div class="container">
							<div class="col">
								Input blend
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
							<img data-src="/talks/assets/scar_input.png" class="plain"></img>
						</div>
							</div>

						<div class="col">
							<span class="fragment" data-fragment-index="0">Solution</span>
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
									  <img class="fragment current-visible plain" data-src="/talks/assets/old_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
									  <img class="fragment  plain" data-src="/talks/assets/pix_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							</div>
						</div>

						<div class="col">
							<span class="fragment" data-fragment-index="0">Residuals</span>
							<div style="position:relative; width:480px; height:480px; margin:0 auto;">
									  <img class="fragment current-visible plain" data-src="/talks/assets/old_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
									  <img class="fragment  plain" data-src="/talks/assets/pix_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
							</div>
						</div>
						</div>

						<ul>
								<li class="fragment fade-up" data-fragment-index="0">Classic priors (monotonicity, symmetry).</li>
								<br>

								<li class="fragment fade-up" data-fragment-index="1">Deep Morphology prior.</li>
						</ul>

					</section>
					<section>
						<div class="container">
							<div class="col">
								True Galaxy
							<img data-src="/talks/assets/true_input.png" class="plain"></img>
						</div>

						<div class="col">
							Deep Morphology Prior Solution

										<img class=" plain" data-src="/talks/assets/pix_rec2.png"  />

						</div>

						<div class="col">
							Monotonicity + Symmetry Solution
										<img class=" plain" data-src="/talks/assets/scar_rec2.png" />
							</div>
						</div>
					</section>
		</section> -->
<!--
 									<section>
 										<h3 class="slide-title">Example of application to deblending</h3>
 										<br>
 										<div class="container">
 											<div class="col">
 												<ul>
 													<li><em>Hybrid Physical-Deep Learning Model for Astronomical Inverse Problems</em><br> <b> F. Lanusse</b>, P. Melchior, F. Moolekamp<br>
 														<a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
 														<a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
 													</li>
 												</ul>
 												<br> <br>
 												$\mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i)$
 												<br> <br>
 												<img class=" plain" data-src="/talks/assets/scarlet_hsc.png" />
 												<br> <br>
 											</div>

 											<div class="col fragment fade-in">
 												<div class="container">
 													<div class="col">
 														isolated galaxy
 														<img data-src="/talks/assets/gal_1.png" class="plain"></img>
 														<span> $\log p_\theta(x) = 3293.7$ </span>
 													</div>

 													<div class="col">
 														artificial blend
 														<img data-src="/talks/assets/gal_2.png" class="plain"></img>
 														<span> $\log p_\theta(x) = 3100.5 $ </span>
 													</div>
 												</div>
 											</div>
 										</div>
 									</section> -->
<!-- 
									<section>
										<section>
											<h3 class="slide-title">Uncertainty quantification in Magnetic Resonance Imaging (MRI)</h3>
											<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
														src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
											</div>
											<br>
											<br>
											$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
											<div><video data-autoplay loop="loop" data-src="/talks/assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
											</div>
											<br>

											<br>

											<br>

											<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
										</section>
									</section> -->


									<section>
										<h3 class="slide-title">Other examples of Deep Priors</h3>
										<br>
										<div class="container">
										  <div class="col">
											<ul>
											  <li><em>Hybrid Physical-Deep Learning Model for Astronomical Inverse Problems</em><br> <b> F. Lanusse</b>, P. Melchior, F. Moolekamp<br>
												<a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
												<a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
											  </li>
											</ul>
											<br> <br>
											$\mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i)$
											<br> <br>
											<img class=" plain" data-src="/talks/assets/scarlet_hsc.png" style="height: 300px;" />
											<br> <br>
								
										  </div>
										  <!-- <div class="col fragment">
											<ul>
											  <li><em>Denoising Score-Matching for Uncertainty Quantification in Inverse Problems</em><br> Z. Ramzi, B. Remy, <b>F. Lanusse</b>, P. Ciuciu, J.L. Starck<br>
												<a href="https://arxiv.org/abs/2011.08698"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
											  </li>
											</ul>
											<img class="plain" data-src="/talks/assets/knee.gif" style="height:410px;"/>
								
										  </div> -->
										</div>
									  </section>


			<section>
				<h1>Convergence Map Reconstruction</h1>
				<hr>
			</section>

			<section>
				<section>
											<h3 class="slide-title">Weak Lensing Mass-Mapping as an Inverse Problem</h3>
											<div class="container">
												<div class="col">
													Shear <b class="alert">$\gamma$</b><br>
													<img data-src="/talks/assets/shear_cat1.png" style="width:450px;"></img>
												</div>

												<div class="col fragment fade-up">
													Convergence <b class="alert">$\kappa$</b><br>
													<img data-src="/talks/assets/kappa.png" style="width:450px;"></img>
												</div>
											</div>

											<div style="position:relative; width:1000px; height:100px; margin:0 auto;">
												<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
													$$\gamma_1 = \frac{1}{2} (\partial_1^2 - \partial_2^2) \ \Psi \quad;\quad \gamma_2 = \partial_1 \partial_2 \ \Psi \quad;\quad \kappa = \frac{1}{2} (\partial_1^2 + \partial_2^2) \ \Psi$$
												</div>
												<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
													$$\boxed{\gamma = \mathbf{P} \kappa}$$
												</div>
											</div>
										</section>

										<section>
											<h3 class="slide-title"> Illustration on the Dark Energy Survey (DES) Y3</h3>
											<div style="float:right; font-size: 20px">Jeffrey, et al. (2021)
											</div><br>
											<img data-src="/talks/assets/DESY3map.png" style="height:600px;"></img>
										</section> 
									</section> 

			
			<section>
				<h3 class="slide-title">Writing down the convergence map log posterior</h3>

					$$ \log p( \kappa | e) = \underbrace{\log p(e | \kappa)}_{\simeq -\frac{1}{2} \parallel e - P \kappa \parallel_\Sigma^2} + \log p(\kappa) +cst $$

					<ul>
						<li> The likelihood term is <b class="alert">known analytically</b>.
						</li>

						<li class="fragment fade-up"> There is <b class="alert">no close form expression for the full non-Gaussian prior</b> of the convergence.
							<br> However:
							<ul>
								<li class='fragment'> <b>We do have access to samples of full  <b class="alert">implicit</b> prior</b> through simulations: $X = \{x_0, x_1, \ldots, x_n \}$ with $x_i \sim \mathbb{P}$
									<img data-src='/talks/assets/plot_massive_nu.png' />
								</li>
							</ul>
						</li>
					</ul>
					<div class="fragment">$\Longrightarrow$ Our strategy: <b class="alert">Learn the prior from simulation</b>,
						and then <b class="alert">sample the full posterior</b>.</div>
          </section>


		<section class="inverted" data-background="#000">
			<h2> How do you do this in practice in very high dimensional problems?</h2>
		</section>

		<section>
			<section>
			  <h3 class="slide-title">First realization: The score is all you need!</h3>
			  <br>
			  <div class="container">
				  <div class="col">
					  <ul>
						  <li> Whether you are looking for the MAP or sampling with HMC or MALA, you
							  <b class="alert">only need access to the score</b> of the posterior:
							  $$\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x \color{orange}|\color{orange} y\color{orange})}{\color{orange}
							  d
							  \color{orange}x}$$
							  <ul>
								  <li>Gradient descent: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) $</li>
								  <li>Langevin algorithm: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) + \sqrt{2\tau} n_t$ </li>
							  </ul>
						  </li>
						  <br>
					  </ul>
				  </div>
				  <div class="col">
					  <img data-src="/talks/assets/score_two_moons.png"></img>
				  </div>
			  </div>
			  <br>
			  <br>
			  <ul>
				  <li > The score of the full posterior is simply:
					  $$\nabla_x \log p(x |y) = \underbrace{\nabla_x \log p(y |x)}_{\mbox{known explicitly}} \quad + \quad \underbrace{\nabla_x \log p(x)}_{\mbox{known implicitly}}$$
					  $\Longrightarrow$ "all" we have to do is <b class="alert">model/learn the score of the prior</b>.
				  </li>
			  </ul> 
		  </section>
  
		  <section>
			  <h3 class="slide-title">Neural Score Estimation by Denoising Score Matching (Vincent 2011)</h3>
			  <ul>
				  <li><b>Denoising Score Matching</b>: An optimal <b class="alert">Gaussian denoiser learns the score</b> of a given distribution.
					  <ul>
						  <li class="fragment fade-up"> If $x \sim \mathbb{P}$ is corrupted by additional Gaussian noise $u \in \mathcal{N}(0, \sigma^2)$ to yield
							  $$x^\prime = x + u$$
						  </li>
						  <li class="fragment fade-up"> Let's consider a denoiser $r_\theta$ trained under an $\ell_2$ loss:
							  $$\mathcal{L}=\parallel x - r_\theta(x^\prime, \sigma) \parallel_2^2$$
						  </li>
						  <li class="fragment fade-up"> The optimal denoiser $r_{\theta^\star}$ verifies:
							  $$\boxed{\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma) = \boldsymbol{x}' + \sigma^2 \nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}$$
						  </li>
					  </ul>
				  </li>
			  </ul>
  
			  <div class="fragment fade-up">
				  <div class="container">
					  <div class="col">$\boldsymbol{x}'$
					  </div>
					  <div class="col">$\boldsymbol{x}$
					  </div>
					  <div class="col">$\boldsymbol{x}'- \boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
					  </div>
					  <div class="col">$\boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
					  </div>
				  </div>
				  <img data-src="/talks/assets/denoised_mnist.png" style='width:1200px;'></img>
			  </div>
		  </section>
		  </section>
  
		  <section>
			  <h3 class="slide-title">Second Realization: Annealing is everything!</h3>
  
			  <ul>
				  <li> Even with knowledge of the score, <b class="alert">sampling in high number of dimensions is difficult!</b><br>
				  </li>
				  <br>
				  <li class="fragment fade-up"> Convolving a target distribution $p$ with a noise kernel, makes $p_\sigma(x) = \int \mathcal{N}(x; x^\prime, \sigma^2) (x^\prime) d x^{\prime}$ it much better behaved
					  <div>
						  $$\sigma_1 > \sigma_2 > \sigma_3 > \sigma_4 $$
						  <img data-src="/talks/assets/annealing.png" />
					  </div>
				  </li>
  
				  <li class="fragment fade-up"> Hints to running many MCMC chains in parallel, progressively annealing the $\sigma$ to 0, <b class="chain">keep last point in the chain as independent sample</b>.
				  </li>
			  </ul>
		  </section>
  
		  <section>
			  <h3 class="slide-title">Score-Based Generative Modeling <a href="https://arxiv.org/abs/2011.13456"> Song et al. (2021)</a></h3>
			  <img data-src="/talks/assets/diffusion.png" style="height:350px;"/><br>
			  <br>
			  <ul>
				  <li class="fragment"> The SDE defines a <b class="alert">marginal distribution $p_t(x)$</b> as the convolution of the target distribution $p(x)$ with a noise kernel $p_{t|s}(\cdot | x_s)$:
					  $$p_t(x) = \int p(x_s) p_{t|s}(x | x_s) d x_s$$
				  </li>
				  <li class="fragment"> For a given forward SDE that evolves $p(x)$ to $p_T(x)$, there exists a <b>reverse SDE</b> that evolves $p_T(x)$ back into $p(x)$. It involves having access to the <b class="alert">marginal score $\nabla_x \log_t p(x)$</b>.</li>
			  </ul>
		  </section>
  
  
		  <section>  
  
			  <section>
				  <h3 class="slide-title">Example of one chain during annealing</h3>
				  <img data-src="/talks/assets/hmc-annealing.gif"/>
		  </section>
  
		  <section>
			  <h3 class="slide-title">Validating Posterior Sampling under a Gaussian prior</h3>
  
			  <img  style="height:600px;" data-src="/talks/assets/Remy2022Wiener.png"/>
  
		  </section>
  
		  <section>
			<h3 class="slide-title">Some details: We don't actually know the marginal posterior score!</h3>
			<ul>
				<li>We know the following quantities: 
					<ul>
						<li>Annealed likelihood (analytically): $p_\sigma(y | x) = \mathcal{N}(y; \mathbf{A} x, \mathbf{\Sigma} + \sigma^2 \mathbf{I})$</li>
						<li>Annealed prior score (by score matching): $\nabla_x \log p_\sigma(x)$ </li>
					</ul>
				</li>
				<li class="fragment" data-fragment-index="1">But, unfortunately: $\boxed{p_\sigma(x|y) \neq p_\sigma(y|x) \  p_\sigma(x)}$
					$\Longrightarrow$ <b class="alert">We don't know the marginal posterior score!</b>
				</li>
			</ul>
			<div class="r-stack">
				<!-- <img data-src="/talks/assets/post_prod.png" style="height: 500px;"/> -->
				<div class="fragment highlight-red">
					<video data-fragment-index="1" data-src="/talks/assets/post_prod.mp4" style="height: 500px;"></video>
				</div>
			</div>
		</section>

		<section>
			<ul>
				<li>We cannot use the reverse SDE/ODE of diffusion models to sample from the posterior.
					$$\mathrm{d} x = [f(x, t) - g^2(t) \underbrace{\nabla_x \log p_t(x|y)}_{\mbox{unknown}} ] \mathrm{d}t + g(t) \mathrm{d} w$$
				</li>
			</ul> 
			<div class="r-stack">
			<div class="block fragment fade-up current-visible" >
				<div class="block-title">
				 Proposed sampling strategy
				</div>
				<div class="block-content">
				  <ul>
					<li> Even if not equivalent to the marginal posterior score, $\nabla_x \log p_{\sigma^2}(y | x) + \nabla_x \log p_{\sigma^2}(x)$ still 
						has good properties:
						<ul>
							<li>Tends to an isotropic Gaussian distribution for large $\sigma$ </li>
							<li>Corresponds to the target posterior for $\sigma=0$ </li>
						</ul> 
					</li>
					<br>
					<li> If we simulate this SDE sufficiently slowly (i.e. timescale of change of $\sigma$ is much larger than the timescale of the SDE)
						we can expect to sample from the target posterior. 
					</li>
				  </ul>

			  </div>
			  </div>

				<img class="fragment current-visible" data-src="/talks/assets/sampling_1.png" style="height: 500px;"/>
				<img class="fragment "  data-src="/talks/assets/sampling_2.png" style="height: 500px;"/>
			</div>
			<div class="fragment">
				$\Longrightarrow$ In practice, we sample the annealed distribution using an Hamiltonian Monte-Carlo,
				with discrete annealing steps.
			</div>
		</section>

		  </section>
  



			<section>
				<section>
					<h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>
					<div class="container">
						<div class="col">
								<div style="float:right; font-size: 20px"> Remy, Lanusse, et al. (2022) <a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
						</div>
					</div>
					<div class="container">
						<div class="col">
							<img data-src='/talks/assets/ref_ktng.png' style="width:350px; height:350px;" />
							<br>
							True convergence map
						</div>
						<div class="col">
							<div class="block-content">
								<div style="position:relative; width:350px; height:350px; top:10px; left:40px;">
									<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="0">
										<img data-src='/talks/assets/ks_ktng.png' style="width:350px; height:350px;" />
									</div>
									<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="1">
										<img data-src='/talks/assets/wiener_ktng.png' style="width:350px; height:350px;" />
									</div>
									<div class="plain fragment" style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="2">
										<img data-src='/talks/assets/mean_post_ktng.png' style="width:350px; height:350px;" />
									</div>
								</div>
								<div class="block-content">
									<div style="position:relative; width:350px; height:20px; top:50px; left:10px;">
										<div class="fragment current-visible " data-fragment-index="0" style="position:absolute;top:0;left:0;width:350px;">Traditional Kaiser-Squires</div>
										<div class="fragment current-visible " data-fragment-index="1" style="position:absolute;top:0;left:0;width:350px;">Wiener Filter</div>
										<div class="fragment" data-fragment-index="2" style="position:absolute;top:0;left:0;width:350px;">Posterior Mean (ours)</div>
									</div>
								</div>
								<br>
								<br>
							</div>

						</div>
						<div class="col fragment">
							<img data-src='/talks/assets/cropped.gif' style="width:350px; height:350px;" />
							<br>
							Posterior samples
						</div>
					</div>

				</section>


				<section>
					<h3 class="slide-title">Reconstruction of the <b class="alert">HST/ACS COSMOS field</b></h3>

					<ul>
					<li> COSMOS shear data from <a href=https://ui.adsabs.harvard.edu/abs/2010A%26A...516A..63S/abstract>Schrabback et al. 2010</a>
					</li>
					<li> Prior learned from $\kappa$-TNG simulation from <a href=https://arxiv.org/abs/2010.09731>Osato et al. 2021</a>.
					</li>
				</ul>
					<br>
					<div class="container">
						<div class="col">
							<div class="block-content">
								<div style="position:relative; height:570px; top:0px; left:0px;">
									Massey et al. (2007)
									<img data-src="/talks/assets/massey.png" style="height:500px;"></img>
								</div>
							</div>
						</div>

						<div class="col">
							<div class="block-content">
								<div style="position:relative; height:570px; top:0px; left:0px;">
									<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
										Remy et al. (2022) <b class="alert">Posterior mean</b>
										<img data-src='/talks/assets/remy.png' style="height:500px;" />
									</div>

									<div class="plain fragment" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
										Remy et al. (2022) <b class="alert">Posterior samples</b>
										<img data-src='/talks/assets/cosmos_samples.gif' style="height:500px;" />
									</div>

								</div>
							</div>
						</div>

					</div>
				</section>
			</section>



<!--
			<section>
				<h3 class="slide-title">Uncertainty quantification in Magnetic Resonance Imaging (MRI)</h3>
				<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
							src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
				</div>
				<br>
				<br>
				$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
				<div><video data-autoplay loop="loop" data-src="/talks/assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
				</div>
				<br>

				<br>

				<br>

				<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
			</section> -->
     			<section>
      				<h1>Simulation-Based Inference</h1>
					<hr>
      			</section>

				  <section>
					<section>
						<h3 class='slide-title'> The limits of traditional cosmological inference </h3>
						<div class='container'>
							<div class='col'>
								<div style="position:relative; width:480px; height:30px; margin:0 auto;">
									<div class="fragment current-visible" style="position:absolute;top:0;" data-fragment-index="1">HSC cosmic shear power spectrum</div>
									<div class="fragment" style="position:absolute;top:0;" data-fragment-index="2">HSC Y1 constraints on $(S_8, \Omega_m)$</div>
								</div>
								<div style="position:relative; width:480px; height:300px; margin:0 auto;">
									<div class="fragment current-visible" style="position:absolute;top:0;left:0;" data-fragment-index="0">
										<img class="plain" data-src="/talks/assets/alonso_g1.png" />
										<img class="plain" data-src="/talks/assets/alonso_g2.png" />
									</div>
									<img class="fragment current-visible plain" data-src="/talks/assets/hsc_correlation_function.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
									<img class="fragment  plain" data-src="/talks/assets/hsc_constraints.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
								</div>
								<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Hikage et al. 2018)</div>
							</div>
					
							<div class='col'>
								<ul>
									<li class="fragment" data-fragment-index="0"> Measure the ellipticity $\epsilon = \epsilon_i + \gamma$ of all galaxies<br>
										$\Longrightarrow$ Noisy tracer of the weak lensing shear $\gamma$ </li>
									<br>
									<li class="fragment" data-fragment-index="1"> Compute <b class="alert">summary statistics</b> based on 2pt functions, <br>e.g. the <b>power spectrum</b> </li>
									<br>
									<li class="fragment" data-fragment-index="2"> Run an MCMC to recover a posterior on model parameters, using an <b class="alert">analytic likelihood</b>
										$$ p(\theta | x ) \propto \underbrace{p(x | \theta)}_{\mathrm{likelihood}} \ \underbrace{p(\theta)}_{\mathrm{prior}}$$
									</li>
								</ul>
							</div>
						</div>
					
						<div class="block fragment">
							<div class="block-title">
								Main limitation: the need for an explicit likelihood
							</div>
							<div class="block-content">
								We can only compute <b>from theory</b> the likelihood for <b class="alert">simple summary statistics</b> and on <b class="alert">large scales</b>
								<br>
								<br>
								<div class="fragment"> $\Longrightarrow$ We are dismissing a significant fraction of the information! </div>
							</div>
						</div>
					</section>
					
					<section>
						<h3 class='slide-title'>Full-Field Simulation-Based Inference</h3>
					
						<div class='container'>
							<div class='col'>
								<ul>
									<li data-fragment-index="0" > Instead of trying to analytically evaluate the likelihood of
										sub-optimal summary statistics, let us build a forward model of the full observables.<br>
										$\Longrightarrow$ <b class="alert">The simulator becomes the physical model</b>.
									</li>
									<br>
									<li class="fragment" data-fragment-index="1"> Each component of the model is now tractable, but at the
										cost of a <b>large number of latent variables</b>.
									</li>
								</ul>
					
								<br>
								<br>
					
								<div class="block fragment">
									<div class="block-title">
										Benefits of a forward modeling approach
									</div>
									<div class="block-content">
										<ul>
											<li> Fully exploits the information content of the data
												(aka "full field inference").
											</li>
					
											<br>
											<li> Easy to incorporate systematic effects.
											</li>
											<br>
											<li> Easy to combine multiple cosmological probes by joint simulations.
											</li>
										</ul>
									</div>
								</div>
							</div>
					
							<div class='col'>
								<div style="position:relative; width:600px; height:600px; margin:0 auto;">
									<img class=" plain" data-src="/talks/assets/forward_model.png" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
									<img class="fragment plain" data-src="/talks/assets/porqueres_hbm.png" style="position:absolute;top:0;left:0;width:500px;background-color: rgba(0, 0, 0, 0.7); backdrop-filter: blur(10px);" data-fragment-index="1" />
									<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Porqueres et al. 2021)</div>
								</div>
							</div>
						</div>
					</section>
					
					<section>
						<h3 class="slide-title">...so why is this not mainstream?</h3>
							<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
					
								<div class="r-stack">
					
									<img class="plain fragment" data-src="/talks/assets/plot_massive_nu.png" style="width:1000px;"/>
					
										<div class="block fragment">
											<div class="block-title">
												The Challenge of Simulation-Based Inference
											</div>
											<div class="block-content">
												$$ p(x|\theta) = \int p(x, z | \theta) dz = \int p(x | z, \theta) p(z | \theta) dz $$
												Where $z$ are <b>stochastic latent variables</b> of the simulator.<br><br>
												$\Longrightarrow$ This <b class="alert">marginal likelihood is intractable</b>!
											</div>
										</div>
									</div>
					</section>
					</section>
	
		
				<section>
				 <section>
					<h3 class="slide-title">Black-box Simulators Define Implicit Distributions</h3>
					<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:750px;"/>
					<ul>
						<li>A black-box simulator <b class="alert">defines $p(x | \theta)$ as an implicit distribution</b>, you can <b>sample from it</b> but you cannot evaluate it.
						</li>
						<li class='fragment'> <b class="alert">Key Idea</b>: Use a <b>parametric distribution model $\mathbb{P}_\varphi$ to approximate the implicit distribution $\mathbb{P}$</b>.
						</li>
					</ul>
		
					<div class="container">
						<div class="col fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
							<br>
							True $\mathbb{P}$
						</div>
		
						<div class="col  fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
							<br>
							Samples $x_i \sim \mathbb{P}$
						</div>
		
						<div class="col  fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
							<br>
							Model $\mathbb{P}_\varphi$
						</div>
					</div>
				</section>
		
				<!-- <section>
					<h3 class="slide-title">Why isn't it easy?</h3>
					<br>
					<ul>
						<li> The <b class="alert">curse of dimensionality</b> put all points far apart in high dimension
						</li>
					</ul>
					<div class="container">
						<div class="col fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756597/pasted-from-clipboard.png" class="plain"></img>
						</div>
		
						<div class="col fragment fade-up">
							<img style="height:350px;" data-src="https://developers.google.com/machine-learning/clustering/images/CurseofDimensionality.svg" class="plain"></img>
							<br>Distance between pairs of points drawn from a Gaussian distribution.
						</div>
					</div>
		
					<br>
					<ul>
						<li class="fragment"><b>Classical methods</b> for estimating probability densities, i.e. Kernel Density Estimation (KDE) start to <b>fail in high dimension</b> because of all the gaps
						</li>
					</ul>
				</section>  -->
		
				<section>
					<h3 class="slide-title">Deep Learning Approaches to Implicit Inference</h3>
		
					<div class="block fragment">
						<div class="block-title">
							A two-steps approach to Implicit Inference
						</div>
						<div class="block-content">
							<ul>
								<li> Automatically learn an <b>optimal low-dimensional summary statistic</b>
									$$y = f_\varphi(x) $$
								</li>
		
								<li class="fragment"> Use Neural Density Estimation to either:
									<ul>
										<li>build an <b>estimate $p_\phi$ of the likelihood function $p(y \ | \ \theta)$</b> (Neural Likelihood Estimation)
		
										</li>
										<br>
		
										<li>build an <b>estimate $p_\phi$ of the posterior distribution $p(\theta \ | \ y)$</b> (Neural Posterior Estimation)
		
										</li>
									</ul>
								</li>
							</ul>
						</div>
					</div>
				</section>
				</section>
		
		
				<section>
					<section>
						<h3 class="slide-title">Automated Summary Statistics Extraction</h3>
					  <img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
							<ul>
								<li> Introduce a parametric function $f_\varphi$ to <b class="alert">reduce the dimensionality of the
									data while preserving information</b>.
								</li>
							</ul>
							<div class="container">
								<div class="col">
									<div class="r-stack">
										<img class="plain fragment"  data-fragment-index="0"  data-src="/talks/assets/mutual_information.png" />
										<!-- <img class="plain fragment" data-fragment-index="1"  data-src="/talks/assets/imnn.png" /> -->
									</div>
									<!-- <div class="fragment" style="float:right; font-size: 15px"  data-fragment-index="1"> Makinen, Charnock, Alsing, Wandelt (2021) <a href="https://arxiv.org/abs/2107.07405"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2107.07405-B31B1B.svg" class="plain"
											style="height:20px;vertical-align:middle;" /></a></div> -->
				
								</div>
								<div class="col">
									<div class="block fragment" data-fragment-index="0">
										<div class="block-title">
											Information-based loss functions
										</div>
										<div class="block-content">
											<ul>
												<li> Summary statistics $y$ is sufficient for $\theta$ if
													$$ I(Y; \Theta) = I(X; \Theta) \Leftrightarrow p(\theta | x ) = p(\theta | y) $$
												</li>
												<li class="fragment" > Variational Mutual Information Maximization
													$$ \mathcal{L} \ = \ \mathbb{E}_{x, \theta} [ \log q_\phi(\theta | f_\varphi(x)) ] \leq  I(Y; \Theta) $$
	
														<div style="float:right; font-size: 15px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
																	style="height:20px;vertical-align:middle;" /></a></div>
												</li>
	
												<!-- <li class="fragment" data-fragment-index="1" > Information Maximization Neural Network
													$$\mathcal{L} \ = \ - | \det \mathbf{F} |  \ \mbox{with} \ \mathbf{F}_{\alpha, \beta} = tr[ \mu_{\alpha}^t C^{-1} \mu_{\beta} ] $$
													<div style="float:right; font-size: 15px"> Charnock, Lavaux, Wandelt (2018) <a href="https://arxiv.org/abs/1802.03537"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1802.03537-B31B1B.svg" class="plain"
															style="height:20px;vertical-align:middle;" /></a></div>
												</li> -->
											</ul>
										</div>
									</div>
								</div>
							</div>
					</section>
				</section>
		
		
				<section>
				<section>
					<h3 class="slide-title">Unrolling the Probabilistic Learning Recipe</h3>
					<ul>
						<li class="fragment fade-up"> I assume a forward model of the observations:
							\begin{equation}
							p( x ) = p(x | \theta) \ p(\theta) \nonumber
							\end{equation}
							All I ask is the ability to sample from the model, to obtain $\mathcal{D} = \{x_i, \theta_i \}_{i\in \mathbb{N}}$
						</li>
						<br>
						<li class="fragment fade-up"> I am going to assume $q_\phi(\theta | x)$ a <b>parametric conditional density</b>
						</li>
						<br>
						<li class="fragment fade-up">Optimize the parameters $\phi$ of $q_{\phi}$ according to
							\begin{equation}
							\min\limits_{\phi} \sum\limits_{i} - \log q_{\phi}(\theta_i | x_i) \nonumber
							\end{equation}
							In the limit of <b>large number of samples</b> and <b>sufficient flexibility</b>
							\begin{equation}
							\boxed{q_{\phi^\ast}(\theta | x) \approx p(\theta | x)} \nonumber
							\end{equation}
						</li>
					</ul>
		
					<div style="position:relative; height:30px; margin-left: 4em;">
						<div class="fragment current-visible" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
							optimizing a <b>parametric estimator</b> over<br> the <b>Bayesian joint distribution</b>
						</div>
						<div class="fragment" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
							optimizing a <b class="alert">Deep Neural Network</b> over<br> a <b class="alert">simulated training set</b>.
						</div>
					</div>
				</section>
	
	
				<section>
					<h3 class="slide-title">Illustration on log-normal lensing simulations</h3>
			
					<div class="container">
						<div class="col"> 
							<!-- <div class="container"> -->
								<!-- <div class="col">
									<img data-src="https://avatars.githubusercontent.com/u/72620117?v=4" style='width:150px; height:150px;'></img>
									<img data-src="/talks/assets/justine.jpeg" style='width:150px; height:150px;'></img>
									<br>
									<small>Denise Lanzieri (left) and Justine Zeghal (right) </small>
								</div>
			
								<div class="col"> -->
									<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
									<br>
									<a href="https://github.com/DifferentiableUniverseInitiative/sbi_lens">DifferentiableUniverseInitiative/sbi_lens</a><br>
									JAX-based log-normal lensing simulation package 
								<!-- </div> -->
			
							 <!-- </div> -->
							<img data-src="/talks/assets/mass_map_tomo.png" />
							<ul>
								<li>10x10 deg$^2$ maps at LSST Y10 quality, conditioning the log-normal shift
									parameter on $(\Omega_m, \sigma_8, w_0)$
								</li>
								<br>
								<li class="fragment" data-fragment-index="0"> Infer full-field posterior on cosmology:
									<ul>
										<li> <b>explicitly</b> using an Hamiltonian-Monte Carlo (NUTS) sampler 
											</li>
										<li class="fragment"  data-fragment-index="1"> <b>implicitly</b> using a learned summary statistics and conditional density estimation.
									</li>
								</li>
							</ul>
							
						</div>
						<div class="col r-stack">
							<img class="fragment current-visible" data-fragment-index="0" data-src="/talks/assets/compare_ff_ps_contour_plot_multi_tomo_bins.png"/>
							<img class="fragment"  data-fragment-index="1" data-src="/talks/assets/compare_contour_plot_multi_tomo_bins.png"/>
						</div>
					</div>
				</section>
	
				</section>
	
				<section>
					<h3 class="slide-title">A variety of algorithms</h3>
					<div style="float:right; font-size: 20px">Lueckmann, Boelts, Greenberg, Gon√ßalves, Macke (2021) <a href="https://arxiv.org/abs/2101.04653"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2101.04653-B31B1B.svg" class="plain"
								style="height:25px;vertical-align:middle;" /></a></div>
					<img class="plain" data-src="/talks/assets/sbibm_comparison.png"/>
		
					<br>
					<br>
						A few important points:
						<br><br>
						<ul>
							<li> <b>Amortized</b> inference methods, which estimate $p(\theta | x)$, can greatly speed up posterior estimation once trained.
							</li>
							<br>
		
							<li> <b>Sequential</b> Neural Posterior/Likelihood Estimation methods can actively sample simulations needed to refine the inference.
							</li>
						</ul>
				</section>
	
							<section>
								<section>
									<h3 class="slide-title">Example of application: Constraining Dark Matter Substructures</h3>
									<div class="container">
										<div class="col">
											<div style="float:right; font-size: 20px">
												Brehmer, Mishra-Sharma, Hermans, Louppe, Cranmer (2019) <a href="https://arxiv.org/abs/1909.02005"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A1909.02005-B31B1B.svg" class="plain"
														style="height:25px;vertical-align:middle;" /></a></div>
										</div>
									</div>
					
									<div class="r-stack">
										<img data-src="/talks/assets/Brehmer2019a.png" style='height:500px;'/>
										<img class="fragment" data-src="/talks/assets/Brehmer2019b.png" style='height:500px;'/>
										<img class="fragment" data-src="/talks/assets/Brehmer2019.gif" style="height:500px;"/>
									</div>
					
								</section>
					
								<section>
									<h3 class="slide-title">Example of application: Infering Microlensing Event Parameters</h3>
									<div class="container">
										<div class="col">
											<div style="float:right; font-size: 20px"> Zhang, Bloom, Gaudi, <b>Lanusse</b>, Lam, Lu (2021) <a href="https://arxiv.org/abs/2102.05673"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2102.05673-B31B1B.svg" class="plain"
														style="height:25px;vertical-align:middle;" /></a></div>
										</div>
									</div>
									<div class="r-stack">
									<div class="fragment current-visible">
										<img data-src="/talks/assets/Zhang2021a.png" style="height:500px"/>
									</div>
					
									<div class="fragment">
										<img data-src="/talks/assets/Zhang2021b.png" style="height:500px"/>
									</div>
					
								</div>
					
								</section>
					
											<section>
												<h3 class="slide-title">Example of application: Likelihood-Free parameter inference with DES SV</h3>
												<div class="container">
													<div class="col">
														<div style="float:right; font-size: 20px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
																	style="height:25px;vertical-align:middle;" /></a></div>
													</div>
												</div>
					
												<div class="container">
													<div class="col">
														<img class="plain" data-src="/talks/assets/ks_sv.png" style="height:550px;"></img>
													</div>
					
													<div class="col r-stack">
														<div class="fragment current-visible">
														<img class="plain" data-src="/talks/assets/orthant.png" style="height:300px;" />
														<img class="plain" data-src="/talks/assets/sim_params.png" style="height:300px;" /><br>
														Suite of N-body + raytracing simulations: $\mathcal{D}$
													</div>
					
													<div class="fragment current-visible">
														<img class="plain" data-src="/talks/assets/jeffrey_model.png" style="height:550px" /><br>
													</div>
					
													<div class="fragment">
														<img class="plain" data-src="/talks/assets/jeffrey_s8.png" />
													</div>
					
													</div>
												</div>
											</section>
										</section>
					
		
				<section>
					<h3 class="slide-title"> Main takeaways</h3>
		
					<br>
					<br>
		
					<ul>
						<li> This approach <b>automatizes cosmological inference</b>
								<ul>
									<li> Turns the summary extraction and inference problems into an <b class="alert">optimization problems</b>
									</li>
									<br>
								</ul>
						</li>
		
						<br>
		
						<li class="fragment"> If neural networks fail, inference will be <b class="alert">sub-optimal but not necessarily biased</b>.
						</li>
		
						<br>
						<br>
						<br>
		
		
						<li class="fragment"> Some resources and links:
							<ul>
								<li> Review on Simulation-Based Inference: <a href="https://arxiv.org/abs/1911.01429">Cranmer, Brehmer, Louppe (2020) </a>
		
								<li> Recent full $w$CDM Likelihood-Free Inference constraints on DES Y3: <a href="https://arxiv.org/abs/2201.07771">Fluri, Kacprzak, Lucchi, Schneider, Refregier, Hofmann (2022)</a>
		
								<li> Simulation Based Inference packages: <a href="https://www.mackelab.org/sbi/">sbi</a>, <a href="https://github.com/justinalsing/pydelfi">pydelfy</a>, <a href="https://github.com/tomcharnock/IMNN">Information Maximizing Neural Networks</a></li>
							</ul>
		
						</li>
					</ul>
				</section>
						
			<section>
				<h1>Conclusion</h1>
			</section>

			<section>
				<h3 class="slide-title">Open questions for discussion</h3>
				<br>

				<ul>
					<li><b>Regarding shape measurement</b>:
						<ul>
							<li>Can a foward modeling approach beat metacalibration?</li>

							<li>Particular benefits of such forward modeling approach for non-weak shear and flexion regime</li>
						</ul>
					</li>
					<br>
					<li><b>Regarding convergence reconstruction</b>:
						<ul>
							<li>Are posterior samples of external shear beneficial for strong lens modelers?</li>
						</ul>
					</li>
					<br>
					<li><b>Regarding Implicit Inference</b>:
						<ul>
						<li>How well can we simulate WL full-field observables?</li>

						<li>How do we detect systematics/model mispecification when working at the full-field level?</li>

						</ul>
					</li>

					<br>

					<li> Ask me about differentiable physics ;-)
					</li>
				</ul>
			</section>


			<section>
				<h1> Automatically Differentiable Physics </h1>
			</section>


      					<section>
      						<h3 class="slide-title">the hammer behind the Deep Learning revolution: Automatic Differentation</h3>

      						<ul>
      							<li class="fragment"> <b>Automatic differentiation</b> allows you to compute analytic derivatives of arbitraty expressions:<br>
      								If I form the expression $y = a * x + b$, it is separated in fundamental ops:
      								$$ y = u + b \qquad u = a * x $$
      								then gradients can be obtained by the chain rule:
      								$$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \frac{ \partial u}{\partial x} = 1 \times a = a$$
      							</li>
      							<br>
      							<li class="fragment"> This is a fundamental tool in Machine Learning, and autodiff frameworks include TensorFlow and PyTorch.
      							</li>
      						</ul>
      						<br>
      						<br>
      						<div class="block fragment">
      							<div class="block-title">
      								Enters JAX: NumPy + Autograd + GPU
      							</div>
      							<div class="block-content">

      								<div class="container">
      									<div class="col">
      										<ul>
      											<li>JAX follows the NumPy api!
      												<pre class="python"><code data-trim data-noescape>
      										import jax.numpy as np
      									</code></pre>
      											</li>
      											<li>Arbitrary order derivatives</li>
      											<li>Accelerated execution on GPU and TPU</li>

      										</ul>
      									</div>
      									<div class="col" align="center">

      										<img data-src="https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png" class="plain" />
      									</div>
      								</div>
      					</section> 

      					<section>
      						<section>
      							<h3 class="slide-title"> jax-cosmo: Finally a differentiable cosmology library, and it's in JAX!</h3>
										<div class="container">
											<div class="col">
												<div style="float:right; font-size: 20px"> Campagne, <b>Lanusse</b>, Zuntz et al. (2023)
													<a href="https://arxiv.org/abs/2302.05163"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2302.05163-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
												</div>
											</div>
										</div>
      							<div class="container">
      								<div class="col">
      									<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
      									<div> <a href="https://github.com/DifferentiableUniverseInitiative/jax_cosmo/">https://github.com/DifferentiableUniverseInitiative/jax_cosmo</a>
      									</div>

      									<pre class="python"><code data-trim data-noescape>
      										import jax.numpy as np
      										import jax_cosmo as jc

      										# Defining a Cosmology
      										cosmo = jc.Planck15()

      										# Define a redshift distribution with smail_nz(a, b, z0)
      										nz = jc.redshift.smail_nz(1., 2., 1.)

      										# Build a lensing tracer with a single redshift bin
      										probe = probes.WeakLensing([nz])

      										# Compute angular Cls for some ell
      										ell = np.logspace(0.1,3)
      										cls = angular_cl(cosmo_jax, ell, [probe])
      									</code></pre>

      									<div class="block">
      										<div class="block-title">
      											Current main features
      										</div>
      										<div class="block-content">
      											<ul>
      												<li>Weak Lensing and Number counts probes</li>
      												<li>Eisenstein & Hu (1998) power spectrum + halofit</li>
      												<li>Angular $C_\ell$ under Limber approximation </li>
      											</ul>
      											<div>$\Longrightarrow$ 3x2pt DES Y1 capable </div>
      										</div>
      									</div>

      								</div>

      								<div class="col">
      									<img class="plain" data-src="/talks/assets/jc_vs_ccl_lensing.png" />
      									<img class="plain" data-src="/talks/assets/jc_vs_ccl_clustering.png" />
      									<br>
      									Validated against the <a href="https://github.com/LSSTDESC/CCL">DESC Core Cosmology Library</a>
      								</div>
      							</div>
      						</section>

      						<section>
      							<h3 class="slide-title"> let's compute a Fisher matrix</h3>

      							<br>

      							$$F = - \mathbb{E}_{p(x | \theta)}[ H_\theta(\log p(x| \theta)) ] $$

      							<br>

      							<div class="container">
      								<div class="col fragment">

      									<pre class="python"><code data-trim data-noescape>
      					import jax
      					import jax.numpy as np
      					import jax_cosmo as jc

      					# .... define probes, and load a data vector

      					def gaussian_likelihood( theta ):
      					  # Build the cosmology for given parameters
      					  cosmo = jc.Planck15(Omega_c=theta[0], sigma8=theta[1])

      					  # Compute mean and covariance
      					  mu, cov = jc.angular_cl.gaussian_cl_covariance_and_mean(cosmo,
      					                                                    ell, probes)
      					  # returns likelihood of data under model
      					  return jc.likelihood.gaussian_likelihood(data, mu, cov)

      					# Fisher matrix in just one line:
      					F = - jax.hessian(gaussian_likelihood)(theta)
      					</code></pre>
      									<a href="https://colab.research.google.com/github/DifferentiableUniverseInitiative/jax_cosmo/blob/master/docs/notebooks/jax-cosmo-intro.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg"
      											alt="Open In Colab" class="plain" style="height:25px;" /></a>
      								</div>

      								<div class="col fragment">
      									<img data-src="/talks/assets/Fisher_mat.png" class="plain"><br><br>
      								</div>
      							</div>

      							<ul>
      								<li class="fragment"> <b class="alert">No derivatives were harmed by finite differences in the computation of this Fisher!</b> </li>
      								<li class="fragment"> Only a small additional compute time compared to one forward evaluation of the model</li>
      							</ul>

      						</section>


									      					<section>
									      						<h3 class="slide-title"> Inference becomes fast and scalable</h3>

									      						<div class="container">
									      							<div class="col">

									      								<ul>
									      									<li>Current cosmological MCMC chains take <b>days</b>, and typically require access
									      										to large computer clusters.</li>
									      									<br>
									      									<li class="fragment" data-fragment-index="1"><b class="alert">Gradients of the log posterior are required for modern efficient and scalable inference</b> techniques:
									      										<ul>
									      											<li>Variational Inference</li>
									      											<li>Hamiltonian Monte-Carlo</li>
									      										</ul>
									      									</li>
									      									<br>
									      									<li class="fragment" data-fragment-index="2">In jax-cosmo, we can trivially obtain <b>exact</b> gradients:
									      										<pre class="python"><code data-trim data-noescape>
									      					def log_posterior( theta ):
									      					    return gaussian_likelihood( theta ) + log_prior(theta)

									      					score = jax.grad(log_posterior)(theta)
									      					</code></pre>
									      									</li>

									      									<br>
									      									<li class="fragment" data-fragment-index="3">On a DES Y1 analysis, we find convergence in 70,000 samples with vanilla HMC, 140,000 with Metropolis-Hastings</li>
									      								</ul>

									      							</div>

									      							<div class="col">
									      								<div class="fragment" data-fragment-index="3">
									      									<img data-src="/talks/assets/jc_3x2pt_hmc.png" class="plain" /><br>
									      									DES Y1 posterior, jax-cosmo HMC vs Cobaya MH <br>(credit: Joe Zuntz)
									      								</div>
									      							</div>
									      						</div>
									      					</section>
      					</section> 


      					<section>
									<section>
										<h3 class="slide-title">LSST DESC 3x2pt Tomography Challenge</h3>
										<div class="container">
											<div class="col">
												<div style="float:right; font-size: 20px"> Zuntz, <b>Lanusse</b>, et al. (2021)
													<a href="https://arxiv.org/abs/2108.13418"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2008.13418-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
												</div>
											</div>
										</div>
										<div class="block">
											<div class="block-title">
												Description of the challenge
											</div>
											<div class="block-content">
												<blockquote>
													&ldquo;Given (g)riz photometry, find a tomographic bin assignment
													method that optimizes a 3x2pt analysis.&rdquo;
												</blockquote>
												<ul>
													<li> Metrics: Total Signal-to-Noise: $m_{SNR} = \sqrt{\mu^t \mathbf{C}^{-1} \mu}$ ; <b class="alert">DETF Figure of Merit</b>: $m_{FOM} = \frac{1}{\sqrt{ \det(\mathbf{F}^{-1})}}$
													</li>
													<li> Idealized setting: assumes perfect training set. More info at: <a href="https://github.com/LSSTDESC/tomo_challenge">https://github.com/LSSTDESC/tomo_challenge</a>
													</li>
												</ul>
											</div>
										</div>

										<div class="container">
											<div class="col">
												<img class="plain fragment" data-src="/talks/assets/nnexample.png" data-fragment-index="1" />
											</div>

											<div class="col">
												<ul>
													<li class="fragment" data-fragment-index="0"> <b>Conventional strategy</b>: Use a photoz code to estimate redshifts,
														then bin galaxies based on their photoz.
													</li>
													<br>
													<li class="fragment" data-fragment-index="1"> <b class="alert">Strategy with Differentiable Physics</b>:
														<ul>
															<li> Introduce a parametric bin assignement function $f_\theta(x_{phot})$</li>
															<li> Optimize $\theta$ by back-propagating through the challenge metrics. </li>
													</li>

												</ul>
											</div>
										</div>
									</section>

      						<section>
      						<iframe width="100%" height="849" frameborder="0"
      						  src="https://observablehq.com/embed/@eiffl/tomo-challenge-results-visualization?cells=viewof+results_bands%2Cmain_plot"></iframe>
      						</section>
      					</section> 

								<section>
									<h3 class='slide-title'>Back to forward modeling: the Hierarchical Bayesian Inference perspective</h3>

									<div class='container'>
										<div class='col'>
											<ul>
												<li> Another approach to using simulations is to consider them as large <b class="alert">Hierarchical Bayesian Models</b>.</li>
												<br>
												<li class="fragment" data-fragment-index="1"> Each component of the model is now tractable, but at the
													cost of a <b>large number of latent variables</b>.
												</li>
											</ul>

											<br>
											<div class="fragment">
												$\Longrightarrow$ How to peform efficient inference in this large number of dimensions?
											</div>
											<br>
											<br>
											<ul class="fragment"> A non-exhaustive list of methods:
												<li> Hamiltonian Monte-Carlo
												</li>
												<li> Variational Inference
												</li>
												<li> MAP+Laplace
												</li>
												<li> Gold Mining
												</li>
												<li> Dimensionality reduction by Fisher-Information Maximization
												</li>
											</ul>
											<br>
											<br>
											<div class="fragment">
												What do they all have in common?<br>
												-> They require fast, accurate, <b class="alert">differentiable</b> forward simulations
											</div>
										</div>

										<div class='col'>

											<div style="position:relative; width:600px; height:600px; margin:0 auto;">
												<img class="fragment current-visible plain" data-src="/talks/assets/forward_model.png" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
												<img class="fragment plain" data-src="/talks/assets/pgm_lensing.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
												<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Schneider et al. 2015)</div>
											</div>
										</div>
									</div>
								</section>


											<section class="inverted" data-background="#000">
												<h2>How do we simulate the Universe in a fast and differentiable way?</h2>
											</section>

											<section>
												<h3 class='slide-title'>Forward Models in Cosmology</h3>
												<div class="container">
													<div class='col'>
														<img data-src="/talks/assets/fieldinit.png" class="plain" style="height:300px;" />
														<b class="alert"> Linear Field </b>
													</div>
													<div class='col fragment' data-fragment-index='2'>
														<img data-src="/talks/assets/fieldfin.png" class="plain " style="height:300px;" />
														<b class="alert"> Final Dark Matter </b>
													</div>
													<hr style="width: 1px; height: 400px; background: white; border: none;" />
													<div class='col fragment' data-fragment-index='3'>
														<img data-src="/talks/assets/fieldhalo.png" class="plain " style="height:300px;" />
														<b class="alert"> Dark Matter Halos </b>
													</div>
													<div class='col fragment' data-fragment-index='4'>
														<img data-src="/talks/assets/fieldgal.png" class="plain " style="height:300px;" />
														<b class="alert"> Galaxies </b>
													</div>
												</div>
												<div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
													<div class='col fragment' data-fragment-index='2'>
														<font size="10"> $\longrightarrow$ </font> <br>
														<div class="fragment grow" data-fragment-index='5'>N-body simulations </div>
													</div>
													<div class='col fragment' data-fragment-index='3'>
														<font size="10"> $\longrightarrow$ </font> <br> Group Finding <br> algorithms
													</div>
													<div class='col fragment' data-fragment-index='4'>
														<font size="10"> $\longrightarrow$ </font> <br> Semi-analytic &amp <br> distribution models
													</div>
													<!-- 		<div class='fragment' data-fragment-index='2'> N-body simulations <div> -->
													<!-- <div class='fragment' data-fragment-index='3'> Group Finding algorithms <div> -->
													<!-- <div class='fragment' data-fragment-index='4'> Semi-analytic models <div> -->
												</div>
											</section>

											<section>
												<h3 class='slide-title'>You can try to learn the simulation...</h3>
												<div style="float:right; font-size: 25px">Learning particle displacement with a UNet. S. He, et al. (2019)</div><br><br>

												<img data-src="/talks/assets/Model-Comparison.jpg" style="height:400px;" />

												<div class="block fragment">
													<div class="block-title">
														The issue with using deep learning as a <i>black-box</i>
													</div>
													<div class="block-content">
														<ul>
															<li> No guarantees to work outside of training regime.
															</li>
															<li> No guarantees to capture dependence on cosmology accurately.
															</li>
														</ul>
													</div>
												</div>
											</section>

											<section>
												<h3 class='slide-title'>the Fast Particle-Mesh scheme for N-body simulations</h3>
												<b>The idea</b>: approximate gravitational forces by estimating densities on a grid.

												<div class='container'>
													<div class='col'>
														<ul>
															<li>The numerical scheme:
																<br>
																<br>
																<ul>
																	<li class="fragment" data-fragment-index="1"> Estimate the density of particles on a mesh<br>
																		=> compute gravitational forces by FFT
																	</li>

																	<br>

																	<li class="fragment" data-fragment-index="2"> Interpolate forces at particle positions
																	</li>

																	<br>

																	<li class="fragment" data-fragment-index="3"> Update particle velocity and positions, and iterate
																	</li>
																</ul>
															</li>
															<br>

															<li class='fragment'> Fast and simple, at the cost of approximating short range interactions.
															</li>

														</ul>
													</div>

													<div class='col'>

														<div style="position:relative; width:550px; height:550px; margin:0 auto;">
															<img class="fragment current-visible plain" data-src="/talks/assets/particle_positions_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
															<img class="fragment current-visible plain" data-src="/talks/assets/particle_density_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
															<img class="fragment current-visible plain" data-src="/talks/assets/particle_positions_0.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
															<img class="fragment  plain" data-src="/talks/assets/particle_positions_1.png" style="position:absolute;top:0;left:0;" data-fragment-index="3" />

														</div>

													</div>
												</div>

												<div class="fragment"> $\Longrightarrow$ Only a series of FFTs and interpolations.</div>
											</section>


									<section>
      					<section>
      							<h3 class='slide-title'>introducing FlowPM: Particle-Mesh Simulations in TensorFlow</h3>
      							<div class="container">
      								<div class="col">
      									<div style="float:right; font-size: 20px"> Modi, <b>Lanusse</b>, Seljak (2020)
      										<a href="https://arxiv.org/abs/2010.11847"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2010.11847-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
      								</div>
      							</div>
      							<div class='container'>
      								<div class='col'>
      									<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
      									<img data-src="/talks/assets/TF_FullColor_Horizontal.png" class='plain' style="height: 70px;" />

      									<div> <a href="https://github.com/DifferentiableUniverseInitiative/flowpm">https://github.com/DifferentiableUniverseInitiative/flowpm</a>
      									</div>
      									<pre class="python"><code data-trim data-noescape>
      													import tensorflow as tf
      													import flowpm
      													# Defines integration steps
      													stages = np.linspace(0.1, 1.0, 10, endpoint=True)

      													initial_conds = flowpm.linear_field(32,       # size of the cube
      													                                   100,       # Physical size
      													                                   ipklin,    # Initial powerspectrum
      													                                   batch_size=16)

      													# Sample particles and displace them by LPT
      													state = flowpm.lpt_init(initial_conds, a0=0.1)

      													# Evolve particles down to z=0
      													final_state = flowpm.nbody(state, stages, 32)

      													# Retrieve final density field
      													final_field = flowpm.cic_paint(tf.zeros_like(initial_conditions),
      													                               final_state[0])
      												</code></pre>
      									<ul>
      										<li> Seamless interfacing with deep learning components
      										</li>
      										<li> <b class="alert">Mesh TensorFlow</b> implementation for distribution on supercomputers
      										</li>
      									</ul>
      									<br>
      									<br>
      									<br>
      									<br>
      									<br>
      								</div>

      								<div class='col'>
												<img data-src="/talks/assets/flowpm.gif"></img>
      									<!-- <div class="fig-container" data-file="flowpm_16.html" data-style="height: 550px;"></div> -->
      									<br>
      									<br>
      									<br>
      									<br>
      								</div>
      							</div>
      						</section>

									<section>
										<h3 class='slide-title'>Mesh FlowPM: distributed, GPU-accelerated, and automatically differentiable simulations</h3>
										<!--
							<img data-src="/talks/assets/mesh_flopwm.png" class="plain" style="height:450px;" /> -->

										<div class="container">
											<div class="col">
												<img data-src="/talks/assets/mfpm_demo_1024.png" />
											</div>

											<div class="col">
												<ul>
													<li> We developed a <b class="alert">Mesh TensorFlow</b> implementation that can scale on GPU clusters (horovod+NCCL).
													</li>
													<br>
													<br>
													<li> For a $2048^3$ simulation:
														<ul>
															<li>Distributed on <b>256</b> NVIDIA V100 GPUs</li>
															<li>Runtime: 3 mins</li>
														</ul>
													</li>
													<br>
													<br>
													<li> Don't hesitate to reach out if you have a use case for model parallelism!<br>
														<img data-src="/talks/assets/github.png" class="plain" style="height:70px" /><br>

														<div> <a href="https://github.com/DifferentiableUniverseInitiative/mesh">https://github.com/DifferentiableUniverseInitiative/mesh</a>
														</div>
													</li>
												</ul>
											</div>
										</div>
									</section>

									</section>

									<section>
										<h2>Hybrid Physical-Neural ODEs for Fast N-body Simulations</h2>
										<a href="https://arxiv.org/abs/2207.05509"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2207.05509-B31B1B.svg" class="plain" style="height:25px;" /></a>
										<hr>
										<div class="container">
											<div class="col">
												<div align="left" style="margin-left: 20px;">
													<h3>Work in collaboration with: <br>
														Denise Lanzieri
													</h3>
													<img data-src="https://avatars.githubusercontent.com/u/72620117?v=4" style='width:200px; height:200px;'></img>
						
													<br>
						
													$\Longrightarrow$ <b class="alert">Learn residuals to known physical equations</b> to improve accuracy of fast PM simulations.
												</div>
											</div>
											<div class="col">
												<img class="plain" data-src="/talks/assets/cluster_2D_PM_NN.png" style="width:450px;" />
											</div>
										</div>
										<br>
									</section> 

									<section>
										<h3 class='slide-title'>Fill the gap in the accuracy-speed space</h3>
											<div class='container'>
												<div class='col'style=" position: relative;bottom: 290px; ">
													<ul>
														N-body PM simulation:
														<br>
														<br>
															<ul>
																<li>Fast (we don't solve the full N-body problem)
																</li>
																<br>
																<li> Not able to resolve structures with scales smaller than the mesh resolution
																	<ul>
																		  <br>
																			<li  class="fragment" data-fragment-index="0">  $\to$  Overdensity structures less sharp than full N-body counterparts
																			</li>
																			<br>
																			<li class="fragment" data-fragment-index="1"> $\to$ Lack power on small scales
																			</li>
																	</ul>
																</li>
															</ul>
															<br>
														<div  class="fragment" data-fragment-index="2">
														 The correction idea : mimics the physics that is missing
													</div>
													</ul>
												</div>
												<div  class='col'>
													<div class="plain fragment current-visible "  data-fragment-index="0">
														<p style="position:relative; top:10px; left:0px;">Camels simulations</p>
														<img  data-src="/talks/assets/cluster_2D_Camels.png" style="height:250px; position:relative; top:-30px; "></img>
													</div>
													<div class="plain fragment current-visible "  data-fragment-index="0">
														<p style="position:relative; top:-50px; left:0px;">PM simulations</p>
														<img  data-src='/talks/assets/cluster_2D_PM.png' style="height:250px; position:relative; top:-90px;" />
												 </div>
													<img  class="fragment" data-fragment-index="1" data-src="/talks/assets/comparison_pk_intro.png" class='plain' style="height: 400px; width:700px; position: relative;bottom: 650px; " />
											  </div>
										  </div>
									</section>
					
						  <section>
									<section>
											<h3 class="slide-title"> Augment the physical equations with a neural network</h3>
											<br><br>
												We compute the time integration from a system of ordinary differential equations (ODE)
																$$\left\{ \begin{array}{ll}
																\frac{d  \color{#6699CC}{\mathbf{x}} }{d a} & = \frac{1}{a^3 E(a)} \color{#6699CC}{\mathbf{v}} \\
																\frac{d  \color{#6699CC}{\mathbf{v}}}{d a} & =  \frac{1}{a^2 E(a)} F_\theta( \color{#6699CC}{\mathbf{x}} , a), \\
																F_\theta( \color{#6699CC}{\mathbf{x}}, a) &= \frac{3 \Omega_m}{2}  \nabla \left[ \color{#669900}{\phi_{PM}} (\color{#6699CC}{\mathbf{x}}) \right]
					
																\end{array} \right. $$
												<ul>
													<li>   <span style='color:#6699CC'>$\mathbf{x}$</span> and <span style='color:#6699CC'>$\mathbf{v}$</span> define the position and the velocity of the particles
													</li>
													<li><span style='color:#669900'>$\phi_{PM}$</span> is the gravitational potential in the mesh
													</li>
												</ul>
												<br>
												<p  class='fragment' data-fragment-index="1"> $\to$ We can use this parametrisation to complement the physical ODE with neural networks.
												</p>
												<br>
												<p  class='fragment' data-fragment-index="1">
													$$F_\theta(\mathbf{x}, a) = \frac{3 \Omega_m}{2}  \nabla \left[ \phi_{PM} (\mathbf{x}) \ast  \mathcal{F}^{-1} (1 + \color{#996699}{f_\theta(a,|\mathbf{k}|)}) \right] $$
												</p>
												<br>
												<div class="fragment" data-fragment-index="1" style="position:relative; top:0px; ">Correction integrated as a Fourier-based isotropic filter <span style='color:#996699'>$f_{\theta}$</span> $\to$ incorporates translation and rotation symmetries </div>
									</section>
					
									<section>
												<h3 class="slide-title">Learn the Neural Filter</h3>
											<ul>
											  <li> <span style='color:#996699'>$f_{\theta}(a)$</span> is defined as B-spline functions whose coefficients are the output of the Neural Network of parameters $\theta$.
												</li>
										 </ul>
										 <div>
												 <img data-src="/talks/assets/nn_manim.png" class='plain' style="height: 600px; width:950px" />
										 </div>
									</section>
									<section>
										<h3 class="slide-title">Train and validation loss</h3>
										<div class="container">
											<div class="col">
													<div  >
													$$\mathcal{L} =  \sum_{i}^{snapshots} \lambda_1||   \color{#6699CC}{\mathbf{x}^{ref}_i} -  \color{#6699CC}{\mathbf{x}_i}||_2^2  + \lambda_2 || \frac{\color{#996699}{p_i(k)}}{\color{#996699}{p_i^{ref}(k)}} -1 ||_2^2 $$
													</div>
											</div>
											<div class="col">
												<ul>
													<li >We adopt a loss function penalizing both the <span style='color:#6699CC'>particle positions</span> and the overall <span style='color:#996699'>matter power spectrum</span> at different snapshot times
													</li>
													<br>
													<li > We train and compare the model to the CAMELS simulations <a style="color:#GOLD"; href=" https://arxiv.org/pdf/2010.00619.pdf:">(Villaescusa-Navarro et al., 2021) </a>
													</li>
													<br>
													<li> 	We use a single N-body simulation of $25^3$ ($h^{-1}$ Mpc)$^3$ volume, $64^3$ dark matter particles at the fiducial cosmology of $\Omega_m = 0.3$ and $\sigma_8 = 0.8$
													</li>
													<br>
													<li> Whole code implemented in the Python package <span style='color:#669900'>Jax<span/>.
													</li>
												</ul>
											</div>
										</div>
									</section>
					
					
									<section>
										<h3 class="slide-title">Backpropagation through the ODE solver</h3>
											We are following the technique from Neural ODEs to <b>backpropagate through an ODE solver</b> (<a style="color:#FFAA7F; font-size: 20px" href="https://proceedings.neurips.cc/paper/2018/file/69386f6bb1dfed68692a24c8686939b9-Paper.pdf">Neural Ordinary Differential Equations, Chen et al. 2018</a>).
											<br><br>
											<!-- <li> $\to$ Treat the ODE solver as a black box and compute gradients using the adjoint sensitivity method (Pontryagin et al., 1962).
											</li> -->
												<div class="block">
												<div class="block-title" style='color:white'>
												 How	optimize a <span style='color:#6699CC'>loss function</span> with input the result of an ODE solver:  <span style='color:#6699CC'>$\textbf{L}$</span>(ODESolve$(\color{#996699}{z}(t_0),f,t_0,t_1,\color{#ecad60}{\theta}))$?
												</div>
												<div class="block-content">
													<br>
													 To optimize  <span style='color:#6699CC'>$\textbf{L}$</span>, we require gradients with respect to <span style='color:#ecad60'>$\theta$</span>:
													<ul>
													<ol>
													<br>
													<li class='fragment' data-fragment-index="0"> Determine how the gradient of the loss (the <span style='color:#669900'>adjoint</span>)  depends on the hidden state <span style='color:#996699'>$z$</span>(t) at each instant:
														$$\color{#669900}{\textbf{a}}(t)=\frac{\partial \color{#6699CC}{L}}{\partial \color{#996699}{\textbf{z}}(t)}$$
													</li>
													<li class='fragment' data-fragment-index="1"> Compute the <span style='color:#669900'>adjoint</span> dynamics by solving a another ODE:
														$$ \frac{d\color{#669900}{\textbf{a}}(t)}{dt}=\color{#669900}{\textbf{a}}(t)^{T}\frac{\partial f(\color{#996699}{\textbf{z}}(t),t,\color{#ecad60}{\theta})}{\partial \color{#996699}{\textbf{z}}}
															 $$
													</li>
													<li class='fragment' data-fragment-index="2"> Compute the gradients with respect to the parameters $\theta$ evaluating a third integral:
													$$ \frac{d\color{#6699CC}{L}}{d\color{#ecad60}{\theta}}=\int_{t_1}^{t_0}\color{#669900}{\textbf{a}}(t)^T \frac{\partial f (\color{#996699}{\textbf{z}}(t),t,\theta)}{\partial \color{#ecad60}{\theta}}dt $$
													</li>
											  </ol>
											  </ul>
											</div>
									</section>
					
					
					
									<section >
										<h3 class="slide-title">Hybrid Physical-Neural ODE</h3>
										<div class="container">
											<div class="col">
												<img data-src="/talks/assets/comparison_pk_without.png"/>
												<br>
												Without neural correction
											</div>
											<div class="col">
												<img data-src="/talks/assets/comparison_pk_with.png"/>
												<br>
												With neural correction
											</div>
										</div>
									</section>
								</section>
					
									<section >
										<h3 class='slide-title'>	Potential Gradient Descent (PGD)</h3>
											 <div class='container'>
												<div class='col'>
													<ul>
														<li>Additional displacements to sharpen the halos
														</li>
														<br>
														<li>The direction of the displacements points towards the halo center (local potential minimum).</li>
														<br>
														<li>The gravitational force:
																\begin{equation}
																	\mathbf{F}=-\nabla\Phi
																\end{equation}
														</li>
														<li> The PGD correction displacement:
															\begin{equation}
																\mathbf{S}=-\alpha\nabla \hat{O}_{h}\hat{O}_{l}\Phi
															\end{equation}
															High pass filter prevents the large scale growth, low pass filter reduces the numerical effect
														</li>
													</ul>
												</div>
												<div class='col'>
													 <img data-src="/talks/assets/pot_pgd.png" class='plain' style="height: 450px; width:800px" />
													<div style="float:right; font-size: 20px"><a style="color:#996699"; href=" https://iopscience.iop.org/article/10.1088/1475-7516/2018/11/009/pdf?casa_token=3b8_RUCo4uAAAAAA:aqUgqZUFV1jao2LlJSqI25p2GEh-2KmGTS_ab4p1F9TSK5d0SytTPG6rN_YRhKYdnd9lBiX32A:">(Biwei Dai et al. 2018)</a></div>
												</div>
											</div>
									</section>
					
									<section>
											<section>
												<h3 class="slide-title"> Projections of final density field</h3>
												<br>
												<br>
												<div class="container">
													<div class="col">
														<div class="block-content">
															<div style="position:relative; height:570px; width:700px top:0px; left:0px;">
																Camels simulations
																<img data-src="/talks/assets/cluster_2D_Camels.png" style="height:400px;width:1500px"></img>
															</div>
														</div>
													</div>
													<div class="col">
														<div class="block-content">
															<div style="position:relative; height:570px; top:0px; left:0px;">
																<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
																	PM simulations
																	<img data-src='/talks/assets/cluster_2D_PM.png' style="height:400px;" />
																</div>
					
																<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
																	PM+NN correction
																	<img data-src='/talks/assets/cluster_2D_PM_NN.png' style="height:400px;" />
																</div>
					
																<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="2">
																	PM+PGD correction
																	<img data-src='/talks/assets/cluster_2D_PM_PGD.png' style="height:400px;" />
																</div>
															</div>
														</div>
													</div>
					
					
												</div>
											</section>
					
											<section>
												<h3 class="slide-title">Results</h3>
													<br>
													<div >
														<li>
															Netural network trained using single CAMELS simulation of <span style='color:#996699'>$25^3$ ($h^{-1}$ Mpc)$^3$ volume</span> and <span style='color:#669900'>$64^3$ dark matter particles</span> at the fiducial cosmology of  <span style='color:#6699CC'>$\Omega_m = 0.3$</span>
														</li>
													</div>
													<br><br>
													<div class="container">
														<div class="col">
															<img data-src="/talks/assets/camels_residual_CV_0.png"/>
														</div>
														<div class="col" >
															<img style=" position: relative;bottom: 21px;" data-src="/talks/assets/cross_corr_CV_0.png" />
														</div>
													</div>
											</section>
					
										<section>
											<h3 class="slide-title">Results: Robustness to changes in resolution and cosmological parameters </h3>
												<br>
												<div >
													<li>
														Netural network trained using single CAMELS simulation of <span style='color:#996699'>$25^3$ ($h^{-1}$ Mpc)$^3$ volume</span> and <span style='color:#669900'>$64^3$ dark matter particles</span> at the fiducial cosmology of  <span style='color:#6699CC'>$\Omega_m = 0.3$</span>
													</li>
												</div>
												<br><br>
												<div class="container">
													<div class="col">
														<img data-src="/talks/assets/camels_residual_diff_resolution_CV_0.png"/>
														<br>
														Higher resolution
													</div>
													<div class="col">
														<img data-src="/talks/assets/halofit_residuals_wrong_boxsize_res.png"/>
														<br>
														Lower resolution
													</div>
													<div class="col">
														<img data-src="/talks/assets/camels_residual_diffomega_1P_1_n5.png"/>
														<br>
														Different Cosmology
													</div>
												</div>
										</section>
					
										<section>
											<h3 class="slide-title">Results: Robustness to changes in resolution and cosmological parameters  </h3>
												<br>
												<div>
													<li>
														Netural network trained using single CAMELS simulation of <span style='color:#996699'>$25^3$ ($h^{-1}$ Mpc)$^3$ volume</span> and <span style='color:#669900'>$64^3$ dark matter particles</span> at the fiducial cosmology of  <span style='color:#6699CC'>$\Omega_m = 0.3$</span>
													</li>
												</div>
												<br>
												<div class="container">
													<div class="col">
														<img data-src="/talks/assets/cross_corr_diffresolution_CV_0.png"/>
														<br>
														Higher resolution
													</div>
													<div class="col">
														<img data-src="/talks/assets/cross_corr_diffomega_1P_1_n5.png"/>
														<br>
														Different Cosmology!
													</div>
												</div>
										</section>
									</section>

      						<section>
      							<section>
      								<h3 class='slide-title'>Example use-case: reconstructing initial conditions by MAP optimization</h3>
      								<img data-src="/talks/assets/evolvingLSS.jpg" class="plain" /><br>
      								<div class="fragment">Going back to simpler times...</div>

      								<div class="fragment">
      									$$\arg\max_z \ \log p(x_{dm} = f(z)) \ + \ p(z) $$
      									where:<br>
      									<ul>
      										<li> $f$ is <b>FlowPM </b>
      										</li>
      										<li> $z$ are the initial conditions (early universe)
      										</li>
      										<li> $x_{dm}$ is the present day dark matter distribution
      										</li>
      									</ul>
      								</div>
      							</section>

      							<section>
      								<h3 class="slide-title"> MAP optimization in action</h3>
      								$$\arg\max_z \ \log p(x_{dm} = f(z)) \ + \ p(z) $$
      								<div style="float:right; font-size: 16px">credit: <a href="https://github.com/modichirag">C. Modi</a></div>
      								<br>
      								<div class="container">
      									<div class="col fragment fade-up">
      										<img data-src="/talks/assets/init_field.png" style='height:250px;' />
      										<br> True initial conditions <br> $z_0$
      									</div>

      									<div class="col">
      										<img data-src="/talks/assets/reconim_init.gif" style='height:250px;' />
      										<br> Reconstructed initial conditions $z$
      									</div>

      									<div class="col">
      										<img data-src="/talks/assets/reconim_fin.gif" style='height:250px;' />
      										<br> Reconstructed dark matter distribution $x = f(z)$
      									</div>

      									<div class="col">
      										<img data-src="/talks/assets/fin_field.png" style='height:250px;' />
      										<br> Data <br> $x_{DM} = f(z_0)$
      									</div>
      								</div>
      								<br>
      								<br>

      								<div class="fragment">
      									Check out this blogpost for more details <br> <a href=https://blog.tensorflow.org/2020/03/simulating-universe-in-tensorflow.html>
      										https://blog.tensorflow.org/2020/03/simulating-universe-in-tensorflow.html</a>
      								</div>
      							</section>
      						</section>

									<section>
										<section>
											<h3 class="slide-title">CosmicRIM: Recurrent Inference Machines for Initial Condition Reconstruction</h3>
											<div class="container">
												<div class="col">
													<div style="float:right; font-size: 20px"> Modi, <b>Lanusse</b>, Seljak, Spergel, Perreault-Levasseur (2021)
														<a href="https://arxiv.org/abs/2104.12864"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2104.12864-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a>
													</div>
												</div>
											</div>
											<div class="container">

												<div class="col">
													Recurrent Neural Network Architecture
													<img data-src="/talks/assets/cosmic_rim.png" width="450" />
												</div>

												<div class="col fragment">
														Initial conditions cross-correlation
														<img data-src="/talks/assets/cosmic_rim_rc.png" width="500" />
												</div>
											</div>
											<ul>
												<li class="fragment">CosmicRIM: Learn to optimize by embedding a Neural Network in the optimization algorithm.<br>
													$\Longrightarrow$ converges 40x faster than LBFGS.</li>
											</ul>
										</section>

										<!-- <section>
											<h3 class="slide-title">Experiments</h3>
											<div class="block ">
												<div class="block-title">
													Settings
												</div>
												<div class="block-content">
													<ul>
														<li>Forward model: $64^3$ particles, 400 Mpc/h box, 2LPT dynamics with 2nd order bias model
														</li>
														<li> RIM: 10 steps, trained under l2 loss
														</li>
													</ul>
												</div>
											</div>

											<div class="container">

												<div class="col">
													Initial conditions cross-correlation
													<img data-src="/talks/assets/cosmic_rim_rc.png" width="500" />

												</div>
												<div class="col">
													Transfer function<br>
													<img data-src="/talks/assets/rim_transfer.png" width="500" />
												</div>

											</div>
											<ul>
												<li>CosmicRIM: Learn to optimize by embedding a Neural Network in the optimization algorithm.<br>
													$\Longrightarrow$ converges 40x faster than LBFGS.</li>
											</ul>

										</section> -->
									</section>

									<section>
										<h1> Conclusion </h1>
									</section>

									<section>
										<h3 class="slide-title"> Conclusion </h3>
										<div class="block ">
											<div class="block-title">
												Merging Deep Learning with Physical Models for Bayesian Inference
											</div>
											<div class="block-content">
												$\Longrightarrow$ Makes <b>Bayesian inference possible</b> at scale and with non-trivial models!
												<br>
												<br>
												<ul>


												<li class="fragment"> Complement known physical models with data-driven components
													<ul>
														<li>Use data-driven generative model as prior for solving inverse problems.</il>
													</ul>
													</il>
													<br>

													<li class="fragment"> Enables inference in high dimension from numerical simulators.
														<ul>
															<li>Automagically construct summary statistics.</li>
															<li>Provides the density estimation tools needed.</li>
														</ul>
														</il>
														<br>



													<li class="fragment"> Differentiable physical models for fast inference
														<ul>
															<li> Differentiability enables Bayesian inference over large scale simulations.</li>
															<li> Models can directly be embedded alongside deep learning components.</li>
														</ul>
													</li>
													<br>
												</ul>
											</div>
										</div>
										<br>

										<br>
										<p class="fragment">Thank you ! </p>
										<br> <br> <br>
									</section>


		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		} */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});
	</script>
</body>

</html>
