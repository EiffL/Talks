<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>A Review of Model-Based Deep Learning for Astrophysics</title>

	<meta name="description" content="Lensing Odyssey, Sept. 19th 2022">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>A Review of Model-Based Deep Learning for Astrophysics</h1>
						<h3>Lensing Odyssey, Kouremenos, Crete, Sept. 19th 2022 </h3>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>Fran√ßois Lanusse</h2>
								<br>
								<img src="/talks/assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="/talks/assets/logo_cnrs.png" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="/talks/assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a
							href="https://eiffl.github.io/Kouremenos2022">eiffl.github.io/talks/Kouremenos2022</a>
					</div>
				</div>
			</section>

			<section>
				<h3 class="slide-title"> Why are we here today talking about Deep Learning? </h3>
				<img data-src="/talks/assets/deep.png" />
			</section>

			<section>
				<section data-background="/talks/assets/fig1w.png">
				</section>

				<section data-background="/talks/assets/fig1.png">
				</section>

				<section data-background="/talks/assets/hsc_illustration_tract.png">
					<h3>WWAD: <span class="fragment">What Would an Astrophysicist Do ?</span></h3>
					<div class="fragment fade-up">
						<img data-src="/talks/assets/fig2.png" style="height: 525px;">
						<div class="container">
							<div class="col"> gri composite </div>
							<div class="col"> g - $\alpha$ i </div>
							<div class="col"> detected areas </div>
							<div class="col"> HST images </div>
						</div>
						<div style="float:right; font-size: 20px;"> RingFinder (Gavazzi et al. 2014) </div>
					</div>
					<br>
				</section>

				<section>
					<canvas data-chart="bar">
						<!--
											{
											 "data": {
											  "labels": ["CFHTLS"," DES", "LSST"],
											  "datasets": [
											   {
											    "data":[220, 2400, 120000],
											    "label":"Number of strong lenses","backgroundColor":"#374A67"
											   },
											   {
											    "data":[75, 2500, 10000],
											    "label":"Person-hour required for visual inspection [RingFinder]","backgroundColor":"#A63446"
											   }
											  ]
											 },
											 "options": { "responsive": "true",
						        "scales": {
						            "yAxes": [{
						                "type": "logarithmic"
						            }]
						        }
						 					}
											}
											-->
					</canvas>
					<div style="float:right; font-size: 20px">Gavazzi et al. (2014), Collett (2015)</div>
					<br>
					<div class="fragment fade-up">$\Longrightarrow$ Plainly intractable at the scale of LSST</div>
				</section>

				<!-- <section>
					<h3 class="slide-title"> The Deep Learning approach: Convolutional Neural Networks</h3>
					<img data-src="/talks/assets/single_layer.png" class="plain"></img>
				</section> -->

				<section>
					<div class="container">
						<div class="col">
							<div style="position:relative; height:600px; margin:0 auto;">
								<img height="600px" data-src="/talks/assets/deeplens.png" class="plain"
									style="position:absolute;top:0;left:200px;" />
								<img height="600px" data-src="/talks/assets/candidates.png" class="plain fragment"
									style="position:absolute;top:200px;left:0;height:300px;" data-fragment-index="1" />
							</div>

							<br>
							CMUDeepLens Architecture<br> (Lanusse et al. 2017)
						</div>
						<div class="col">
							The Deep Learning approach in three steps:
							<br>
							<br>
							<ul>
								<li> <b>Step I</b>: Introduce a <b class="alert">parameteric function $f_\theta$</b>,
									this is your neural network. For images, choose a CNN.
								</li>
								<br>
								<br>
								<li class="fragment" data-fragment-index="1"> <b>Step II</b>: Create a <b
										class="alert">dataset $\mathcal{D}$</b> of examples $x_i$ and associated labels
									$y_i$:
									$$\mathcal{D} = \{ (x_i, y_i) \}_{i \in [0, N]}$$
								</li>
								<br>
								<li class="fragment"> <b>Step III</b>: <b class="alert">Optimize the parameters
										$\theta$</b> as to minimize an
									appropriate loss function.<br> For classification, the binary cross
									entropy:
									$$\arg\min_\theta \sum_{i=1}^N y_i \log f_\theta(x_i) + (1 - y_i) \log
									f_{\theta}(x_i)$$
								</li>
							</ul>
						</div>
				</section>

				<section>
					<h3 class="slide-title">The Euclid strong-lens finding challenge</h3>
					<div style="float:right; font-size: 20px">Metcalf, . . ., <b>Lanusse</b>, et al. (2018)</div>
					<br>
					<div class="container">
						<div class="col">
							<img height=500 data-src="/talks/assets/ground_lenses.png"></img>
						</div>
						<div class="col">
							<img height=500 data-src="/talks/assets/space_based.jpg"></img>
						</div>
					</div>

				</section>
				<section>
					<div class="container">
						<div class="col">
							<img data-src="/talks/assets/roc_ground_small.png" class="plain"
								style="height: 250px;"></img>

							<br>
							<div class="fragment fade-up"> <b class="alert">Better accuracy than human visual
									inspection!</b></div>
						</div>

						<div class="col fragment">
							<iframe src="https://giphy.com/embed/Ztzt8zhmmpVPUiSNMX" width="480" height="400"
								frameBorder="0" class="giphy-embed" allowFullScreen></iframe>
							<p><a href="https://giphy.com/gifs/theoffice-episode-13-the-office-tv-Ztzt8zhmmpVPUiSNMX">via
									GIPHY</a></p>
						</div>
					</div>

					<div class="block fragment">
						<div class="block-title">
							A true paradigm shift
						</div>
						<div class="block-content">
							<ul>
								<li class="fragment"> <b>Before</b>: <b class="alert">Experts</b> would carefully craft
									image statistics they would expect a priori to be useful for their problem.</li>
								<br>
								<li class="fragment"> <b>After</b>: <b class="alert">Reframe the task as an optimization
										problem</b>, and let the optimization figure out how to solve the task </li>
							</ul>
						</div>
					</div>
				</section>
			</section>

			<section>
				<section>
					<h3 class="slide-title">The Deep Learning Explosion in Astrophysics</h3>

					<div class="container">
						<canvas data-chart="bar">
							<!--
								{
								 "data": {
									"labels": ["2012", "2013", "2014","2015", "2016" ,"2017", "2018", "2019", "2020", "2021", "2022"],
									"datasets": [
									 {
										"data":[ 21, 15, 17, 22, 31, 71, 113, 231, 322, 422, 513 ],
										"label":"Deep Learning || CNN || Neural Network ","backgroundColor":"#A63446"
									}
									]
								 },
								 "options": { "responsive": "true",
							"scales": {
									"yAxes": [{
											"type": "linear"
									}]
							}
								}
								}
								-->
						</canvas><br>
					</div>
					<div><b>astro-ph</b> abstracts mentioning <b>Deep Learning</b>, <b>CNN</b>, or <b>Neural
							Networks</b></div>
				</section>

				<section>
					<h3 class='slide-title'>Selected examples of Deep Learning in Astrophysics</h3>

					<div class='container'>
						<div class='col'>
							<ul>
								<li class="fragment" data-fragment-index="0"> <b>Classifying objects</b> <br>
									<div style="font-size: smaller;"><a href="https://arxiv.org/abs/1905.07424">Galaxy
											Zoo: Probabilistic Morphology through Bayesian CNNs and Active
											Learning,Walmsley et al. 2019</a>
									</div>
								</li>

								<li class="fragment" data-fragment-index="1"> <b>Inferring object properties</b> <br>
									<div style="font-size: smaller;"><a
											href="https://arxiv.org/abs/1806.06607">Photometric redshifts from SDSS
											images using a Convolutional
											Neural Network, Pasquet et al. 2018</a>
									</div>
								</li>


								<li class="fragment" data-fragment-index="2"> <b>Self-supervised Representation
										Learning</b> <br>
									<div style="font-size: smaller;"><a
											href="https://arxiv.org/abs/2110.13151">Self-supervised similarity search
											for large scientific datasets, Stein et al. 2021</a>
									</div>
								</li>


								<li class="fragment" data-fragment-index="3"> <b>Solve inverse problems</b> <br>
									<div style="font-size: smaller;"><a
											href="https://arxiv.org/abs/2201.05561">Probabilistic Mass Mapping with
											Neural Score Estimation, Remy et al. 2022</a> </div>
								</li>


								<li class="fragment" data-fragment-index="4"> <b>Emulate costly physical models</b> <br>
									<div style="font-size: smaller;"><a
											href="https://arxiv.org/abs/1911.11778">SPECULATOR: Emulating stellar
											population synthesis for fast and accurate galaxy spectra and photometry,
											Alsing et al. 2020</a> </div>
								</li>


								<li class="fragment" data-fragment-index="5"> <b>Accelerating numerical simulations</b>
									<br>
									<div style="font-size: smaller;"><a href="https://arxiv.org/abs/2010.06608">
											AI-assisted super-resolution cosmological simulations, Li et al. 2021</a>
								</li>


								<li class="fragment" data-fragment-index="6"> <b>Enabling Simulation-Based Inference</b>
									<br>
									<div style="font-size: smaller;"><a
											href="https://arxiv.org/abs/2009.08459">Likelihood-free inference with
											neural compression of DES SV weak lensing map statistics, Jeffrey et. al
											2021</a> </div>
								</li>

							</ul>
						</div>

						<div class='col'>
							<div style="position:relative; width:600px; height:600px; margin:0 auto;">
								<img class="fragment current-visible plain" data-src="/talks/assets/walmsley2019.png"
									style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
								<img class="fragment current-visible plain" data-src="/talks/assets/Pasquet2018.png"
									style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1" />
								<img class="fragment current-visible plain" data-src="/talks/assets/Stein2021.png"
									style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="2" />
								<img class="fragment current-visible plain" data-src="/talks/assets/cropped.gif"
									style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="3" />
								<img class="fragment current-visible plain" data-src="/talks/assets/Alsing2020.png"
									style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="4" />
								<img class="fragment current-visible plain" data-src="/talks/assets/li2020.png"
									style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="5" />
								<img class="fragment plain" data-src="/talks/assets/jeffrey_s8.png"
									style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="6" />
							</div>
						</div>
					</div>
				</section>
			</section>


			<section class="inverted" data-background="#000">
				<h2> Will Deep Learning solve all our problems? </h2>
			</section>

			<!-- 
			<section>
				<section class="inverted" data-background="#000">
					<h2> Will Deep Learning solve all our problems? </h2>
				</section>

				<section>
					<h3 class="slide-title">Be careful to ask the right questions!</h3>
					The most common pitfall: <b>Covariate Shift</b><br>
					<img data-src="/talks/assets/covariate_shift.png" />
					<br>
					<br>
					$\Longrightarrow$ The training data plays an integral part in the answer
				</section>
			</section> 
			-->

			<section>
				<section>
					<h3 class="slide-title" style="position:absolute;top:0;">A Motivating Example: Image Deconvolution
					</h3>
					<br>
					<br>

					$ y = P \ast x + n $

					<div class="container">

						<div class="col">
							<p> <b class="alert"> Observed $y$</b></p>
							<img class="plain" data-src="/talks/assets/cosmos_gal_ground.png" style="width: 250px" />
							<br>Ground-Based Telescope
						</div>

						<div class="col fragment fade-up" data-fragment-index='0'>
							<p> <b class="alert">$f_\theta$</b> </p>
							<img class="plain " data-src="/talks/assets/generic_network_inv.png"
								style="height: 250px; width:500px" />
							<br>some deep Convolutional Neural Network
						</div>

						<div class="col">
							<p><b class="alert"> Unknown $x$</b> </p>
							<img class="plain" data-src="/talks/assets/cosmos_gal.png" style="width: 250px" />
							<br>Hubble Space Telescope
						</div>
					</div>
					<br>
					<br>
					<ul>
						<li class="fragment fade-up" data-fragment-index='0'> A standard approach would be to train a
							neural network $f_\theta$ to <b class="alert">estimate $x$ given $y$</b>.
						</li>
					</ul>
				</section>

				<section>
					<ul>
						<li> <i>Step I</i>: Assemble from <b>data</b> or <b>simulations</b> a <b class="alert">training
								set</b> of images
							$$\mathcal{D} = \{(x_0, y_0),
							(x_1, y_1), \ldots, (x_N, y_N) \}$$
							$\Longrightarrow$ the dataset contains <b class="alert">hardcoded assumptions</b> about PSF
							$P$
							noise $n$, and galaxy morphology $x$.
						</li>
						<li class="fragment fade-up"> Step II: Train the neural network $f_\theta$ under a <b
								class="alert">regression loss</b> of the type:
							$$ \mathcal{L} = \sum_{i=0}^N \parallel x_i - f_\theta(y_i)\parallel^2 $$
						</li>
					</ul>
					<div class="container fragment">
						<div class="col">
							<img class="plain" data-src="/talks/assets/cosmos_gal_ground.png" style="width: 250px" />
							<p>$$ y $$</p>
						</div>

						<div class="col">
							<img class="plain " data-src="/talks/assets/generic_network_inv.png"
								style="height: 250px; width:500px" />
							<p>$$f_\theta$$</p>
						</div>

						<div class="col">
							<img class="plain" data-src="/talks/assets/rec_median.png" style="width: 250px" />
							$$f_\theta(y)$$
						</div>

						<div class="col fragment fade-up" style="float:center;">
							<img class="plain" data-src="/talks/assets/cosmos_gal.png" style="width: 250px" />
							<br>
							<p>$$ \mbox{True } x $$</p>
						</div>
					</div>
					<div class="fragment">$\Longrightarrow$Why is the network output different from the truth? If it's
						not the truth, then <b>what is $f_\theta(y)$?</b></div>
				</section>

				<section class="inverted" data-background="#000">
					<figure>
						<blockquote>
							A neural network will <b class="alert">try</b> to answer <br>the <b class="alert">question
								you
								ask</b>.
						</blockquote>
						<figcaption style="align-items: right;">
							@EiffL - Kouremenos, Sept. 2022
						</figcaption>
					</figure>
				</section>

				<section>
					<p>Let's try to understand the neural network output by looking at the <b class="alert">loss
							function</b></p>
					$$ \mathcal{L} = \sum_{(x_i, y_i) \in \mathcal{D}} \parallel x_i - f_\theta(y_i)\parallel^2 \quad
					\simeq \quad \int \parallel x - f_\theta(y) \parallel^2 \ p(x,y) \ dx dy $$

					<div class="fragment" data-fragment-index="1">$$\Longrightarrow \int \left[ \int \parallel x -
						f_\theta(y) \parallel^2 \ p(x|y) \ dx \right] p(y) dy $$ </div>

					<div class="block fragment" data-fragment-index="2">
						<div class="block-content">
							$\mathcal{L}$ minimized when $f_{\theta^\star}(y) = \int x \ p(x|y) \ dx $, i.e.
							when <b class="alert">$f_{\theta^\star}(x)$ is predicting the mean of $p(x|y)$</b>.
						</div>
					</div>
					<div class="container">
						<div class="col">
							<div style="position:relative; width:500px; height:500px; margin:0 auto;">
								<img class="fragment current-visible plain" data-src="/talks/assets/nn_l2.png"
									style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
								<img class="fragment current-visible plain" data-src="/talks/assets/nn_l2_mean.png"
									style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="4" />
								<img class="fragment current-visible plain" data-src="/talks/assets/nn_l1.png"
									style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="5" />
								<img class="fragment plain" data-src="/talks/assets/nn_l1_median.png"
									style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="6" />
							</div>
						</div>
						<div class="col">
							<ul>
								<li class="fragment" data-fragment-index="3"> Using an <b class="alert">$\ell_2$ loss
										learns the mean</b> of the $p(x | y)$
								</li>
								<br>
								<li class="fragment" data-fragment-index="5"> Using an <b class="alert">$\ell_1$ loss
										learns the median</b> of $p(x|y)$
								</li>
								<br>
								<li class="fragment" data-fragment-index="7"> In general, training a neural network for
									regression doesn't
									achieve de mode of the distribution.<br>
									<br>
									<div style='vertical-align:middle; display:inline;'>Check this <a
											href="https://medium.com/cosmostat/regression-in-the-presence-of-uncertainties-with-tensorflow-probability-1b7449f1083b"
											target="blank_">blogpost</a> and this
										notebook to learn how to do that: <a
											href=" https://colab.research.google.com/drive/1yi_BY09LCS8qHCfJqvCIftKuW6jNe-t1"
											target="_blank"><img
												src="https://colab.research.google.com/assets/colab-badge.svg"
												alt="Open In Colab" class="plain"
												style="height:25px;vertical-align:middle; display:inline;" /></a></div>
								</li>
							</ul>
						</div>
					</div>
				</section>
			</section>

			<section>
				<h3 class="slide-title">A Bayesian understanding of a regression network</h3>
				<div class="container">
					<div class="col">
						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
							<img class="plain" data-src="/talks/assets/cosmos_gal_ground.png"
								style="position:absolute;top:0;left:0;width:200px;" />
						</div>
						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
							<div class='col ' style="position:absolute;top:0;left:0;width:200px;"> Data $y$</div>
						</div>
						<br>
					</div>

					<div class="col" data-fragment-index='0'>
						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
							<div><video data-autoplay data-loop data-src="/talks/assets/rec_samples.mp4"
									type="video/mp4" style="height: 200px;" />
							</div>
						</div>
						<div>Posterior samples</div>
					</div>

					<div class="col">
						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
							<img class="plain " data-src="/talks/assets/rec_median.png"
								style="position:absolute;top:0;left:0;width:200px;" />
						</div>
						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
							<div class='col' style="position:absolute;top:0;left:0;width:200px;">Posterior mean</div>
						</div>
					</div>

					<div class="col">
						<div style="position:relative; width:200px; height:200px; margin:0 auto;">
							<img class="plain " data-src="/talks/assets/cosmos_gal.png"
								style="position:absolute;top:0;left:0;width:200px;" />
						</div>

						<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
							<div class='col' data-fragment-index='1'
								style="position:absolute;top:0;left:0;width:200px;"> True $x$</div>
						</div>
					</div>
				</div>
				<br>
				<ul>
					<li> The distribution $p(x|y)$ can be understood as a <b>Bayesian posterior distribution</b>:
						$$ p(x | y) \propto \underbrace{p(y | x)}_{\mbox{likelihood}} \quad
						\underbrace{p(x)}_{\mbox{prior}} $$
					</li>
					<br>
					<li class="fragment"> Both priors and likelihoods are <b>learned implicitly</b> by the neural
						network from the training set.
						<br>$\Longrightarrow$ <b class="alert">priors AND likelihoods are hardcoded</b> in the training
						set.
					</li>
					<br>
					<li class="fragment"> The network only returns a <b>point summary</b> of this posterior distribution
						(<b class="alert">no uncertainty quantification</b>).
					</li>
				</ul>
			</section>

			<!-- 
			<section>
				<section>
					<h3 class="slide-title" style="position:absolute;top:0;">A Physicist's approach: let's build a model
					</h3>
					<div class="container">
						<div class="col">
							<div style="float:right; font-size: 20px"> <b>Lanusse</b> et al. (2020) <a
									href="https://arxiv.org/abs/2008.03833"><img
										src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg"
										class="plain" style="height:25px;vertical-align:middle;" /></a></div>
						</div>
					</div>
					<div class="container">
						<div class="col">
							<img class="plain fragment" data-src="/talks/assets/rand_z_square.png" style="height: 150px"
								data-fragment-index="4" />
						</div>
						<div class="col">
							<img class="plain fragment" data-src="/talks/assets/cosmos_gal.png" style="width: 200px"
								data-fragment-index="3" />
						</div>
						<div class="col">
							<img class="plain fragment" data-src="/talks/assets/cosmos_gal_psf.png" style="width: 200px"
								data-fragment-index="2" />
						</div>

						<div class="col">
							<img class="plain fragment" data-src="/talks/assets/cosmos_gal_pix.png" style="width: 200px"
								data-fragment-index="1" />
						</div>

						<div class="col">
							<img class="plain fragment" data-src="/talks/assets/cosmos_gal_ground.png"
								style="width: 200px" data-fragment-index="0" />
						</div>
					</div>

					<div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
						<div class='col fragment' data-fragment-index='4'>
							<font size="10"> $\longrightarrow$ </font> <br> $g_\theta$
						</div>
						<div class='col fragment' data-fragment-index='3'>
							<font size="10"> $\longrightarrow$ </font> <br> PSF
						</div>
						<div class='col fragment' data-fragment-index='2'>
							<font size="10"> $\longrightarrow$ </font> <br> Pixelation
						</div>
						<div class='col fragment' data-fragment-index='1'>
							<font size="10"> $\longrightarrow$ </font> <br> Noise
						</div>
					</div>

					<div class="container">
						<div class="col">
							<div style="position:relative; width:400px; height:300px; margin:0 auto;">
								<img data-src="/talks/assets/pgm_0.png" class="plain fragment current-visible "
									style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="0" />
								<img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible "
									style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="1" />
								<img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible "
									style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="2" />
								<img data-src="/talks/assets/pgm_2.png" class="plain fragment current-visible "
									style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="3" />
								<img data-src="/talks/assets/pgm_3.png" class="plain fragment "
									style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="4" />
							</div>
						</div>
						<div class=" col">
							<div class="block fragment" data-fragment-index="0">
								<div class="block-title">
									Probabilistic model
								</div>
								<div class="block-content">
									<div style="position:relative; width:400px; height:100px; margin:0 auto;">
										<div class="plain fragment current-visible "
											style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="0">
											$$ x \sim ? $$ </div>
										<div class="plain fragment current-visible "
											style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="1">
											$$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br>latent $z$ is a
											denoised galaxy image</div>
										<div class="plain fragment current-visible "
											style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="2">
											$$ x \sim \mathcal{N}( \mathbf{P} z, \Sigma) \quad z \sim ?$$<br>latent $z$
											is a super-resolved and denoised galaxy image</div>
										<div class="plain fragment current-visible "
											style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="3">
											$$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast z), \Sigma) \quad z \sim ?
											$$<br>latent $z$ is a deconvolved, super-resolved, and denoised galaxy image
										</div>
										<div class="plain fragment " style="position:absolute;top:0;left:0;width:400px;"
											data-fragment-index="4"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast
											g_\theta(z)), \Sigma) \quad z \sim \mathcal{N}(0, \mathbf{I}) $$ <br>latent
											$z$ is a Gaussian sample<br> <b class="alert"> $\theta$ are parameters of
												the model</b> </div>
									</div>
									<br>
									<br>
									<br>
								</div>
							</div>
						</div>
					</div>
					<div class="fragment"> $\Longrightarrow$ <b class="alert"> Decouples the morphology model from the
							observing conditions</b>.</div>
				</section>

				<section>
					<h3 class="slide-title">Bayesian Inference a.k.a. Uncertainty Quantification</h3>
					<div class="container">
						<div class="col">
							<img data-src="/talks/assets/pgm.png" class="plain" style="height: 250px;"></img>
						</div>
						<div class="col">
							The Bayesian view of the problem:
							$$ p(z | x ) \propto p_\theta(x | z, \Sigma, \mathbf{\Pi}) p(z)$$
							where:
							<br>
							<ul>
								<li>$p( z | x )$ is the <b class="alert">posterior</b></li>
								<li>$p( x | z )$ is the data likelihood, <b class="alert">contains the physics</b></li>
								<li>$p( z )$ is the <b>prior</b> </li>
							</ul>
						</div>
					</div>

					<div class="container">
						<div class="col">
							<div style="position:relative; width:200px; height:200px; margin:0 auto;">
								<img class="plain fragment current-visible"
									data-src="/talks/assets/cosmos_gal_ground.png"
									style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0" />
								<img class="plain fragment" data-src="/talks/assets/cosmos_gal.png"
									style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1" />
							</div>
							<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
								<div class='col fragment current-visible' data-fragment-index='0'
									style="position:absolute;top:0;left:0;width:200px;"> Data<br> $x_n$</div>
								<div class='col fragment' data-fragment-index='1'
									style="position:absolute;top:0;left:0;width:200px;"> Truth<br> $x_0$ </div>
							</div>
							<br>
						</div>

						<div class="col fragment" data-fragment-index='0'>
							<div style="position:relative; width:200px; height:200px; margin:0 auto;">
								<div><video data-autoplay data-loop data-src="/talks/assets/rec_samples.mp4"
										type="video/mp4" style="height: 200px;" />
								</div>
							</div>
							<div>Posterior samples<br> $g_\theta(z)$</div>
						</div>

						<div class="col">
							<div style="position:relative; width:200px; height:200px; margin:0 auto;">
								<div><video class="fragment current-visible" data-autoplay data-loop
										data-src="/talks/assets/rec_lsst.mp4" type="video/mp4"
										style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0" />
								</div>
								<img class="plain fragment " data-src="/talks/assets/rec_median.png"
									style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1" />
							</div>

							<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
								<div class='col fragment current-visible' data-fragment-index='0'
									style="position:absolute;top:0;left:0;width:200px;"> <br> $\mathbf{P} (\Pi \ast
									g_\theta(z))$</div>
								<div class='col fragment' data-fragment-index='1'
									style="position:absolute;top:0;left:0;width:200px;"> Median </div>
							</div>
						</div>

						<div class="col">
							<div style="position:relative; width:200px; height:200px; margin:0 auto;">
								<div><video class="fragment current-visible" data-autoplay data-loop
										data-src="/talks/assets/res_lsst.mp4" type="video/mp4"
										style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0" />
								</div>
								<img class="plain fragment " data-src="/talks/assets/rec_std.png"
									style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1" />
							</div>

							<div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
								<div class='col fragment current-visible' data-fragment-index='0'
									style="position:absolute;top:0;left:0;width:200px;"> Data residuals <br> $x_n -
									\mathbf{P} (\Pi \ast g_\theta(z))$</div>
								<div class='col fragment' data-fragment-index='1'
									style="position:absolute;top:0;left:0;width:200px;"> Standard Deviation </div>
							</div>
						</div>
					</div>
					<div class="fragment"> $\Longrightarrow$ <b class="alert">Uncertainties are fully captured by the
							posterior</b>.</div>
				</section>
			</section> -->

			<section>
				<h3 class="slide-title">Program for this session</h3>
				<div class=container>
					<div class="col">
						<div class="fig-container" data-file="venn.html" data-style="height: 600px;"></div>
					</div>

					<div class="col">

						<br>
						<br>
						<div class="block fragment">
							<div class="block-title">
								"Model-Based" Deep Learning
							</div>
							<div class="block-content">
								Generic approach to <b>uncertainty quantification</b> and <b>interpretability</b>:
								<br>
								<ul>
									<li>(Differentiable) Physical Forward Models</li>
									<li>Deep Generative Models</li>
									<li>Bayesian Inference</li>
								</ul>
							</div>
						</div>
						<br>
						<br>
						<div class="fragment">
							<b>Plan for this session</b>
							<ul>
								<li>Understanding Uncertainties in Deep Learning</li>

								<li>Deep Generative Models</li>

								<li>Bayesian Inference over Hybrid physical/deep learning models</li>
							
								<li>Simulation-Based Inference</li>
							</ul>
						</div>
					</div>
				</div>
			</section>


			<section>
				<h1>Part I: Understanding Uncertainties in Deep Learning</h1>
				<h2>a.k.a Deep Probabilistic Learning</h2>

				<p class="alert">Let's move to this <a href="https://slides.com/eiffl/deep-probabilistic-learning-acd756">slide deck</a></p>
			</section>
<!-- 
			<section>
				<h3 class="slide-title"> Learning Objectives</h3>
				<br>
				<br>
				<b>ML theory</b>:<br>
				<ul>
					<li>Epistemic and Aleatoric Uncertainties</li>

					<li>Conditional Neural Density Estimation</li>

					<li>Interpretation of Neural Networks as Bayesian Posterior estimators</li>

				</ul>
				<br>
				<br>
				<br>
				<b>Practical skills</b>:<br>
				<ul>
					<li><b>Jax & Flax</b>: How to implement and train a Neural Network in JAX</li>

					<li><b>TensorFlow Probability</b>: How to combine Neural Networks with Probability Distributions</li>

				</ul>

			</section>


			<section>
				<h3 class="slide-title">Our Case Study: Dynamical Mass Measurment of Galaxy Clusters</h3>
				
				<br>

				<div class="container">
					<div class="col">
						<div class="r-stack">
						<div class="sl-block" data-block-type="image" data-name="image-db0ea6" data-block-id="b5d39e0b1f8c9b2ae917385113a95dba" style="width: 417.577px; height: 312.8px; left: 800px; top: 288px; min-width: 1px; min-height: 1px;">
							<div class="sl-block-content" style="z-index: 13;"><img style="" data-natural-width="546" data-natural-height="409" data-lazy-loaded="" src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/9653402/pasted-from-clipboard.png"></div>
						</div>
						<div class="sl-block" data-block-type="image" data-name="image-4b0ed3" data-block-id="2426725cddf0e0a79436d0e9f8490c2a" style="width: 417.577px; height: 312.8px; left: 800px; top: 288px; min-width: 1px; min-height: 1px;">
							<div class="sl-block-content fragment" style="z-index: 14;" data-fragment-index="0"><img style="" data-natural-width="546" data-natural-height="409" data-lazy-loaded="" src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/9653404/pasted-from-clipboard.png"></div>
						</div>
						</div>
						<p><span style="font-size:0.7em">Figures and data from <a href="https://arxiv.org/abs/1902.05950" target="_blank">Ho et al. 2019</a></span></p>
							
						<div class="sl-block" data-block-type="image" data-name="image-58216d" data-block-id="5e47c5737ce8cea65cd5698d711392e6" style="">
							<div class="sl-block-content" style="z-index: 16;"><img style="" data-natural-width="281" data-natural-height="81" data-lazy-loaded="" src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/9653420/pasted-from-clipboard.png"></div>
						</div>
					</div>

					<div class="col fragment">
						Let's try to estimate a cluster mass with a Neural Network.
						<br>
						<br>
						<p><strong>What we will feed the model</strong>:</p>

						<ul>
							<li>Richness</li>
							<li>Velocity Dispersion</li>
							<li>Information about member galaxies:
								<ul>
									<li>radial distribution</li>
									<li>stellar mass distribution</li>
									<li>LOS velocity distribution</li>
								</ul>
							</li>
						</ul>

						<br>
						<br>

						<p>Training data from MultiDark Planck 2 N-body simulation (Klypin et al. 2016) with 261287 clusters.</p>
					</div>

				</div>

				<br>
				<br>
			</section>

			<section>
				<h3 class="slide-title">Let's try it!</h3>
				<br>
				<br>
				<br>
				We will be using <a href="https://colab.research.google.com/drive/1A9wG9UVIRbWjxtM5XzDO3Sqc-aY5TzLJ?usp=sharing">this notebook</a>.
				<br>
				<br>
				<br>
				<p class="alert">Your goal:</p> Building a regression model with a Mean Squared Error loss in JAX/Flax
			</section>


			<section>
				<h3 class="slide-title">Modeling the various sources of Uncertainties</h3>
				<p>From this <a
						href="https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf"
						target="_blank">excellent tutorial</a>:&nbsp;</p>

				<div class="container">
					<div class="col">
						<div class="sl-block" data-block-type="image"
							style="min-width: 1px; min-height: 1px; width: 610px; height: 144px; left: 9px; top: 158px;"
							data-block-id="b71ca86455fded0c4346dab524245bbf">
							<div class="sl-block-content" style="z-index: 12;"><img data-natural-width="1600"
									data-natural-height="378" style="" data-lazy-loaded=""
									src="talks/assets/pasted-from-clipboard.png">
							</div>
						</div>
						<div class="sl-block" data-block-type="image"
							style="min-width: 1px; min-height: 1px; width: 638px; height: 138px; left: 15px; top: 302px;"
							data-block-id="56ceb39d29122abe8ac50fc231ca890a">
							<div class="sl-block-content fragment" style="z-index: 13;" data-fragment-index="0"><img
									data-natural-width="1600" data-natural-height="345" style="" data-lazy-loaded=""
									src="talks/assets/pasted-from-clipboard1.png">
							</div>
						</div>
						<div class="sl-block" data-block-type="image"
							style="min-width: 1px; min-height: 1px; width: 647px; height: 140px; left: 15px; top: 440px;"
							data-block-id="4340801a83e5ef0fe35da77186b39c5a">
							<div class="sl-block-content fragment" style="z-index: 14;" data-fragment-index="1"><img
									style="" data-natural-width="1600" data-natural-height="346" data-lazy-loaded=""
									src="talks/assets/pasted-from-clipboard2.png">
							</div>
						</div>
						<div class="sl-block" data-block-type="image"
							style="min-width: 1px; min-height: 1px; width: 808px; height: 145px; left: 15px; top: 575px;"
							data-block-id="238440e5882b1fca2c72ca2af786082a">
							<div class="sl-block-content fragment" style="z-index: 15;" data-fragment-index="2"><img
									data-natural-width="1600" data-natural-height="287" style="" data-lazy-loaded=""
									src="talks/assets/pasted-from-clipboard3.png">
							</div>
						</div>

					</div>

					<div class="col">
						<br>
						<ul>
							<li>Linear regression
								$$\hat{y} = a x$$
								<br>
							</li>
							<li class="fragment" data-fragment-index="0">Aleatoric Uncertainties
								$$\hat{y} \sim \mathcal{N}(a x, \sigma^2)$$
								<br>
							</li>
							<li class="fragment" data-fragment-index="1">Epistemic Uncertainties
								$$\hat{y} = w x \quad w \sim p(w | \{x_i, y_i\})$$
								<br>
							</li>
							<li class="fragment" data-fragment-index="2">Epistemic+ Aleatoric Uncertainties
								$$\hat{y} \sim \mathcal{N}(w x, \sigma^2) \\ w, \sigma \sim p(w,
								\sigma | \{x_i, y_i\})$$
							</li>
					</ul>

					</div>
				</div>

			</section> -->


			<section>
				<h1>Part II: Deep Generative Modeling</h1>
			</section>

			<section>
				<section>
					<h3 class="slide-title"> What is generative modeling?</h3>
					<br>
					<ul>
						<li>The goal of generative modeling is to <b>learn an <b class="alert">implicit</b> distribution
								$\mathbb{P}$</b>
							from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
						</li>
						<br>
						<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
							that tries to be close to $\mathbb{P}$.
						</li>
					</ul>

					<br>
					<div class="container">
						<div class="col fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png"
								class="plain"></img>
							<br>
							True $\mathbb{P}$
						</div>

						<div class="col  fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png"
								class="plain"></img>
							<br>
							Samples $x_i \sim \mathbb{P}$
						</div>

						<div class="col  fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png"
								class="plain"></img>
							<br>
							Model $\mathbb{P}_\theta$
						</div>
					</div>
					<br>
					<br>
					<ul>
						<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b>
							and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
						</li>
					</ul>

				</section>

				<section>
					<h3 class="slide-title">Why isn't it easy?</h3>
					<br>
					<ul>
						<li> The <b class="alert">curse of dimensionality</b> put all points far apart in high dimension
						</li>
					</ul>
					<div class="container">
						<div class="col fragment fade-up">
							<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756597/pasted-from-clipboard.png"
								class="plain"></img>
						</div>

						<div class="col fragment fade-up">
							<img style="height:350px;"
								data-src="https://developers.google.com/machine-learning/clustering/images/CurseofDimensionality.svg"
								class="plain"></img>
							<br>Distance between pairs of points drawn from a Gaussian distribution.
						</div>
					</div>

					<br>
					<ul>
						<li class="fragment"><b>Classical methods</b> for estimating probability densities, i.e. Kernel
							Density Estimation (KDE) start to <b>fail in high dimension</b> because of all the gaps
						</li>
					</ul>
				</section>
			</section>

			<section>
				<h3 class="slide-title"> The evolution of generative models </h3>

				<br> <br> <br>
				<div class='container'>
					<div class='col'>
						<div style="position:relative; width:500px; height:500px; margin:0 auto;">
							<img class="fragment current-visible plain" data-src="/talks/assets/DBN.png"
								style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="0" />
							<img class="fragment current-visible plain" data-src="/talks/assets/vae_faces.jpg"
								style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
							<img class="fragment current-visible plain" data-src="/talks/assets/gan-samples-1.png"
								style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="2" />
							<img class="fragment plain" data-src="/talks/assets/karras2017.png"
								style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
						</div>
					</div>

					<div class='col'>
						<ul>
							<li class="fragment" data-fragment-index="0"> Deep Belief Network <br> (Hinton et al. 2006)
							</li>
							<br>
							<li class="fragment" data-fragment-index="1"> Variational AutoEncoder <br> (Kingma & Welling
								2014) </li>
							<br>
							<li class="fragment" data-fragment-index="2"> Generative Adversarial Network <br>
								(Goodfellow et al. 2014)</li>
							<br>
							<li class="fragment" data-fragment-index="3"> Wasserstein GAN <br> (Arjovsky et al. 2017)
							</li>
						</ul>
					</div>
				</div>
				<br> <br> <br>
			</section>

			<section>
				<h3 class="slide-title"> A visual Turing test </h3>
				<div class="container">
					<div class="col">
						<img data-src="/talks/assets/samples_pixel_cnn.png" class="plain" style="height: 500px;"></img>
						<br>
						<div class="fragment fade-up" data-fragment-index="0"> Fake PixelCNN samples </div>
					</div>
					<div class="col">
						<img data-src="/talks/assets/sdss5.png" class="plain" style="height: 500px;"></img>
						<br>
						<div class="fragment fade-up" data-fragment-index="0"> Real galaxies from SDSS </div>
					</div>
				</div>
			</section>

			<section>
				<h3 class="slide-title">A brief survey of Generative Models Families </h3>

				<div class="container">

				<div class="col">
					<div class="r-stack">
						<!-- <img class="fragment current-visible" data-fragment-index="0" data-src="talks/assets/DiffusionModels.png" style="width: 600px;"/> -->

						<img class="fragment current-visible" data-fragment-index="1" data-src="talks/assets/pixel_cnn_conv.png" style="width: 600px;"/>

						<img class="fragment" data-fragment-index="2"  data-src="talks/assets/DiffusionModels.png" style="width: 600px;"/>


					</div>
					</div>

				<div class="col">
				<ul>
					<li class="fragment" data-fragment-index="0"><b>Latent Variable Models</b>:<br>
						Assume the following form for the model disribution:
						$$ x = f_\theta(z) \qquad \mbox{with} \qquad z \sim p(z)$$
						This is the case of <b>Generative Adversarial Networks</b>, <b>Variational Auto-Encoders</b>, <b>Normalizing Flows</b>.
					</li>
					<br>
					<li class="fragment" data-fragment-index="1"><b>Auto-Regressive Models</b>:<br>
					Assume an autoregressive decomposition of the signal into products of 1D conditional distributions:
					$$ p_\theta(x) = p_\theta(x_0) p_\theta(x_1 | x_0) \ldots p_\theta(x_n | x_{n-1}, \ldots x_{0}) $$
						This is the case of <b>PixelCNN</b>, <b>GPT-3</b>
				</li>
					<br>
					<li class="fragment" data-fragment-index="2"><b>Diffusion Models</b>:<br>
						Target distribution generated by a reverse noise diffusion process controled by a Stochastic Differential Equation (SDE).
					</li>
				</ul>
			</div>
				</div>
			</section>

			<section>

			  <section>
				<h3 class="slide-title">The Variational Auto-Encoder</h3>
				<div class="container">
					<div class="col">
					  <ul>
						<li>We assume a <b class="alert">prior distribution $p(z)$</b> for the latent variable, and a <b class="alert">likelihood $p_\theta(x |z)$</b> defined by a neural network. 
							<br>Typically $p_\theta(x|z) = \mathcal{N}(x; f_\theta(x), \sigma^2)$
						</li>
						<br>

						<li class="fragment fade-up"> Training the generative model amounts to finding $\theta_\star$ that
						  <b>maximizes the marginal likelihood</b> of the model:
							$$p_\theta(x) = \int \mathcal{N}(x; f_\theta(z), \sigma^2) \ p(z) \ dz$$
							<div> $\Longrightarrow$ This is <b class="alert">generally intractable</b></div>
						</li>
						<br>
						<li class="fragment fade-up"> In a VAE, efficient training of parameter $\theta$ is made possible by <b class="alert">Amortized Variational Inference</b>.
						</li>
					  </ul>
					</div>
				</div>

				<div class="block fragment fade-up">
				<div class="block-title">
				 Auto-Encoding Variational Bayes (Kingma & Welling, 2014)
				</div>
				<div class="block-content">
				  <ul>
					<li class="fragment fade-up"> We introduce a <b>parametric distribution</b> $q_\phi(z | x)$ which aims to model the
					posterior $p(z | x)$. We want to minimize $\mathbb{D}_{KL}\left( q_\phi(z | x) \parallel p(z | x) \right)$
					</li>
					<br>
					<li class="fragment fade-up"> Working out the KL divergence between these two distributions leads to:

					  $$\log p_\theta(x) \quad \geq \quad - \mathbb{D}_{KL}\left( q_\phi(z | x) \parallel p(z) \right) \quad + \quad \mathbb{E}_{z \sim q_{\phi}(. | x)} \left[ \log p_\theta(x | z)  \right]$$

					  $\Longrightarrow$ This is the <b>Evidence Lower-Bound</b>, which is differentiable with respect to $\theta$ and $\phi$.
					</li>
				  </ul>
			  </div>
			  </div>

			  </section>

			  <section>
				<h3 class="slide-title">The famous Variational Auto-Encoder</h3>
				<img data-src="/talks/assets/vae.png" class="plain" style="height: 450px;"> </img>
				<br>
				<br>
				$$\log p_\theta(x) \geq - \underbrace{\mathbb{D}_{KL}\left( q_\phi(z | x) \parallel p(z) \right)}_{\mbox{code regularization}} + \underbrace{\mathbb{E}_{z \sim q_{\phi}(. | x)} \left[ \log p_\theta(x | z)  \right]}_{\mbox{reconstruction error}} $$
				</section>

					<section>
							  <h3 class="slide-title"> Sampling from the model</h3>
						<div class="container">
						<div class="col fragment fade-up">
						  <img data-src="/talks/assets/vae_samples_bad.png" class="plain" ></img>
						  Woups... what's going on?
						</div>
						<div class="col">
						  <img data-src="/talks/assets/latent_space.png" class="plain fragment fade-up" ></img>
						</div>
					  </div>
				</section>



					<section>
						<h3 class="slide-title"> Tradeoff between code regularization and image quality</h3>

				  <br>
				  $$\log p_\theta(x| \Sigma, \Pi ) \geq - \underbrace{\mathbb{D}_{KL}\left( q_\phi(z | x, \Sigma, \Pi) \parallel p(z) \right)}_{\mbox{code regularization}} + \underbrace{\mathbb{E}_{z \sim q_{\phi}(. | x, \Sigma, \Pi)} \left[ \log p_\theta(x | z, \Sigma, \Pi)  \right]}_{\mbox{reconstruction error}} $$

				  <img data-src="/talks/assets/sdss_ae_kl.png" class="plain" ></img>

				</section>

				<section data-background-image=https://media.giphy.com/media/3o85xIO33l7RlmLR4I/source.gif>
				</section>

					<section>
						<h3 class="slide-title"> Conditional sampling in VAE latent space</h3>

					<div class="container">
					<div class="col">
						<img data-src="/talks/assets/conditional_flow.png" class="plain" ></img>
						<p>(Lanusse et al. 2020)</p>
					</div>
						<div class="col">

							<a href="https://arxiv.org/abs/2008.03833"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg" class="plain" style="height:25px;"/></a>

							<ul>
								<li> We build a latent space model $p_\varphi(z)$ using a Masked Autoregressive Flow (MAF) (Papamakarios, et al. 2017)
								</li>
								<br>
								<li class="fragment"> While we are learning to sample from the latent space, we can also <b class="alert"> learn to sample conditionaly</b>:
											$$  p_\varphi(z | y) $$
								</li>
								<br>

								<li class="fragment"> Here we learn to sample images conditioned on:
									<ul>
										<li> Size: half-light radius $r$
										</li>
										<li> Brightness: I band magnitude $mag\_auto$
										</li>
										<li> Redshift: COSMOS photometric redshift $zphot$
										</li>
									</ul>
								</li>
							</ul>
						</div>
					</div>
				</section>

				<section>
					<h3 class="slide-title"> Flow-VAE samples</h3>
			  <br>
			  <br>
			  <img class="current-visible plain" data-src="/talks/assets/lanusse2020_figure1.png"/>
		  </section>
			  </section>

			  <section>
			<section>
				<h3 class="slide-title"> Normalizing Flows</h3>

				<p> Still a latent variable model, except the mapping $f_\theta$ is made to be <b class="alert">bijective</b>.</p>
		<div class="container">
		<div class="col">
		  <img data-src="/talks/assets/flow_dinh_1.png" class="plain fragment fade-up" data-fragment-index="1"></img>
		  <img data-src="/talks/assets/flow_dinh_2.png" class="plain fragment fade-up" data-fragment-index="3"></img>

		  <br>
				 <div class="fragment fade-up" style="float:right; font-size: 20px" data-fragment-index="1">Dinh et al. 2016</div>
		</div>
		<div class="col">
				  <div class="block fragment fade-up" data-fragment-index="1">
				  <div class="block-title">
				   Normalizing Flows
				  </div>
				  <div class="block-content">
					<ul>
					  <li> Assumes a <b class="alert">bijective</b> mapping between
						data space $x$ and latent space $z$ with prior $p(z)$:
						$$ z = f_{\theta} ( x ) \qquad \mbox{and} \qquad x = f^{-1}_{\theta}(z)$$
					  </li>
					  <li class="fragment" data-fragment-index="2"> Admits an explicit marginal likelihood:
						$$ \log p_\theta(x) = \log p(z) + \log \left| \frac{\partial f_\theta}{\partial x}  \right|(x)    $$
					  </li>
					</ul>
				</div>
				</div>
				<br>
				  <br>
				  <div class="fragment">
				  $\Longrightarrow$ The challenge is in designing mappings $f_\theta$ that are both: 
				  <b>easy to invert, easy to compute the jacobian of</b>.</div>
				  <br>
				  <br>
		</div>
	</div>

		</section>

		<section>
			<h3 class="slide-title">One example of NF: RealNVP</h3>
			<br>
			<br>

			<div class="container">
				<div class="col">
					<img data-src="/talks/assets/realNVP_jacobian.png" style="height: 300px;"/>
					<br> Jacobian of an affine coupling layer
				</div>
				<div class="col">
					In a standard affine RealNVP, one Flow layer is defined as:

					$$ \begin{matrix} y_{1\ldots d} &=& x_{1 \ldots d} \\
					y_{d+1\ldots N} &=& x_{d+1 \ldots N} ‚äô \sigma_\theta(x_{1 \ldots d}) + \mu_\theta(x_{1 \ldots d})
					\end{matrix} $$
					where $\sigma_\theta$ and $\mu_\theta$ are unconstrained neural networks.
					<br>
					<br>
					We will call this layer an <b class="alert">affine coupling</b>.

				</div>


			</div>

			<br>
			<br>
			$\Longrightarrow$ This structure has the advantage that the Jacobian of this layer will be lower triangular which makes computing its determinant easy.
			

		</section>
	</section>

			<!-- <section>
				<h3 class="slide-title">Diffusion Models</h3>

			</section> -->

			<section data-vertical-align-top>
				<h3 class="slide-title" >Not all generative models are created equal</h3>
							<img data-src="/talks/assets/generative_models_table.png" class="plain"></img>
								<div style="float:right; font-size: 20px">Grathwohl et al. 2018</div>
					<br>
					<br>
				<ul>
					<li> Of particular interest are models with an <b class="alert">explicit $\log p(x)$</b> (not the case of VAEs, GANs, Denoising Diffusion Models).</li>
					<br>
				</ul>
			</section>

			<section>
				<h3 class="slide-title">Why are these generative models useful?</h3>

				<b class="alert">Implicit distributions are everywhere!</b>
				<br>
				<br>
				<div class="container">
					<div class="col fragment">
						<b>Case I</b>: Examples from data, no accurate physical model<br>
						<img data-src="/talks/assets/real_gal-inv-small.png" style="height:400px;" /><br>
						<div style="float:right; font-size: 20px">Mandelbaum et al. 2014</div>
						<br>
					</div>

					<div class="col fragment">
						<b>Case II</b>: Physical model only available as a simulator<br>
						<img data-src='/talks/assets/convergence.png' style="height:400px;" /><br>
						<div style="float:right; font-size: 20px">Osato et al. 2020</div>
						<br>
					</div>
				</div>
				<br>
				<div class="fragment">$\Longrightarrow$ Generative models <b class="alert">will enable Bayesian
						inference</b> in cases where
					implicit distributions are involved, by providing a tractable $p_\theta(x)$.
				</div>
			</section>


			<section>
				<h3 class="slide-title">Your Turn!</h3>

				<br>
				<br>

				We will be using <a href="https://colab.research.google.com/github/EiffL/Tutorials/blob/master/NormalizingFlowsInJAX.ipynb">this notebook</a> to implement a Normalizing Flow in JAX+Flax+TensorFlow Probability

				<img data-src="/talks/assets/points.png"/>
				<br>
				<br>

				<br>
				<br>
			</section>


			<section>
				<h1>Part III: Hybrid Physical/Deep Learning Modeling</h1>
			</section>



			<section>
				<section>
				  <h3 class="slide-title" style="position:absolute;top:0;">Back to our Motivating Example</h3>
				  <br>
				  <br>
				   <div class="container">
					 <div class="col">
						 <img class="plain" data-src="/talks/assets/cosmos_gal.png" style="width: 250px"/>
						 <br><b class="alert">Hubble Space Telescope</b>
					 </div>

					 <div class="col fragment fade-up">
						 <img class="plain " data-src="/talks/assets/generic_network.png" style="height: 250px; width:500px"/>
						 <br>some deep neural network
					 </div>

					 <div class="col">
					   <img class="plain" data-src="/talks/assets/cosmos_gal_ground.png" style="width: 250px"/>
					   <br><b class="alert">Simulated Ground-Based Telescope</b>
					 </div>
				   </div>
		   <!--
				   <div> <img class="plain" data-src="/talks/assets/galaxygan.png"></div> -->
				   <br>
				   <br>

				   <div class="block fragment" >
				   <div class="block-title">
					The issue with generic black box deep learning inference
				   </div>
				   <div class="block-content">
					   <ul>
						 <li class="fragment">No explicit control of noise, PSF, depth.
						   <ul>
							 <li>Unless covered by the training data, the result becomes unpredictable.
							 </li>
						   </ul>
						 </li>
						 <br>
						 <li class="fragment">No guarantees some physical properties are preserved
						   <br>$\Longrightarrow$ In this case, the flux of the deconvolved object
						 </li>
						 <br>
						 <li class="fragment"><b>Robust</b> quantification of uncertainties is extremely difficult.
						 </li>
					   </ul>
				   <br>
				 </div>
				 </div>

				</section>

				<section>
				   <h3 class="slide-title" style="position:absolute;top:0;">A Physicist's approach: let's build a model</h3>
						   <div class="container">
							   <div class="col">
								   <div style="float:right; font-size: 20px"> <b>Lanusse</b> et al. (2020) <a href="https://arxiv.org/abs/2008.03833"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
							   </div>
						   </div>
				   <div class="container">
					 <div class="col">
						 <img class="plain fragment" data-src="/talks/assets/rand_z_square.png" style="height: 150px" data-fragment-index="4"/>
					 </div>
					 <div class="col">
						 <img class="plain fragment" data-src="/talks/assets/cosmos_gal.png" style="width: 200px" data-fragment-index="3"/>
					 </div>
					 <div class="col">
					   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_psf.png" style="width: 200px" data-fragment-index="2"/>
					 </div>

					 <div class="col">
					   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_pix.png" style="width: 200px" data-fragment-index="1"/>
					 </div>

					 <div class="col">
					   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_ground.png" style="width: 200px" data-fragment-index="0"/>
					 </div>
				   </div>

				 <div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
				   <div class='col fragment' data-fragment-index='4'> <font size="10"> $\longrightarrow$ </font> <br> $g_\theta$ </div>
				   <div class='col fragment' data-fragment-index='3'> <font size="10"> $\longrightarrow$ </font> <br> PSF </div>
				   <div class='col fragment' data-fragment-index='2'> <font size="10"> $\longrightarrow$ </font> <br> Pixelation</div>
				   <div class='col fragment' data-fragment-index='1'> <font size="10"> $\longrightarrow$ </font> <br> Noise </div>
				 </div>

				 <div class="container">
					 <div class="col">
					   <div style="position:relative; width:400px; height:300px; margin:0 auto;">
					   <img data-src="/talks/assets/pgm_0.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="0"/>
					   <img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="1"/>
					   <img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="2"/>
					   <img data-src="/talks/assets/pgm_2.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="3"/>
					   <img data-src="/talks/assets/pgm_3.png" class="plain fragment " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="4"/>
					   </div>
					 </div>
					 <div class=" col">
					   <div class="block fragment" data-fragment-index="0">
					   <div class="block-title">
						Probabilistic model
					   </div>
					   <div class="block-content">
					   <div style="position:relative; width:400px; height:100px; margin:0 auto;">
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="0"> $$ x \sim ? $$ </div>
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="1"> $$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br>latent $z$ is a denoised galaxy image</div>
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="2"> $$ x \sim \mathcal{N}( \mathbf{P} z, \Sigma) \quad z \sim ?$$<br>latent $z$ is a super-resolved and denoised galaxy image</div>
						 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="3"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast z), \Sigma) \quad z \sim ? $$<br>latent $z$ is a deconvolved, super-resolved, and denoised galaxy image </div>
						 <div class="plain fragment " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="4"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast g_\theta(z)), \Sigma) \quad z \sim \mathcal{N}(0, \mathbf{I}) $$ <br>latent $z$ is a Gaussian sample<br> <b class="alert"> $\theta$ are parameters of the model</b> </div>
					   </div>
					   <br>
					   <br>
					   <br>
					 </div>
					 </div>
					 </div>
				 </div>
				 <div class="fragment"> $\Longrightarrow$ <b class="alert"> Decouples the morphology model from the observing conditions</b>.</div>
				</section>

			   <section>
				 <h3 class="slide-title">Bayesian Inference a.k.a. Uncertainty Quantification</h3>
				 <div class="container">
					 <div class="col">
					   <img data-src="/talks/assets/pgm.png" class="plain" style="height: 250px;" ></img>
					 </div>
					 <div class="col">
					   The Bayesian view of the problem:
							$$ p(z | x ) \propto p_\theta(x | z, \Sigma, \mathbf{\Pi}) p(z)$$
						where:
						<br>
						  <ul>
							<li>$p( z | x )$ is the <b class="alert">posterior</b></li>
							<li>$p( x | z )$ is the data likelihood, <b class="alert">contains the physics</b></li>
							<li>$p( z )$ is the <b>prior</b> </li>
						  </ul>
					 </div>
				 </div>

				 <div class="container">
					 <div class="col">
					   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
						 <img class="plain fragment current-visible" data-src="/talks/assets/cosmos_gal_ground.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0" />
						 <img class="plain fragment" data-src="/talks/assets/cosmos_gal.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
					   </div>
					   <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
						 <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;"> Data<br> $x_n$</div>
						 <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Truth<br> $x_0$ </div>
					   </div>
					   <br>
					 </div>

					 <div class="col fragment" data-fragment-index='0' >
					   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
						 <div><video data-autoplay data-loop data-src="/talks/assets/rec_samples.mp4" type="video/mp4" style="height: 200px;"/>
						 </div>
					   </div>
					   <div>Posterior samples<br> $g_\theta(z)$</div>
					 </div>

					 <div class="col">
					   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
						 <div><video class="fragment current-visible" data-autoplay data-loop data-src="/talks/assets/rec_lsst.mp4" type="video/mp4" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0"/></div>
						 <img class="plain fragment " data-src="/talks/assets/rec_median.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
					   </div>

					   <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
						 <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;">  <br> $\mathbf{P} (\Pi \ast g_\theta(z))$</div>
						 <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Median </div>
					   </div>
					 </div>

					 <div class="col">
					   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
						 <div><video class="fragment current-visible" data-autoplay data-loop data-src="/talks/assets/res_lsst.mp4" type="video/mp4" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0"/></div>
						 <img class="plain fragment " data-src="/talks/assets/rec_std.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
					   </div>

					   <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
						 <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;"> Data residuals <br> $x_n - \mathbf{P} (\Pi \ast g_\theta(z))$</div>
						 <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Standard Deviation </div>
					   </div>
					 </div>
				 </div>
				 <div class="fragment"> $\Longrightarrow$ <b class="alert">Uncertainties are fully captured by the posterior</b>.</div>
			   </section>
			 </section>



			<section>
				<section>
					<h3 class="slide-title">Linear inverse problems</h3>

					$\boxed{y = \mathbf{A}x + n}$
					<br>
					<br>
					$\mathbf{A}$ is known and encodes our physical understanding of the problem.
					<span class="fragment"><br>$\Longrightarrow$ When non-invertible or ill-conditioned, the inverse
						problem is ill-posed with no unique solution $x$</span>
					<div class="container fragment fade-up">
						<div class="col">
							<img data-src="/talks/assets/pluto_smooth.png" class="plain"></img>
							Deconvolution
						</div>
						<div class="col">
							<img data-src="/talks/assets/pluto_missing.png" class="plain"></img>
							Inpainting
						</div>
						<div class="col">
							<img data-src="/talks/assets/plutoNoise.png" class="plain"></img>
							Denoising
						</div>
					</div>

				</section>

				<section data-vertical-align-top>
					<h3 class="slide-title">A Bayesian view of the problem</h3>
					$\boxed{y = \mathbf{A}x + n}$
					<br>

					<br>
					<div class="fragment">
						$$ p(x | y) \propto p(y | x) \ p(x) $$
					</div>
					<br>

					<ul>
						<li class="fragment fade-up">$p(y | x)$ is the data likelihood, which <b>contains the
								physics</b><br>
						</li>
						<br>
						<li class="fragment fade-up">$p(x)$ is our prior knowledge on the solution.</li>
					</ul>
					<br>
					<br>
					<div class="fragment fade-up">
						With these concepts in hand, we want to estimate the Maximum A Posteriori solution:
						<br>
						<br>
						$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
						<br>
						For instance, if $n$ is Gaussian, $\hat{x} = \arg\max\limits_x \ - \frac{1}{2} \parallel y -
						\mathbf{A} x \parallel_{\mathbf{\Sigma}}^2 + \log p(x)$
					</div>
					<br>
					<div class="fragment fade-up">
						<h3>How do you choose the prior ?</h3>
					</div>
				</section>

				<section>
					<h3 class="slide-title"> Classical examples of signal priors </h3>
					<div class="container">
						<div class="col">
							Sparse
							<img data-src="/talks/assets/wavelet.png" height="400" class="plain"></img><br>
							$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
						</div>
						<div class="col">
							Gaussian
							<img data-src="/talks/assets/zknj8.jpg" height="400" class="plain"></img>
							$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
						</div>
						<div class="col">
							Total Variation
							<img data-src="/talks/assets/shepp-Logan.ppm" class="plain"></img>
							$$ \log p(x) = \parallel \nabla x \parallel_1 $$

						</div>
					</div>
				</section>

				<section data-background="/talks/assets/hsc_screen.png">
					<h2>But what about this?</h2>

				</section>
			</section>

			<section>
				<h3 class="slide-title">Getting started with Deep Priors: deep denoising example</h3>
				$$ \boxed{{\color{Orchid} y} = {\color{SkyBlue} x} + n} $$
				<div class="container">
					<div class="col">
						<div style="position:relative; width:550px; height:550px; margin:0 auto;">
							<img class="fragment current-visible plain" data-src="/talks/assets/points.png"
								style="position:absolute;top:0;left:0;" data-fragment-index="0" />
							<div class="fig-container fragment" data-file="dgm_prior_denoising.html"
								data-style="height: 550px;width: 550px;" style="position:absolute;top:0;left:0;"
								data-fragment-index="1"></div>
						</div>
						<!-- <img data-src="/talks/assets/points.png"/>
																														  															<div class="fig-container" data-file="dgm_prior_denoising.html" data-style="height: 550px;"></div> -->
					</div>

					<div class="col">
						<ul>
							<li class="fragment" data-fragment-index="0"> Let us assume we have access to examples of $
								{\color{SkyBlue} x}$ without noise.</li>
							<br>
							<li class="fragment" data-fragment-index="1">We learn the <b class="alert">distribution of
									noiseless data $\log p_\theta(x)$</b> from samples using a deep generative model.
							</li>
							<br>
							<!-- <li class="fragment"> We measure a noisy ${\color{Orchid} y}$ and we want to estimate a denoised ${\color{SkyBlue} x}$</li>
																														  																<br> -->
							<li class="fragment">The solution should lie on the <b class="alert">realistic data
									manifold</b>, symbolized by the two-moons distribution.
								<div class="fragment">
									<p> We want to solve for the Maximum A Posterior solution: </p>
									$$\arg \max - \frac{1}{2} \parallel {\color{Orchid} y} - {\color{SkyBlue} x}
									\parallel_2^2 + \log p_\theta({\color{SkyBlue} x})$$

									This can be done by <b>gradient descent</b> as long as one has access to the <b
										class="alert">score function</b> $\frac{\color{orange} d \color{orange}\log
									\color{orange}p\color{orange}(\color{orange}x\color{orange})}{\color{orange} d
									\color{orange}x}$.
								</div>
							</li>
						</ul>
					</div>
				</div>
			</section>


			 <section class="inverted" data-background="#000">
			   <h2>What do we need to do this in practice?</h2>
			 </section>

			 <section>
				<section>
					<h3 class="slide-title">Automatically Differentiable Physics thanks to AutoDiff</h3>

					<ul>
						<li class="fragment"> <b>Automatic differentiation</b> allows you to compute analytic derivatives of arbitraty expressions:<br>
							If I form the expression $y = a * x + b$, it is separated in fundamental ops:
							$$ y = u + b \qquad u = a * x $$
							then gradients can be obtained by the chain rule:
							$$\frac{\partial y}{\partial x} = \frac{\partial y}{\partial u} \frac{ \partial u}{\partial x} = 1 \times a = a$$
						</li>
						<br>
						<li class="fragment"> This is a fundamental tool in Machine Learning, and autodiff frameworks include TensorFlow and PyTorch.
						</li>
					</ul>
					<br>
					<br>
					<div class="block fragment">
						<div class="block-title">
							Enters JAX: NumPy + Autograd + GPU
						</div>
						<div class="block-content">


							<div class="container">
								<div class="col">
									<ul>
										<li>JAX follows the NumPy api!
											<pre class="python"><code data-trim data-noescape>
									import jax.numpy as np
								</code></pre>
										</li>
										<li>Arbitrary order derivatives</li>
										<li>Accelerated execution on GPU and TPU</li>

									</ul>
								</div>
								<div class="col" align="center">

									<img data-src="https://raw.githubusercontent.com/google/jax/master/images/jax_logo_250px.png" class="plain" />
								</div>
							</div>
				</section>

				<section>
					<h3 class="slide-title"> jax-cosmo: Finally a differentiable cosmology library, and it's in JAX!</h3>

					<div class="container">
						<div class="col">
							<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
							<div> <a href="https://github.com/DifferentiableUniverseInitiative/jax_cosmo/">https://github.com/DifferentiableUniverseInitiative/jax_cosmo</a>
							</div>

							<pre class="python"><code data-trim data-noescape>
								import jax.numpy as np
								import jax_cosmo as jc

								# Defining a Cosmology
								cosmo = jc.Planck15()

								# Define a redshift distribution with smail_nz(a, b, z0)
								nz = jc.redshift.smail_nz(1., 2., 1.)

								# Build a lensing tracer with a single redshift bin
								probe = probes.WeakLensing([nz])

								# Compute angular Cls for some ell
								ell = np.logspace(0.1,3)
								cls = angular_cl(cosmo_jax, ell, [probe])
							</code></pre>

							<div class="block fragment">
								<div class="block-title">
									Current main features
								</div>
								<div class="block-content">
									<ul>
										<li>Weak Lensing and Number counts probes</li>
										<li>Eisenstein & Hu (1998) power spectrum + halofit</li>
										<li>Angular $C_\ell$ under Limber approximation </li>
									</ul>
									<div class="fragment">$\Longrightarrow$ 3x2pt DES Y1 capable </div>
								</div>
							</div>

						</div>

						<div class="col">
							<img class="plain" data-src="/talks/assets/jc_vs_ccl_lensing.png" />
							<img class="plain" data-src="/talks/assets/jc_vs_ccl_clustering.png" />
							<br>
							Validating against the <a href="https://github.com/LSSTDESC/CCL">DESC Core Cosmology Library</a>
						</div>
					</div>
				</section>
				<section>
					<h3 class='slide-title'>introducing FlowPM: Particle-Mesh Simulations in TensorFlow</h3>
					<div class="container">
						<div class="col">
							<div style="float:right; font-size: 20px"> Modi, <b>Lanusse</b>, Seljak (2020)
								<a href="https://arxiv.org/abs/2010.11847"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2010.11847-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
						</div>
					</div>
					<div class='container'>
						<div class='col'>
							<img data-src="/talks/assets/github.png" class="plain" style="height:70px" />
							<img data-src="/talks/assets/TF_FullColor_Horizontal.png" class='plain' style="height: 70px;" />

							<div> <a href="https://github.com/DifferentiableUniverseInitiative/flowpm">https://github.com/DifferentiableUniverseInitiative/flowpm</a>
							</div>
							<pre class="python"><code data-trim data-noescape>
											import tensorflow as tf
											import flowpm
											# Defines integration steps
											stages = np.linspace(0.1, 1.0, 10, endpoint=True)

											initial_conds = flowpm.linear_field(32,       # size of the cube
																			   100,       # Physical size
																			   ipklin,    # Initial powerspectrum
																			   batch_size=16)

											# Sample particles and displace them by LPT
											state = flowpm.lpt_init(initial_conds, a0=0.1)

											# Evolve particles down to z=0
											final_state = flowpm.nbody(state, stages, 32)

											# Retrieve final density field
											final_field = flowpm.cic_paint(tf.zeros_like(initial_conditions),
																		   final_state[0])
										</code></pre>
							<ul>
								<li> Seamless interfacing with deep learning components
								</li>
								<li> <b class="alert">Mesh TensorFlow</b> implementation for distribution on supercomputers
								</li>
							</ul>
							<br>
							<br>
							<br>
							<br>
							<br>
						</div>

						<div class='col'>
								  <img data-src="/talks/assets/flowpm.gif"></img>
							<!-- <div class="fig-container" data-file="flowpm_16.html" data-style="height: 550px;"></div> -->
							<br>
							<br>
							<br>
							<br>
						</div>
					</div>
				</section>

					  <section>
						  <h3 class='slide-title'>Mesh FlowPM: distributed, GPU-accelerated, and automatically differentiable simulations</h3>
						  <!--
			  <img data-src="/talks/assets/mesh_flopwm.png" class="plain" style="height:450px;" /> -->

						  <div class="container">
							  <div class="col">
								  <img data-src="/talks/assets/mfpm_demo_1024.png" />
							  </div>

							  <div class="col">
								  <ul>
									  <li> We developed a <b class="alert">Mesh TensorFlow</b> implementation that can scale on GPU clusters (horovod+NCCL).
									  </li>
									  <br>
									  <br>
									  <li> For a $2048^3$ simulation:
										  <ul>
											  <li>Distributed on <b>256</b> NVIDIA V100 GPUs</li>
											  <li>Runtime: 3 mins</li>
										  </ul>
									  </li>
									  <br>
									  <br>
									  <li> Don't hesitate to reach out if you have a use case for model parallelism!<br>
										  <img data-src="/talks/assets/github.png" class="plain" style="height:70px" /><br>

										  <div> <a href="https://github.com/DifferentiableUniverseInitiative/mesh">https://github.com/DifferentiableUniverseInitiative/mesh</a>
										  </div>
									  </li>
								  </ul>
							  </div>
						  </div>
					  </section>

					  <section>
						<h3 class="slide-title"> MAP optimization in action</h3>
						$$\arg\max_z \ \log p(x_{dm} = f(z)) \ + \ p(z) $$
						<div style="float:right; font-size: 16px">credit: <a href="https://github.com/modichirag">C. Modi</a></div>
						<br>
						<div class="container">
							<div class="col fragment fade-up">
								<img data-src="/talks/assets/init_field.png" style='height:250px;' />
								<br> True initial conditions <br> $z_0$
							</div>

							<div class="col">
								<img data-src="/talks/assets/reconim_init.gif" style='height:250px;' />
								<br> Reconstructed initial conditions $z$
							</div>

							<div class="col">
								<img data-src="/talks/assets/reconim_fin.gif" style='height:250px;' />
								<br> Reconstructed dark matter distribution $x = f(z)$
							</div>

							<div class="col">
								<img data-src="/talks/assets/fin_field.png" style='height:250px;' />
								<br> Data <br> $x_{DM} = f(z_0)$
							</div>
						</div>
						<br>
						<br>

						<div class="fragment">
							Check out this blogpost for more details <br> <a href=https://blog.tensorflow.org/2020/03/simulating-universe-in-tensorflow.html>
								https://blog.tensorflow.org/2020/03/simulating-universe-in-tensorflow.html</a>
						</div>
					</section>

			 </section>

			 <section>
				<h3 class="slide-title">Scalable Techniques for Fast Posterior Inference</h3>
				<div class="container">

				  <div class="col">
					<img class="plain " data-src="/talks/assets/cosmos_gal_ground.png" style="height:200px;" />
				  </div>

				  <div class="col">
					<div style="height:200px"><video data-autoplay data-loop data-src="/talks/assets/rec_samples.mp4" type="video/mp4" style="height: 200px;"/>
					</div>
				  </div>
				  <hr style="width: 2px; height: 200px; background: white; border: none;" />
				  <div class="col">
					 <img class="fragment plain" data-fragment-index="3" data-src="/talks/assets/deep_uq_4.png" style="height:200px"/>
				  </div>

				  <div class="col">
					<img class="fragment plain"  data-fragment-index="3" data-src="/talks/assets/deep_uq_contours.png"  style="height:200px"/>
				  </div>

				  <div class="col">
					<img class="fragment plain"  data-fragment-index="3" data-src="/talks/assets/deep_uq_recs.png" style="height:200px" />
				  </div>
				</div>

					 <div style="float:right; font-size: 20px" class="fragment" data-fragment-index="3"><a href="https://arxiv.org/abs/1910.10046">(B√∂hm, Lanusse, Seljak 2019)</a></div>
					<ul>
					  <li class="fragment" data-fragment-index="1">Posterior fitting by <b class="alert">Variational Inference</b>
						<br>
						 $$ \mathrm{ELBO} = - \mathbb{D}_{KL}\left( q_\phi(z) \parallel p(z) \right) \quad + \quad \mathbb{E}_{z \sim q_{\phi}} \left[ \log p_\theta(x_n | z, \Sigma_n, \Pi_n)  \right]$$
					  </li>

					  <li class="fragment" data-fragment-index="3">Posterior fitting by <b class="alert">$EL_{2}O$</b> <br>
						$$EL_2O = \arg \min_\theta \mathbb{E}_{z \sim p^{\prime}} \sum_i \alpha_i \parallel \nabla_{z}^n \ln q_\theta(z) - \nabla_{z}^{n} \ln p(z | x_n, \Sigma_n, \Pi_n)  \parallel_2^2$$See <a href="https://arxiv.org/abs/1901.04454"> (Seljak & Yu, 2019)</a> for more details.
					  </li>
					  <br>
					  <li class="fragment" data-fragment-index="5">Or your favorite method...</li>
					</ul>
			   </section>

			   
			   <section>
				<section>
				<h3 class="slide-title">Now, you try!</h3>

					Checkout this <a href="https://colab.research.google.com/drive/1ZnFcG7ajQHAxl-tJRDl8IZCvCE0tqVnP?usp=sharing">notebook</a>
					
					<br>

					<img data-src="/talks/assets/exercise_strong_lensing.png"/>



				</section>

				<section>
					Solution is 
					<a href="https://colab.research.google.com/drive/1hXWXtQtDZ1qjaI1tw1eoMww50n-fK0Py?usp=sharing">notebook</a>.
				</section>

			   </section>













			<section>
				<h1>Part IV: Implicit Inference</h1>
				<h2>a.k.a. Simulation-Based Inference, a.k.a. Likelihood-Free Inference</h2>
			</section>



<!-- Motivation from physics -->
<section>
	<section>
		<h3 class='slide-title'> The limits of traditional cosmological inference </h3>
		<div class='container'>
			<div class='col'>
				<div style="position:relative; width:480px; height:30px; margin:0 auto;">
					<div class="fragment current-visible" style="position:absolute;top:0;" data-fragment-index="1">HSC cosmic shear power spectrum</div>
					<div class="fragment" style="position:absolute;top:0;" data-fragment-index="2">HSC Y1 constraints on $(S_8, \Omega_m)$</div>
				</div>
				<div style="position:relative; width:480px; height:300px; margin:0 auto;">
					<div class="fragment current-visible" style="position:absolute;top:0;left:0;" data-fragment-index="0">
						<img class="plain" data-src="/talks/assets/alonso_g1.png" />
						<img class="plain" data-src="/talks/assets/alonso_g2.png" />
					</div>
					<img class="fragment current-visible plain" data-src="/talks/assets/hsc_correlation_function.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
					<img class="fragment  plain" data-src="/talks/assets/hsc_constraints.png" style="position:absolute;top:0;left:0;" data-fragment-index="2" />
				</div>
				<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Hikage et al. 2018)</div>
			</div>
	
			<div class='col'>
				<ul>
					<li class="fragment" data-fragment-index="0"> Measure the ellipticity $\epsilon = \epsilon_i + \gamma$ of all galaxies<br>
						$\Longrightarrow$ Noisy tracer of the weak lensing shear $\gamma$ </li>
					<br>
					<li class="fragment" data-fragment-index="1"> Compute <b class="alert">summary statistics</b> based on 2pt functions, <br>e.g. the <b>power spectrum</b> </li>
					<br>
					<li class="fragment" data-fragment-index="2"> Run an MCMC to recover a posterior on model parameters, using an <b class="alert">analytic likelihood</b>
						$$ p(\theta | x ) \propto \underbrace{p(x | \theta)}_{\mathrm{likelihood}} \ \underbrace{p(\theta)}_{\mathrm{prior}}$$
					</li>
				</ul>
			</div>
		</div>
	
		<div class="block fragment">
			<div class="block-title">
				Main limitation: the need for an explicit likelihood
			</div>
			<div class="block-content">
				We can only compute the likelihood for <b class="alert">simple summary statistics</b> and on <b class="alert">large scales</b>
				<br>
				<br>
				<div class="fragment"> $\Longrightarrow$ We are dismissing a significant fraction of the information! </div>
			</div>
		</div>
	</section>
	
	
	<section>
		<h3 class='slide-title'>A different road: forward modeling</h3>
	
		<div class='container'>
			<div class='col'>
				<ul>
					<li> Instead of trying to analytically evaluate the likelihood $p(x | \theta)$,
						let us build a forward model of the observables.<br>
						$\Longrightarrow$ <b class="alert">The simulator becomes the physical model</b>.
					</li>
					<br>
					<li class="fragment" data-fragment-index="1"> Each component of the model is now tractable, but at the
						cost of a <b>large number of latent variables</b>.
					</li>
				</ul>
	
				<br>
				<br>
	
				<div class="block fragment">
					<div class="block-title">
						Benefits of a forward modeling approach
					</div>
					<div class="block-content">
	
						<ul>
							<li> Fully exploits the information content of the data
								(aka "full field inference").
							</li>
	
							<br>
							<li> Easy to incorporate systematic effects.
							</li>
							<br>
							<li> Easy to combine multiple cosmological probes by joint simulations.
							</li>
						</ul>
					</div>
				</div>
			</div>
	
			<div class='col'>
				<div style="position:relative; width:600px; height:600px; margin:0 auto;">
					<img class="fragment current-visible plain" data-src="/talks/assets/forward_model.png" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0" />
					<img class="fragment plain" data-src="/talks/assets/pgm_lensing.png" style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
					<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">(Schneider et al. 2015)</div>
				</div>
			</div>
		</div>
	</section>
	
	<section>
		<h3 class="slide-title">...so why is this not mainstream?</h3>
			<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:1000px;"/>
	
				<div class="r-stack">
	
					<img class="plain fragment" data-src="/talks/assets/plot_massive_nu.png" style="width:1000px;"/>
	
						<div class="block fragment">
							<div class="block-title">
								The Challenge of Simulation-Based Inference
							</div>
							<div class="block-content">
								$$ p(x|\theta) = \int p(x, z | \theta) dz = \int p(x | z, \theta) p(z | \theta) dz $$
								Where $z$ are <b>stochastic latent variables</b> of the simulator.<br><br>
								$\Longrightarrow$ This <b class="alert">marginal likelihood is intractable</b>! Hence the phrase <b>"Likelihood-Free Inference"</b>
							</div>
						</div>
	
					</div>
	</section>
	</section>
	
	<section class="inverted" data-background="#000">
		<h2> How to do inference without evaluating the likelihood of the model?</h2>
	</section>
	
	
			<section>
			 <section>
				<h3 class="slide-title">Black-box Simulators Define Implicit Distributions</h3>
				<img class="plain" data-src="/talks/assets/lfi_sim.png" style="width:750px;"/>
				<ul>
					<li>A black-box simulator <b class="alert">defines $p(x | \theta)$ as an implicit distribution</b>, you can <b>sample from it</b> but you cannot evaluate it.
					</li>
					<li class='fragment'> <b class="alert">Key Idea</b>: Use a <b>parametric distribution model $\mathbb{P}_\varphi$ to approximate the implicit distribution $\mathbb{P}$</b>.
					</li>
				</ul>
	
				<div class="container">
					<div class="col fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
						<br>
						True $\mathbb{P}$
					</div>
	
					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Samples $x_i \sim \mathbb{P}$
					</div>
	
					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Model $\mathbb{P}_\varphi$
					</div>
				</div>
			</section>
	
			<section>
				<h3 class="slide-title">Conditional Density Estimation with Neural Networks</h3>
				<ul>
					<li class="fragment fade-up"> I assume a forward model of the observations:
						\begin{equation}
						p( x ) = p(x | \theta) \ p(\theta) \nonumber
						\end{equation}
						All I ask is the ability to sample from the model, to obtain $\mathcal{D} = \{x_i, \theta_i \}_{i\in \mathbb{N}}$
					</li>
					<br>
					<li class="fragment fade-up"> I am going to assume $q_\phi(\theta | x)$ a <b>parametric conditional density</b>
					</li>
					<br>
					<li class="fragment fade-up">Optimize the parameters $\phi$ of $q_{\phi}$ according to
						\begin{equation}
						\min\limits_{\phi} \sum\limits_{i} - \log q_{\phi}(\theta_i | x_i) \nonumber
						\end{equation}
						In the limit of <b>large number of samples</b> and <b>sufficient flexibility</b>
						\begin{equation}
						\boxed{q_{\phi^\ast}(\theta | x) \approx p(\theta | x)} \nonumber
						\end{equation}
					</li>
				</ul>
	
				<div style="position:relative; height:30px; margin-left: 4em;">
					<div class="fragment current-visible" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
						optimizing a <b>parametric estimator</b> over<br> the <b>Bayesian joint distribution</b>
					</div>
					<div class="fragment" style="position:absolute;top:0;"> $\Longrightarrow$ One can asymptotically recover the posterior by
						optimizing a <b class="alert">Deep Neural Network</b> over<br> a <b class="alert">simulated training set</b>.
					</div>
				</div>
			</section>
	
			<section>
				<h3 class='slide-title'>Neural Density Estimation</h3>
				<div class="container">
					<div class="col r-stack">
						<div class="fragment current-visible" data-fragment-index="0">
						<img class="plain" data-src="/talks/assets/MDN.png" style="height:550px" />
						<br>
						<div style="float:left; font-size: 20px">Bishop (1994)</div>
					</div>
						<div class="fragment" data-fragment-index="1">
							<img data-src="/talks/assets/flow_dinh_1.png" class="plain"></img>
							<img data-src="/talks/assets/flow_dinh_2.png" class="plain"></img>
							<br>
							<div style="float:right; font-size: 20px">Dinh et al. 2016</div>
						</div>
	
					</div>
					<div class="col">
	
						<ul>
							<li class="fragment" data-fragment-index="0"> Mixture Density Networks
								\begin{equation}
								p(\theta | x) = \prod_i \pi_i(x) \ \mathcal{N}\left(\mu_i(x), \ \sigma_i(x) \right) \nonumber
								\end{equation}
							</li>
							<br>
	
							<li class="fragment fade-up" data-fragment-index="1">Conditional Normalizing Flows
								\begin{equation}
								p(\theta| x) = p_z \left( z = f^{-1}(\theta, x) \right) \left| \frac{\partial f^{-1}(\theta, x)}{\partial x} \right|
								\end{equation}
							</li>
						</ul>
					</div>
				</div>
			</section>
			</section>
	
			<section>
				<h3 class="slide-title">A variety of algorithms</h3>
				<div style="float:right; font-size: 20px">Lueckmann, Boelts, Greenberg, Gon√ßalves, Macke (2021) <a href="https://arxiv.org/abs/2101.04653"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2101.04653-B31B1B.svg" class="plain"
							style="height:25px;vertical-align:middle;" /></a></div>
				<img class="plain" data-src="/talks/assets/sbibm_comparison.png"/>
	
				<br>
				<br>
					A few important points:
					<br><br>
					<ul>
						<li class="fragment"> <b>Amortized</b> inference methods, which estimate $p(\theta | x)$, can greatly speed up posterior estimation once trained.
						</li>
						<br>
	
						<li class="fragment"> <b>Sequential</b> Neural Posterior/Likelihood Estimation methods can actively sample simulations needed to refine the inference.
						</li>
					</ul>
			</section>
	
	
			<section>
				<h3 class="slide-title">Automated Summary Statistics Extraction</h3>
			  <img class="plain" data-src="/talks/assets/lfi_sim_sum.png" />
					<ul>
						<li> Introduce a parametric function $f_\varphi$ to <b class="alert">reduce the dimensionality of the
							data while preserving information</b>.
						</li>
					</ul>
					<div class="container">
						<div class="col">
							<div class="r-stack">
								<img class="plain fragment current-visible"  data-fragment-index="0"  data-src="/talks/assets/mutual_information.png" />
								<img class="plain fragment" data-fragment-index="1"  data-src="/talks/assets/imnn.png" />
							</div>
							<div class="fragment" style="float:right; font-size: 15px"  data-fragment-index="1"> Makinen, Charnock, Alsing, Wandelt (2021) <a href="https://arxiv.org/abs/2107.07405"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2107.07405-B31B1B.svg" class="plain"
									style="height:20px;vertical-align:middle;" /></a></div>
	
						</div>
						<div class="col">
										<div class="block fragment" data-fragment-index="0">
											<div class="block-title">
												Information-based loss functions
											</div>
											<div class="block-content">
												<ul>
													<li class="fragment" data-fragment-index="0" > Variational Mutual Information Maximization
													  $$ \mathcal{L} \ = \ \mathbb{E}_{y, \theta} [ \log q_\phi(\theta | f_\varphi(x)) ] \leq  I(Y; \Theta) $$
	
															<div style="float:right; font-size: 15px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
																		style="height:20px;vertical-align:middle;" /></a></div>
													</li>
													<br><br>
													<li class="fragment" data-fragment-index="1" > Information Maximization Neural Network
														$$\mathcal{L} \ = \ - | \det \mathbf{F} |  \ \mbox{with} \ \mathbf{F}_{\alpha, \beta} = tr[ \mu_{\alpha}^t C^{-1} \mu_{\beta} ] $$
														<div style="float:right; font-size: 15px"> Charnock, Lavaux, Wandelt (2018) <a href="https://arxiv.org/abs/1802.03537"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1802.03537-B31B1B.svg" class="plain"
																style="height:20px;vertical-align:middle;" /></a></div>
													</li>
												</ul>
											</div>
										</div>
						</div>
					</div>
			</section>
	
			<section>
				<section>
					<h3 class="slide-title">Example of application: Constraining Dark Matter Substructures</h3>
					<div class="container">
						<div class="col">
							<div style="float:right; font-size: 20px">
								Brehmer, Mishra-Sharma, Hermans, Louppe, Cranmer (2019) <a href="https://arxiv.org/abs/1909.02005"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A1909.02005-B31B1B.svg" class="plain"
										style="height:25px;vertical-align:middle;" /></a></div>
						</div>
					</div>
	
					<div class="r-stack">
						<img data-src="/talks/assets/Brehmer2019a.png" style='height:500px;'/>
						<img class="fragment" data-src="/talks/assets/Brehmer2019b.png" style='height:500px;'/>
						<img class="fragment" data-src="/talks/assets/Brehmer2019.gif" style="height:500px;"/>
					</div>
	
				</section>
	
				<section>
					<h3 class="slide-title">Example of application: Infering Microlensing Event Parameters</h3>
					<div class="container">
						<div class="col">
							<div style="float:right; font-size: 20px"> Zhang, Bloom, Gaudi, <b>Lanusse</b>, Lam, Lu (2021) <a href="https://arxiv.org/abs/2102.05673"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2102.05673-B31B1B.svg" class="plain"
										style="height:25px;vertical-align:middle;" /></a></div>
						</div>
					</div>
					<div class="r-stack">
					<div class="fragment current-visible">
						<img data-src="/talks/assets/Zhang2021a.png" style="height:500px"/>
					</div>
	
					<div class="fragment">
						<img data-src="/talks/assets/Zhang2021b.png" style="height:500px"/>
					</div>
	
				</div>
	
				</section>
	
							<section>
								<h3 class="slide-title">Example of application: Likelihood-Free parameter inference with DES SV</h3>
								<div class="container">
									<div class="col">
										<div style="float:right; font-size: 20px"> Jeffrey, Alsing, <b>Lanusse</b> (2021) <a href="https://arxiv.org/abs/2009.08459"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2009.08459-B31B1B.svg" class="plain"
													style="height:25px;vertical-align:middle;" /></a></div>
									</div>
								</div>
	
								<div class="container">
									<div class="col">
										<img class="plain" data-src="/talks/assets/ks_sv.png" style="height:550px;"></img>
									</div>
	
									<div class="col r-stack">
										<div class="fragment current-visible">
										<img class="plain" data-src="/talks/assets/orthant.png" style="height:300px;" />
										<img class="plain" data-src="/talks/assets/sim_params.png" style="height:300px;" /><br>
										Suite of N-body + raytracing simulations: $\mathcal{D}$
									</div>
	
									<div class="fragment current-visible">
										<img class="plain" data-src="/talks/assets/jeffrey_model.png" style="height:550px" /><br>
									</div>
	
									<div class="fragment">
										<img class="plain" data-src="/talks/assets/jeffrey_s8.png" />
									</div>
	
									</div>
								</div>
							</section>
						</section>
	
	
						<section class="inverted" data-background="#000">
							<h2>So... what are the challenges?</h2>
						</section>
	
						<section>
						<section>
							<h3 class="slide-title">Ensuring proper calibration is difficult</h3>
							<div class="container">
								<div class="col">
									<div style="float:right; font-size: 20px">
										Hermans, Delaunoy, Rozet, Wehenkel, Louppe (2021) <a href="https://arxiv.org/abs/2110.06581"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2110.06581-B31B1B.svg" class="plain"
												style="height:25px;vertical-align:middle;" /></a></div>
								</div>
							</div>
								<img data-src="/talks/assets/Hermans2021.png" style="height:600px;"/>
						</section>
	
						<section>
							<h3 class="slide-title">Reaching stable results at low number of simulations is difficult</h3>
	
							<div class="container">
								<div class="col">
								<div class="r-stack">
									<img class="fragment fade-out" data-fragment-index="0" data-src="/talks/assets/tight_prior_lotkavolterra_c2st_1.png"/>
									<img class="fragment fade-in" data-fragment-index="0" data-src="/talks/assets/tight_prior_lotkavolterra_c2st_2.png"/>
								</div>
								<br>
								Quality of posterior estimation as a function of number of simulations on Lotka-Volterra (credit: Justine Zegal)
							</div>
	
								<div class="col">
									<ul>
											<li> Conditional density estimation remains <b>expensive in number of simulations</b>, even with sequential algorithms.
											</li>
	
											<br>
	
											<li class="fragment" data-fragment-index="0"> Possible solution is to extract more information from simulators, i.e. using
												their gradients. See <a href="https://arxiv.org/abs/1805.12244">Brehmer et al. 2020</a>.
													<img data-src="/talks/assets/gradients_benef_5.png"/>
													$\Longrightarrow$ Tied to the development of <b>automatically differentiable numerical simulations</b>.
											</li>
	
									</ul>
	
								</div>
	
							</div>
	
						</section>
						</section>

		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		} */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
				src: 'reveal.js/plugin/markdown/marked.js'
			},
			{
				src: 'reveal.js/plugin/markdown/markdown.js'
			},
			{
				src: 'reveal.js/plugin/notes/notes.js',
				async: true
			},
			{
				src: 'reveal.js/plugin/math/math.js',
				async: true
			},
			{
				src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
			},
			{
				src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
			},
			{
				src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
			},
			{
				src: 'reveal.js/plugin/highlight/highlight.js',
				async: true
			},
			]

		});
	</script>
</body>

</html>