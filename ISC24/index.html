<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Merging AI with Physical Models to Study the Universe</title>

	<meta name="description" content="ISC 24 talk, Hamburg, Germany">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-image="ISC2024_Title-Slide.jpg" data-background-size="contain" data-vertical-align-top >
				<a href="https://eiffl.github.io/talks/ISC24" style="color:crimson">eiffl.github.io/talks/ISC24</a>
				<br> <br> <br> <br> <br> <br> <br> <br><br> <br> <br> <br><br> <br><br> <br><br> <br><br> <br><br> <br><br> <br>
			</section>

			<section data-background-image="/talks/assets/WMAP_timeline_large.jpg">
				<h3 class="slide-title" style="position: absolute; top: 0">
				  the $\Lambda$CDM view of the Universe
				</h3>
				<br />
				<br />
				<div class="container">
				  <div class="col" style="flex: 0 0 40em"></div>
				  <div class="col">
					<img
					  class="plain"
					  data-src="/talks/assets/Euclid.png"
					  style="width: 240px"
					/>
	  
					<img
					  class="plain"
					  data-src="/talks/assets/roman_logo_black_w200px.png"
					  style="width: 240px"
					/>
	  
					<img
					  class="plain"
					  data-src="/talks/assets/vrro.png"
					  style="width: 240px"
					/>
				  </div>
				</div>
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
			  </section>

			<section>
				<section data-background-video="/talks/assets/animation-day-to-night.mov" data-background-video-muted>
					<h3 class='slide-title'>the Rubin Observatory Legacy Survey of Space and Time</h3>
					<div class="container">
						<div class="col">
							<ul>
								<li class="fragment fade-up"> 1000 images each night, 15 TB/night for 10 years</li>
								<br>
								<li class="fragment fade-up"> 18,000 square degrees, observed once every few days</li>
								<br>
								<li class="fragment fade-up"> Tens of billions of objects, each one observed $\sim1000$ times</li>
							</ul>
						</div>

						<div class="col">
							<video data-autoplay class="fragment fade-up" data-fragment-index="1" data-src="/talks/assets/obsim.mp4" type="video/mp4" />
						</div>
					</div>
				</section>

				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_sdss.png" data-vertical-align-top>
					<p>Previous generation survey: SDSS</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_des.png" data-vertical-align-top>
					<p>Current generation survey: DES</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_hsc.png" data-vertical-align-top>
					<p>LSST precursor survey: HSC</p>

					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
			</section>

			<section>
				<section>
					<h3 class="slide-title">We need to rethink all stages of data analysis for modern surveys</h3>

					<div class="r-stack">
						<div class="fragment current-visible" data-fragment-index="0">
							<img data-src="/talks/assets/hsc_shredded.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="0" style="float:right; font-size: 20px">Bosch et al. 2017</div>
						</div>

						<div class="fragment" data-fragment-index="1">
							<img data-src="/talks/assets/deepmass_sims_clean.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">Jeffrey, <b>Lanusse</b>, et al. 2020</div>
						</div>

					</div>
					<ul>
						<li class="fragment" data-fragment-index="0">Galaxies are no longer blobs.</li>
						<li class="fragment" data-fragment-index="1">Signals are no longer Gaussian.</li>
					</ul>
					<br>
					<br>
					<div class="fragment">$\Longrightarrow$ This is the <b class="alert">end of the analytic era</b>...</div>
				</section>

				<section>
					<h3 class="slide-title">... but the <b class="alert">beginning of the data-driven era</b></h3>
						<br>
						 <div class="container">
							 <div class="col fragment" >
									 <b>Case I</b>: Examples from data, no accurate physical model<br>
									 <img data-src="/talks/assets/real_gal-inv-small.png" style="height:350px;"/><br>
										<div style="float:right; font-size: 20px">Mandelbaum et al. 2014</div>
										<br>
							 </div>

							 <div class="col fragment">
								 <b>Case II</b>: Physical model only available as a simulator<br>
								<div>
								 <video data-autoplay style="height:350px;" data-src="/talks/assets/illustris_movie_cube_sub_frame_small.mp4" type="video/mp4" />
								 </div><br> 
									 <div style="float:right; font-size: 20px">Nelson et al. 2015</div>
									  <br> 
							 </div>
						 </div>
						 <br>
						 <div class="fragment">$\Longrightarrow$ Examples of <b class="alert">implicit distributions</b>: we have access to samples $\{x_0, x_1, \ldots, x_n \}$
							 but <b>we cannot evaluate $p(x)$</b>.
						 </div>
				</section>
			</section>

			<section class="inverted" data-background="#000">
				<h2>How can we leverage implicit distributions <br> for <b>physical Bayesian inference</b>?</h2>
			</section>

			<section>
				<h2>High-Dimensional Bayesian Inference for <br>Inverse Problems Under Implicit Priors</h2>
				<a href="https://arxiv.org/abs/2011.08271"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2011.08271-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<a href="https://arxiv.org/abs/2011.08698"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<hr>
				<div class="container">
					<div class="col">
						<div align="left" style="margin-left: 20px;">
							<!-- <h3>Work in collaboration with: <br>
								Benjamin Remy (now in Princeton), Zaccharie Ramzi (now at Meta)
							</h3>
							<img data-src="/talks/assets/benjamin.png" style='width:200px; height:200px;object-fit: cover;'></img>
							<img data-src="http://www.cosmostat.org/wp-content/uploads/2019/03/Portrait-2-1600x2000.jpg" style='width:200px; height:200px;object-fit: cover;'></img> -->

							<br>

							$\Longrightarrow$ <b class="alert">Learn complex priors</b> by Neural Score Estimation and <b class="alert">sample from posterior</b> with gradient-based MCMC.
						</div>
					</div>
					<div class="col">
						<img class="plain" data-src="/talks/assets/cropped.gif" style="width:450px;" />
					</div>
				</div>
				<br>
			</section> 

			<section>
				<section data-background-image="/talks/assets/gravitational-lensing-diagram.jpg">
					<h3 class="slide-title">Let's set the stage: Gravitational lensing</h3>
					<div class="fragment fade-up">
						<img class="plain" data-src="/talks/assets/great.jpg" />

						<div class="block ">
							<div class="block-title">
								Galaxy shapes as estimators for gravitational shear
							</div>
							<div class="block-content">
								$$ e = \gamma + e_i \qquad \mbox{ with } \qquad e_i \sim \mathcal{N}(0, I)$$
								<ul>
									<li> We are trying the measure the <b class="alert"> ellipticity $e$</b> of
										galaxies as an estimator for the <b class="alert">gravitational shear $\gamma$ </b>
									</li>
								</ul>
							</div>
						</div>
					</div>
				</section> 

				<section>
					<h3 class="slide-title">Gravitational Lensing as an Inverse Problem</h3>
					<div class="container">
						<div class="col">
							Shear <b class="alert">$\gamma$</b><br>
							<img data-src="/talks/assets/shear_cat1.png" style="width:450px;"></img>
						</div>

						<div class="col fragment fade-up">
							Convergence <b class="alert">$\kappa$</b><br>
							<img data-src="/talks/assets/kappa.png" style="width:450px;"></img>
						</div>
					</div>

					<div style="position:relative; width:1000px; height:100px; margin:0 auto;">
						<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
							$$\gamma_1 = \frac{1}{2} (\partial_1^2 - \partial_2^2) \ \Psi \quad;\quad \gamma_2 = \partial_1 \partial_2 \ \Psi \quad;\quad \kappa = \frac{1}{2} (\partial_1^2 + \partial_2^2) \ \Psi$$
						</div>
						<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
							$$\boxed{\gamma = \mathbf{P} \kappa}$$
						</div>
					</div>
				</section>

				</section> 

					<section>
						<section data-vertical-align-top>
							<h3 class="slide-title">What Would a Bayesian Do?</h3>
							$\boxed{y = \mathbf{A}x + n}$
							<br>
							<br>
							The Bayesian view of the problem:
							<br>
							$$ p(x | y) \propto p(y | x) \ p(x) $$
							<ul>
								<li class="fragment fade-up">$p(y | x)$ is the data <b>likelihood</b>, which <b class="alert">contains the physics</b><br>
								</li>
								<br>
								<li class="fragment fade-up">$p(x)$ is the <b>prior</b> knowledge on the solution.</li>
							</ul>
							<br>
							<br>
							<div class="fragment fade-up">
								<ul>With these concepts in hand we can:
									<br>
									<li class="fragment">Estimate for instance the <b>Maximum A Posteriori</b> solution:
										<br>
										$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
									</li>
									<li class="fragment">Estimate from the <b>full posterior p(x|y)</b> with MCMC or Variational Inference methods.
									</li>
								</ul>
							</div>
							<br>
							<div class="fragment fade-up">
								<h3>How do you choose the prior ?</h3>
							</div>
						</section>

						<section>
							<h3 class="slide-title"> Classical examples of signal priors </h3>
							<div class="container">
								<div class="col">
									Sparse
									<img data-src="/talks/assets/wavelet.png" height="400" class="plain"></img><br>
									$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
								</div>
								<div class="col">
									Gaussian
									<img data-src="/talks/assets/zknj8.jpg" height="400" class="plain"></img>
									$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
								</div>
								<div class="col">
									Total Variation
									<img data-src="/talks/assets/shepp-Logan.ppm" class="plain"></img>
									$$ \log p(x) = \parallel \nabla x \parallel_1 $$

								</div>
							</div>
						</section>

						<section data-background="/talks/assets/convergence.png">
							<h2>But what about this?</h2>
						</section>
					</section>


					<section>
						<section>
							<h3 class="slide-title"> The answer is: Deep Generative Modeling</h3>
							<br>
							<ul>
								<li>The goal of generative modeling is to <b>learn an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
									from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
								</li>
								<br>
								<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
									that tries to be close to $\mathbb{P}$.
								</li>
							</ul>
		
							<br>
							<div class="container">
								<div class="col fragment fade-up">
									<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
									<br>
									True $\mathbb{P}$
								</div>
		
								<div class="col  fragment fade-up">
									<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
									<br>
									Samples $x_i \sim \mathbb{P}$
								</div>
		
								<div class="col  fragment fade-up">
									<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
									<br>
									Model $\mathbb{P}_\theta$
								</div>
							</div>
							<br>
							<br>
							<ul>
								<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
								</li>
							</ul>
						</section>
		
						<section>
							<h3 class="slide-title"> The evolution of generative models </h3>
			
							<br> 
							<div class='container'>
								<div class='col'>
									<div style="position:relative; width:500px; height:600px; margin:0 auto;">
										<img class="fragment current-visible plain" data-src="/talks/assets/DBN.png"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="0" />
										<img class="fragment current-visible plain" data-src="/talks/assets/vae_faces.jpg"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
										<img class="fragment current-visible plain" data-src="/talks/assets/gan-samples-1.png"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="2" />
										<img class="fragment plain" data-src="/talks/assets/karras2017.png"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
										<img class="fragment plain" data-src="https://preview.redd.it/de6cf3omoqpa1.jpg?width=960&format=pjpg&auto=webp&v=enabled&s=0f4641ba72ad4c7ef106efafc539804cf7247410"
										style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="4" />
									</div>
								</div>
			
								<div class='col'>
									<ul>
										<li class="fragment" data-fragment-index="0"> Deep Belief Network <br> (Hinton et al. 2006)
										</li>
										<br>
										<li class="fragment" data-fragment-index="1"> Variational AutoEncoder <br> (Kingma & Welling
											2014) </li>
										<br>
										<li class="fragment" data-fragment-index="2"> Generative Adversarial Network <br>
											(Goodfellow et al. 2014)</li>
										<br>
										<li class="fragment" data-fragment-index="3"> Wasserstein GAN <br> (Arjovsky et al. 2017)
										</li>
										<br>
										<li class="fragment" data-fragment-index="4"> Midjourney v5 Guided Latent Diffusion (2023)
										</li>
									</ul>
									<br>
									<br>
									<div class="fragment">$\Longrightarrow$ For all intents and purposes <b class="alert">we can now
										model arbitrary distributions</b> even in extremely high dimensions.
									</div>
								</div>
							</div>
							<br> <br> <br>
						</section>
					</section>

						  <section>
							<h3 class="slide-title">Writing down the convergence map log posterior</h3>
			
								$$ \log p( \kappa | e) = \underbrace{\log p(e | \kappa)}_{\simeq -\frac{1}{2} \parallel e - P \kappa \parallel_\Sigma^2} + \log p(\kappa) +cst $$
			
								<ul>
									<li> The likelihood term is <b class="alert">known analytically</b>, given to us by the physics of gravitational lensing.
									</li>
			
									<li class="fragment fade-up"> There is <b class="alert">no close form expression for the prior</b> on dark matter maps $\kappa$.
										<br> However:
										<ul>
											<li class='fragment'> <b>We do have access to samples of full  <b class="alert">implicit</b> prior</b> through simulations: $X = \{x_0, x_1, \ldots, x_n \}$ with $x_i \sim \mathbb{P}$
												<img data-src='/talks/assets/plot_massive_nu.png' />
											</li>
										</ul>
									</li>
								</ul>
								<div class="fragment">$\Longrightarrow$ Our strategy: <b class="alert">Learn the prior from simulation</b>,
									and then <b class="alert">sample the full posterior</b>.</div>
					  </section>
			
					  <section>
					  <section>
						<h3 class="slide-title">The score is all you need!</h3>
						<br>
						<div class="container">
							<div class="col">
								<ul>
									<li> Whether you are looking for the MAP or sampling with HMC or MALA, you
										<b class="alert">only need access to the score</b> of the posterior:
										$$\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x \color{orange}|\color{orange} y\color{orange})}{\color{orange}
										d
										\color{orange}x}$$
										<ul>
											<li>Gradient descent: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) $</li>
											<li>Langevin algorithm: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) + \sqrt{2\tau} n_t$ </li>
										</ul>
									</li>
									<br>
								</ul>
							</div>
							<div class="col">
								<img data-src="/talks/assets/score_two_moons.png"></img>
							</div>
						</div>
						<br>
						<br>
						<ul>
							<li > The score of the full posterior is simply:
								$$\nabla_x \log p(x |y) = \underbrace{\nabla_x \log p(y |x)}_{\mbox{known explicitly}} \quad + \quad \underbrace{\nabla_x \log p(x)}_{\mbox{known implicitly}}$$
								$\Longrightarrow$ "all" we have to do is <b class="alert">model/learn the score of the prior</b>.
							</li>
						</ul> 
					</section>
			
					<section>
						<h3 class="slide-title">Neural Score Estimation by Denoising Score Matching (Vincent 2011)</h3>
						<ul>
							<li><b>Denoising Score Matching</b>: An optimal <b class="alert">Gaussian denoiser learns the score</b> of a given distribution.
								<ul>
									<li class="fragment fade-up"> If $x \sim \mathbb{P}$ is corrupted by additional Gaussian noise $u \in \mathcal{N}(0, \sigma^2)$ to yield
										$$x^\prime = x + u$$
									</li>
									<li class="fragment fade-up"> Let's consider a denoiser $r_\theta$ trained under an $\ell_2$ loss:
										$$\mathcal{L}=\parallel x - r_\theta(x^\prime, \sigma) \parallel_2^2$$
									</li>
									<li class="fragment fade-up"> The optimal denoiser $r_{\theta^\star}$ verifies:
										$$\boxed{\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma) = \boldsymbol{x}' + \sigma^2 \nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}$$
									</li>
								</ul>
							</li>
						</ul>
			
						<div class="fragment fade-up">
							<div class="container">
								<div class="col">$\boldsymbol{x}'$
								</div>
								<div class="col">$\boldsymbol{x}$
								</div>
								<div class="col">$\boldsymbol{x}'- \boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
								</div>
								<div class="col">$\boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
								</div>
							</div>
							<img data-src="/talks/assets/denoised_mnist.png" style='width:1200px;'></img>
						</div>
					</section>
					</section>

<section>
	<h3 class="slide-title">Example of Annealed Hamiltonian Monte-Carlo Sampling</h3>
	<img data-src="/talks/assets/hmc-annealing.gif" style="margin-top: -20px"/>
	<div style="margin-top: -50px;">$$\nabla_x \log q_{\sigma^2}(x |y) = \underbrace{\nabla_x \log p_{\sigma^2}(y |x)}_{\mbox{known explicitly}} \quad + \quad \underbrace{s_
		{\theta}(x, \sigma^2)}_{\mbox{learned by score matching}}$$</div>
</section>

						<section>
							<section>
								<h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>
								<div class="container">
									<div class="col">
											<div style="float:right; font-size: 20px"> Remy, Lanusse, et al. (2023) <a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
									</div>
								</div>
								<div class="container">
									<div class="col">
										<img data-src='/talks/assets/ref_ktng.png' style="width:350px; height:350px;" />
										<br>
										True convergence map
									</div>
									<div class="col">
										<div class="block-content">
											<div style="position:relative; width:350px; height:350px; top:10px; left:40px;">
												<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="0">
													<img data-src='/talks/assets/ks_ktng.png' style="width:350px; height:350px;" />
												</div>
												<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="1">
													<img data-src='/talks/assets/wiener_ktng.png' style="width:350px; height:350px;" />
												</div>
												<div class="plain fragment" style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="2">
													<img data-src='/talks/assets/mean_post_ktng.png' style="width:350px; height:350px;" />
												</div>
											</div>
											<div class="block-content">
												<div style="position:relative; width:350px; height:20px; top:50px; left:10px;">
													<div class="fragment current-visible " data-fragment-index="0" style="position:absolute;top:0;left:0;width:350px;">Traditional Kaiser-Squires</div>
													<div class="fragment current-visible " data-fragment-index="1" style="position:absolute;top:0;left:0;width:350px;">Wiener Filter</div>
													<div class="fragment" data-fragment-index="2" style="position:absolute;top:0;left:0;width:350px;">Posterior Mean (ours)</div>
												</div>
											</div>
											<br>
											<br>
										</div>
			
									</div>
									<div class="col fragment">
										<img data-src='/talks/assets/cropped.gif' style="width:350px; height:350px;" />
										<br>
										Posterior samples
									</div>
								</div>
		
							</section>

							<section>
								<h3 class="slide-title">Reconstruction of the <b class="alert">HST/ACS COSMOS field</b></h3>
			
								<ul>
								<li> COSMOS shear data from <a href=https://ui.adsabs.harvard.edu/abs/2010A%26A...516A..63S/abstract>Schrabback et al. 2010</a>
								</li>
								<li> Prior learned from $\kappa$-TNG simulation from <a href=https://arxiv.org/abs/2010.09731>Osato et al. 2021</a>.
								</li>
							</ul>
								<br>
								<div class="container">
									<div class="col">
										<div class="block-content">
											<div style="position:relative; height:570px; top:0px; left:0px;">
												Massey et al. (2007)
												<img data-src="/talks/assets/massey.png" style="height:500px;"></img>
											</div>
										</div>
									</div>
			
									<div class="col">
										<div class="block-content">
											<div style="position:relative; height:570px; top:0px; left:0px;">
												<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
													Remy et al. (2023) <b class="alert">Posterior mean</b>
													<img data-src='/talks/assets/remy.png' style="height:500px;" />
												</div>
			
												<div class="plain fragment" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
													Remy et al. (2023) <b class="alert">Posterior samples</b>
													<img data-src='/talks/assets/cosmos_samples.gif' style="height:500px;" />
												</div>
			
											</div>
										</div>
									</div>
			
								</div>
							</section>
						</section>
			
						<section>
							<h3 class="slide-title">Other Example of Inverse Problem under Implicit priors: MRI</h3>
							<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
										src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
							</div>
							<br>
							<br>
							$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
							<div><video data-autoplay loop="loop" data-src="/talks/assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
							</div>
							<br>
		
							<br>
		
							<br>
		
							<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
						</section>

						<section>
							<h2>AstroCLIP: Cross-Modal Pretraining for Astronomical Foundation Models</h2>
								<a href="https://arxiv.org/abs/2310.03024"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2310.03024-B31B1B.svg" class="plain" style="height:25px;" /></a>
							<hr>
							<div class="container">
								<div class="col">
									<div align="left" style="margin-left: 20px;">
										<img data-src="/talks/assets/polymathic_logo.png"> 
										<br>
			
										$\Longrightarrow$ Build <b class="alert">informative embeddings of multimodal data</b> in a purely self-supervised way.
									</div>
								</div>
								<div class="col">
									<img class="plain" data-src="/talks/assets/astroclip1.png" style="width:450px;" />
								</div>
							</div>
							<br>
						</section> 

						<section>
						<section>
							<h3 class="slide-title">The Deep Learning Boom in Astrophysics</h3>
				  
							<div class="r-stack">
							<div class="container">
							  <canvas data-chart="bar" style="height: 600px;">
								<!--
										  {
										   "data": {
											  "labels": ["2012", "2013", "2014","2015", "2016" ,"2017", "2018", "2019", "2020", "2021", "2022", "2023"],
											  "datasets": [
											   {
												  "data":[ 21, 15, 17, 22, 31, 71, 113, 231, 322, 422, 537, 646 ],
												  "label":"Deep Learning || CNN || Neural Network ","backgroundColor":"#A63446"
											  }
											  ]
										   },
										   "options": { "responsive": "true",
									  "scales": {
											  "yAxes": [{
													  "type": "linear"
											  }]
									  }
										  }
										  }
										  --> </canvas
							  ><br />
							</div>
							<div class="fragment">
								The <b>vast majority</b> of these results has relied on <br><b>supervised learning</b> and <b>networks trained from scratch</b>.
							</div>
							</div>
							<div>
							  <b>astro-ph</b> abstracts mentioning <b>Deep Learning</b>,
							  <b>CNN</b>, or <b>Neural Networks</b>
							</div>
						  </section>
							</section>

							<section>
							<section>
								<h3 class="slide-title">The Rise of The Foundation Model Paradigm</h3>

								<div class="container">
								<div class="col">
									<div class="sl-block" data-block-type="text" style="width: 640px; left: 0px; top: 84.0065px; height: auto;" data-block-id="492b3f915d09822a197a5b80ce451809">
										<div class="sl-block-content" data-placeholder-tag="p" data-placeholder-text="Text" style="z-index: 11;">
											<ul>
												<li>
													<strong>Foundation Model approach</strong>
													<ul>
														<li>
															<strong class="alert">Pretrain</strong>&nbsp;models on pretext tasks, without supervision, on very large scale datasets.</li>
															<br>
														<li class="fragment" data-fragment-index="1">
															<strong class="alert">Adapt</strong>&nbsp;pretrained models to downstream tasks.&nbsp;</li>
															<br>
														<li class="fragment" data-fragment-index="2">
															<strong class="alert">Combine</strong>&nbsp;pretrained modules in more complex systems.</li>
													</ul>
												</li>
											</ul>
										</div>
									</div>
									<img class="plain" data-src="/talks/assets/pretrain.png" ><p><span style="font-size:0.9em"><a href="https://arxiv.org/abs/2111.06377" target="_blank">He et al. 2021</a></span></p>
				
								</div>
								<div class="col">
									<div class="sl-block" data-block-type="image" style="width: 640px; height: 456.013px; left: 640px; top: 84.0065px; min-width: 1px; min-height: 1px;" data-name="image-c5ccee" data-block-id="96bbfd152ee5d8305589b472d9775516">
										<div class="sl-block-content" style="z-index: 12;"><img class="" data-natural-width="1214" data-natural-height="865" data-lazy-loaded="" src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/10886697/pasted-from-clipboard.png"></div>
									</div>
									<div class="sl-block" data-block-type="text" style="height: auto; width: 300px; left: 980px; top: 576px;" data-name="text-2a10e5" data-block-id="cb4de175c77e31210c4d2f3a0ed64477">
										<div class="sl-block-content" data-placeholder-tag="p" data-placeholder-text="Text" style="z-index: 13;">
											<p><span style="font-size:0.9em"><a href="https://arxiv.org/abs/2108.07258" target="_blank">Bommasani et al. 2021</a></span></p>
										</div>
									</div>
								</div>
								</div>	

							</section>

							<section>
								<h3 class="slide-title">The Advantage of Scale of Data and Compute</h3>

								<div class="container">
									<div class="col">
										<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/11185011/pasted-from-clipboard.png" style="height: 400px;"></img>
										<p><span style="font-size:0.9em"><a href="https://arxiv.org/abs/2201.03545" target="_blank">Liu et al. 2022</a></span></p>
									</div>
									<div class="col fragment">
										<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/11185470/pasted-from-clipboard.png" style="height: 400px;"></img>
										<p><span style="font-size:0.9em"><a href="https://arxiv.org/abs/2106.04560" target="_blank">Zhai et al. 2022</a></span></p>
									</div>
								</div>
							</section>
						</section>

						<section>
							<h3 class="slide-title">What this new paradigm could mean for us astrophysicists</h3>
							<br>
							<ul>
								<li>
									<strong class="alert">Never have to retrain my own neural networks</strong>&nbsp;from scratch
									<ul>
										<li><span style="font-size:0.9em">Existing pre-trained models would already be near optimal, no matter the task at hand</span></li>
										<li>
											<span style="font-size:0.9em">Saves a lot of time and energy</span><br>
											&nbsp;</li>
									</ul>
								</li>
								<br>
								<li class="fragment" data-fragment-index="0">Practical large scale Deep Learning even in <strong class="alert">very few example regime</strong>
									<ul>
										<li class="visible"><span style="font-size:0.9em">Searching for very rare objects in large surveys becomes possible</span></li>
										<li>
											<span style="font-size:0.9em">Pretraining on data itself ensures that all sorts of image artifacts are already folded in the training. </span><br>
											&nbsp;</li>
									</ul>
								</li>
								<br>
								<li class="fragment" data-fragment-index="1">If the information is embedded in a space where it becomes linearly accessible, &nbsp;<strong class="alert">very simple analysis tools are enough </strong>for downstream analysis
									<ul>
										<li><span style="font-size:0.9em">In the future, survey pipelines may add vector embedding of detected objects into catalogs, these would be enough for most tasks, without the need to go back to pixels</span></li>
									</ul>
								</li>
							</ul>
						</section>

						<section>
							<h3 class="slide-title">From CLIP to AstroCLIP</h3>

							<div class="container">
								<div class="col">
									<img data-src="/talks/assets/clip.png" />
									<img data-src="/talks/assets/clip_loss.png" />
									<p>Contrastive Language Image Pretraining (CLIP)<br>
										<a href="https://arxiv.org/abs/2103.00020" target="_blank">(Radford et al. 2021)</a></p>
								</div>

								<div class="col fragment">
									<img data-src="/talks/assets/astroclip1.png" style="width:450px;" />

									<ul>
										<li>We use <b>spectra</b> and multi-band <b>images</b> as two different views for the same underlying physical object.</li>
										<br>
										<li>The image encoder is <b>DINOv2</b> (ViT)  model pretrained on 70 million images before InfoNCE alignment.</li>
									</ul>
								</div>
							</div>

						</section>

						<section>
						<section>
							<h3 class="slide-title">Similarity-Based Retrieval</h3>

							<img data-src="/talks/assets/query-retrieval.png"/>
						</section>

						<section>
							<h3 class="slide-title"> Example use-case: Finding rare astronomical objects</h3>
							<div class="container">
								<div class="col">
										<div style="float:right; font-size: 20px"> Desmons, Lanusse, Brough (2023) <a href="https://arxiv.org/abs/2308.07962"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2308.07962-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
								</div>
							</div>
							<img data-src="/talks/assets/similarity_tidal.png" style="width: 900px;"/>
						</section>
						</section>

						<section>
							<h3 class="slide-title">Downstream applications with no fine-tuning</h3>

							<div class="container">
								<div class="col">
									<img data-src="/talks/assets/table_predictions.png" />
									<br> <div>$R^2$ of regression of physical galaxy properties</div>
								</div>

								<div class="col fragment">
									<img data-src="/talks/assets/astroclip_classification.png" />
									<br> <div>Classification accuracy on Galaxy morphology classification</div>
								</div>
							</div>
						</section>
						
									<section>
										<h3 class="slide-title"> Conclusion </h3>
										<div class="block ">
											
											<div class="block-title">
												Merging Deep Learning with Physical Models
											</div>
											<div class="block-content">
												$\Longrightarrow$ Makes <b>Bayesian inference possible</b> at scale and with non-trivial models and complex data!
												<br>
												<br>
												<ul>

												<li class="fragment"> Complement known physical models with data-driven components
												</li>
													<br>

													<li class="fragment"> Foundation Model paradigm provides a way to leverage large scale data <br> Their representation
														can be calibrated against a physical model.
													</li>
													<br>
												</ul>
											</div>
										</div>
										<br>

										<br>
										<p class="fragment">Thank you ! </p>
										<br> <br> <br>
									</section>


		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		}  */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});

		// utility function that excepts a fragmentshown or fragmenthidden event and returns a boolean indicating whether or not
		// the fragment is a video
		const isVideoFragment = (event) => event.fragment.nodeName === 'VIDEO';

		// Listens for the 'fragmentshown' event; if the fragment being shown is a video, play the video
		Reveal.addEventListener('fragmentshown', (event) => {
		if (isVideoFragment(event)) {
			event.fragment.play();
		}
		});

		// Listens for the 'fragmenthidden' event; if the fragment being hidden is a video, pause the video
		Reveal.addEventListener('fragmenthidden', (event) => {
		if (isVideoFragment(event)) {
			event.fragment.pause();
		}
		});
	</script>
</body>

</html>
