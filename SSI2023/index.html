<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Taming Implicit Distributions with Generative Modeling</title>

	<meta name="description" content="SLAC Summer Institute, August 2023">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">

			<section data-background-iframe="background.html">
				<div class="container">
					<div class="title" style="border-radius: 20px; background-color:rgba(0, 0, 0, 0.4);">
						<h1>Taming Implicit Distributions with Generative Modeling</h1>
						<h3>SLAC Summer Institute, August 2023</h3>
					</div>
				</div>
				<hr>
				<div style="border-radius: 20px; background-color:rgba(0, 0, 0, 0);">
					<div class="container">
						<div class="col">
							<div align="left" style="margin-left: 20px;">
								<h2>Fran√ßois Lanusse</h2>
								<br>
								<img src="/talks/assets/CosmoStatDarkBK.png" class="plain"></img>
								<br>
							</div>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<br>
							<img src="/talks/assets/logo_cnrs.png" class="plain" height="150"></img>
						</div>

						<div class="col">
							<br>
							<br>
							<br>
							<img src="/talks/assets/aim.png" class="plain" height="150"></img>
						</div>
					</div>
					<div> slides at <a
							href="https://eiffl.github.io/SSI2023">eiffl.github.io/talks/SSI2023</a>
					</div>
				</div>
			</section>

			<!-- <section
            data-background-image="/talks/assets/WMAP_timeline_large.jpg"
          >
            <h3 class="slide-title" style="position: absolute; top: 0">
              a new era of wide-field galaxy surveys 
            </h3>
            <br />
            <br />
            <div class="container">
              <div class="col" style="flex: 0 0 40em"></div>
              <div class="col">
                <img
                  class="plain"
                  data-src="/talks/assets/Euclid.png"
                  style="width: 240px"
                />

                <img
                  class="plain"
                  data-src="/talks/assets/roman_logo_black_w200px.png"
                  style="width: 240px"
                />

                <img
                  class="plain"
                  data-src="/talks/assets/vrro.png"
                  style="width: 240px"
                />
              </div>
            </div>
            <br />
            <br />
            <br />
            <br />
            <br />
            <br />
            <br />
            <br />
            <br />
            <br />
            <br />
            <br />
          </section> -->
		  
		<section>
				<section data-background-video="/talks/assets/animation-day-to-night.mov" data-background-video-muted>
					<h3 class='slide-title'>the Rubin Observatory Legacy Survey of Space and Time</h3>
					<div class="container">
						<div class="col">
							<ul>
								<li class="fragment fade-up"> 1000 images each night, 15 TB/night for 10 years</li>
								<br>
								<li class="fragment fade-up"> 18,000 square degrees, observed once every few days</li>
								<br>
								<li class="fragment fade-up"> Tens of billions of objects, each one observed $\sim1000$ times</li>
							</ul>
						</div>

						<div class="col">
							<video data-autoplay class="fragment fade-up" data-fragment-index="1" data-src="/talks/assets/obsim.mp4" type="video/mp4" />
						</div>
					</div>
				</section>

				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_sdss.png" data-vertical-align-top>
					<p>Previous generation survey: SDSS</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_des.png" data-vertical-align-top>
					<p>Current generation survey: DES</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_hsc.png" data-vertical-align-top>
					<p>LSST precursor survey: HSC</p>

					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
			</section>

			<section>
			<section>
				<h3 class="slide-title">We need to rethink all stages of data analysis</h3>

					<div class="r-stack">
						<div class="fragment current-visible" data-fragment-index="0">
							<img data-src="/talks/assets/hsc_shredded.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="0" style="float:right; font-size: 20px">Bosch et al. 2017</div>
						</div>

						<div class="fragment current-visible" data-fragment-index="1">
							<img data-src="/talks/assets/deepmass_sims_clean.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">Jeffrey, <b>Lanusse</b>, et al. 2020</div>
						</div>

						<div class="fragment" data-fragment-index="2">
							<img data-src="/talks/assets/scattering_ps.png" style="height:300px;"/>
							<img class="fragment"  data-src="/talks/assets/ScatteringTransform.png" style="height:400px;"/> <br>
							<div class="fragment" data-fragment-index="2" style="float:right; font-size: 20px">Cheng et al. 2020</div>
						</div>
					</div>
					<ul>
						<li class="fragment" data-fragment-index="0">Galaxies are no longer blobs.</li>
						<li class="fragment" data-fragment-index="1">Signals are no longer Gaussian.</li>
						<li class="fragment" data-fragment-index="2">Cosmological likelihoods are no longer tractable.</li>
					</ul>
					<br>
					<br>
					<div class="fragment">$\Longrightarrow$ This is the <b class="alert">end of the analytic era</b>...</div>
			</section>

			<section>
				<h3 class="slide-title">... but the <b class="alert">beginning of the data-driven era</b></h3>
					<br>
					 <div class="container">
						 <div class="col fragment">
								 <b>Case I</b>: Examples from data, no accurate physical model<br>
								 <img data-src="/talks/assets/real_gal-inv-small.png" style="height:400px;"/><br>
									<div style="float:right; font-size: 20px">Mandelbaum et al. 2014</div>
									<br>
						 </div>

						 <div class="col fragment">
							 <b>Case II</b>: Physical model only available as a simulator<br>
							 <img data-src='/talks/assets/convergence.png' style="height:400px;"/><br>
								 <div style="float:right; font-size: 20px">Osato et al. 2020</div>
								 <br>
						 </div>
					 </div>
					 <br>
					 <div class="fragment">$\Longrightarrow$ Examples of <b class="alert">implicit distributions</b>: we have access to samples $\{x_0, x_1, \ldots, x_n \}$
						 but <b>we cannot evaluate $p(x)$</b>.
					 </div>
			</section>
		</section>


		<section class="inverted" data-background="#000">
			<h2>How can we leverage implicit distributions <br> for <b>physical</b> Bayesian inference?</h2>
		</section>

		<section>
			<section>
				<h3 class="slide-title"> The answer is: Deep Generative Modeling</h3>
				<br>
				<ul>
					<li>The goal of generative modeling is to <b>learn an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
						from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
					</li>
					<br>
					<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
						that tries to be close to $\mathbb{P}$.
					</li>
				</ul>

				<br>
				<div class="container">
					<div class="col fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
						<br>
						True $\mathbb{P}$
					</div>

					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Samples $x_i \sim \mathbb{P}$
					</div>

					<div class="col  fragment fade-up">
						<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
						<br>
						Model $\mathbb{P}_\theta$
					</div>
				</div>
				<br>
				<br>
				<ul>
					<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
					</li>
				</ul>

			</section>
		</section>


		<section>
			<h3 class="slide-title"> In the rest of this talk</h3>
			<br>
			<b class="alert">Main idea</b>: <b>Use generative models to complement physical models with implicit distributions, and perform inference in a Bayesian context.</b>
			<div class=container>
				<div class="col">
				  <div class="fig-container" data-file="venn.html" data-style="height: 600px;"></div>
				</div>	
				<div class="col fragment">
					Several examples today:
					<br>
					<br>
			<ul>
				<div class="fragment grow">
					<li> Implicit Distributions as Priors in <br> Inverse Problems</li>
					<br>
				</div>
				<li class="fragment grow" > Hybrid Physical/Data-Driven Hierarchical <br> Bayesian Models </li>
				<br>
			</ul>
		</div>
		</div>
		<br>
		<br>
		</section>	

			<section>
				<h1>Implicit Distributions as Priors in <br> Inverse Problems</h1>
				<hr>
			</section>

			<section>
				<section data-background="/talks/assets/gal_hsc.png">
							<div class="fragment">
								<div style="float:right; font-size: 20px">Branched GAN model for deblending <a href="https://arxiv.org/abs/1810.10098">(Reiman & G√∂hre, 2018)</a></div>
		
								<img class="plain" data-src="/talks/assets/Reiman2018_1.png" />
							</div>
		
							<div class="block fragment">
								<div class="block-title">
									The issue with using deep learning as a <i>black-box</i>
								</div>
								<div class="block-content">
									<ul>
										<li> No explicit control of noise, PSF, number of sources.
											<ul>
												<li> Model would have to be retrained for all observing configurations
												</li>
											</ul>
										</li>
										<br>
										<li class="fragment"> No guarantees on the network output (e.g. flux preservation, artifacts)
										</li>
										<br>
										<li class="fragment"> No proper uncertainty quantification.
										</li>
									</ul>
								</div>
							</div>
						</section>
		
						<section>
							<img class="plain" data-src="/talks/assets/Reiman2018_3.png" />
						</section>
					</section> 


				<section>
					<h3 class="slide-title">Linear inverse problems</h3>

					$\boxed{y = \mathbf{A}x + n}$
					<br>
					<br>
					$\mathbf{A}$ is known and encodes our physical understanding of the problem.
					<span class="fragment"><br>$\Longrightarrow$ When non-invertible or ill-conditioned, the inverse
						problem is ill-posed with no unique solution $x$</span>
					<div class="container fragment fade-up">
						<div class="col">
							<img data-src="/talks/assets/pluto_smooth.png" class="plain"></img>
							Deconvolution
						</div>
						<div class="col">
							<img data-src="/talks/assets/pluto_missing.png" class="plain"></img>
							Inpainting
						</div>
						<div class="col">
							<img data-src="/talks/assets/plutoNoise.png" class="plain"></img>
							Denoising
						</div>
					</div>
				</section>

				<section>
				<section data-vertical-align-top>
					<h3 class="slide-title">What Would a Bayesian Do?</h3>
					$\boxed{y = \mathbf{A}x + n}$
					<br>

					<br>
					<div class="fragment">
						$$ p(x | y) \propto p(y | x) \ p(x) $$
					</div>
					<br>

					<ul>
						<li class="fragment fade-up">$p(y | x)$ is the data likelihood, which <b class="alert">contains the
								physics</b><br>
						</li>
						<br>
						<li class="fragment fade-up">$p(x)$ is our <b>prior knowledge</b> on the solution.</li>
					</ul>
					<br>
					<br>
					<div class="fragment fade-up">
						With these concepts in hand, we want to estimate the Maximum A Posteriori solution:
						<br>
						<br>
						$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
						<br>
						For instance, if $n$ is Gaussian, $\hat{x} = \arg\max\limits_x \ - \frac{1}{2} \parallel y -
						\mathbf{A} x \parallel_{\mathbf{\Sigma}}^2 + \log p(x)$
					</div>
					<br>
					<div class="fragment fade-up">
						<h3>How do you choose the prior ?</h3>
					</div>
				</section>

				<section>
					<h3 class="slide-title"> Classical examples of signal priors </h3>
					<div class="container">
						<div class="col">
							Sparse
							<img data-src="/talks/assets/wavelet.png" height="400" class="plain"></img><br>
							$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
						</div>
						<div class="col">
							Gaussian
							<img data-src="/talks/assets/zknj8.jpg" height="400" class="plain"></img>
							$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
						</div>
						<div class="col">
							Total Variation
							<img data-src="/talks/assets/shepp-Logan.ppm" class="plain"></img>
							$$ \log p(x) = \parallel \nabla x \parallel_1 $$

						</div>
					</div>
				</section>

				<section data-background="/talks/assets/hsc_screen.png">
					<h2>But what about this?</h2>
				</section>
			</section>

			<section>
				<h3 class="slide-title">Getting started with Deep Priors: deep denoising example</h3>
				$$ \boxed{{\color{Orchid} y} = {\color{SkyBlue} x} + n} $$
				<div class="container">
					<div class="col">
						<div style="position:relative; width:550px; height:550px; margin:0 auto;">
							<img class="fragment current-visible plain" data-src="/talks/assets/points.png"
								style="position:absolute;top:0;left:0;" data-fragment-index="0" />
							<div class="fig-container fragment" data-file="dgm_prior_denoising.html"
								data-style="height: 550px;width: 550px;" style="position:absolute;top:0;left:0;"
								data-fragment-index="1"></div>
						</div>
						<!-- <img data-src="/talks/assets/points.png"/>
																														  															<div class="fig-container" data-file="dgm_prior_denoising.html" data-style="height: 550px;"></div> -->
					</div>

					<div class="col">
						<ul>
							<li class="fragment" data-fragment-index="0"> Let us assume we have access to examples of $
								{\color{SkyBlue} x}$ without noise.</li>
							<br>
							<li class="fragment" data-fragment-index="1">We learn the <b class="alert">distribution of
									noiseless data $\log p_\theta(x)$</b> from samples using a deep generative model.
							</li>
							<br>
							<!-- <li class="fragment"> We measure a noisy ${\color{Orchid} y}$ and we want to estimate a denoised ${\color{SkyBlue} x}$</li>
																														  																<br> -->
							<li class="fragment">The solution should lie on the <b class="alert">realistic data
									manifold</b>, symbolized by the two-moons distribution.
								<div class="fragment">
									<p> We want to solve for the Maximum A Posterior solution: </p>
									$$\arg \max - \frac{1}{2} \parallel {\color{Orchid} y} - {\color{SkyBlue} x}
									\parallel_2^2 + \log p_\theta({\color{SkyBlue} x})$$

									This can be done by <b>gradient descent</b> as long as one has access to the <b
										class="alert">score function</b> $\frac{\color{orange} d \color{orange}\log
									\color{orange}p\color{orange}(\color{orange}x\color{orange})}{\color{orange} d
									\color{orange}x}$.
								</div>
							</li>
						</ul>
					</div>
				</div>
			</section>

			<section>
				<h2> Data-driven priors for astronomical inverse problems</h2>

					  <a href="https://arxiv.org/abs/1912.03980"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A1912.03980-B31B1B.svg" class="plain" style="height:25px;" /></a>
	  <a href="https://www.youtube.com/watch?v=oWOU3qNHoL0"><img src="https://img.shields.io/badge/-youtube-red?logo=youtube&labelColor=grey" class="plain" style="height:25px;" /></a>
						<hr>
						<br>
						<div align="left" style="margin-left: 20px;">
											  <div class="container">
												  <div class="col">
						<h3>Work in collaboration with <br>
						Peter Melchior, Fred Moolekamp, Remy Joseph</h3>
										  <br>
									  </div>
									  <div class="col">
									  </div>
									  <div class="col">
										  <img data-src="/talks/assets/scarlet_data.png" style="height:450px;"/>
									  </div>
									  </div>
						</div>
						<br>
						<br>
			</section>

			<section>
				<h3 class="slide-title"> The Scarlet algorithm: deblending as an optimization problem</h3>
						<div style="float:right; font-size: 20px">Melchior et al. 2018</div>

						$$ \mathcal{L} = \frac{1}{2} \parallel \mathbf{\Sigma}^{-1/2} (\ Y - P \ast A S \ ) \parallel_2^2 - \sum_{i=1}^K \log p_{\theta}(S_i) + \sum_{i=1}^K g_i(A_i) +  \sum_{i=1}^K f_i(S_i)$$

				<div class="container">
				<div class="col">
						<img data-src="/talks/assets/scarlet_data.png" height=450 class="plain"></img>
				</div>

				<div class="col">

					Where for a $K$ component blend:
					<br>
						<ul>
						<li>$P$ is the convolution with the instrumental response</li>
						<br>
						<li>$A_i$ are channel-wise galaxy SEDs, $S_i$ are the morphology models</li>
						<br>
						<li>$\mathbf{\Sigma}$ is the noise covariance</li>
						<br>
						<li>$\log p_\theta$ is a PixelCNN prior</li>
						<br>
						<li>$f_i$ and $g_i$ are arbitrary additional non-smooth consraints, e.g. positivity, monotonicity...</li>
						</ul>
				</div>
			</div>

			<span class="fragment fade-up">$\Longrightarrow$ Explicit physical modeling of the observed sky</span>
			</section>


			<section>
				<h3  class="slide-title">PixelCNN: Likelihood-based Autoregressive generative model</h3>
				<br>
				<br>
				<div class="container">
				<div class="col">
						Models the probability $p(x)$ of an image $x$ as:
						$$ p_{\theta}(x) = \prod_{i=0}^{n} p_{\theta}(x_i | x_{i-1} \ldots x_0) $$
						<ul>
								<li class="fragment">$p_\theta(x)$ is explicit! We get a number out.</li>
								<br>
								<li class="fragment">We can train the model to learn a distribution of isolated galaxy images.</li>
								<br>
								<li class="fragment">We can then evaluate its gradient $\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x\color{orange})}{\color{orange} d \color{orange}x}$.</li>
						</ul>
						<br>
						<br>
				</div>

				<div class="col">
						<img data-src="/talks/assets/pixel_cnn_conv.png" class="plain"></img>
						 <div style="float:right; font-size: 20px">van den Oord et al. 2016</div>
				</div>
			 </div>

			 <br>
			 <br>
			</section>

			 <section>
				<h3  class="slide-title">Training the morphology prior</h3>

				<div class="container">
					<div class="col">
						<img data-src="/talks/assets/cosmos_training.png" height=450 class="plain"></img>
						<div> Postage stamps of isolated COSMOS galaxies used for training, at Roman resolution and fixed fiducial PSF</div>
				</div>

				<div class="col">
				<div class="container fragment fade-in">
					<div class="col">
						isolated galaxy
					<img data-src="/talks/assets/gal_1.png" class="plain"></img>
					<span> $\log p_\theta(x) = 3293.7$ </span>
				</div>

					<div class="col">
						artificial blend
					<img data-src="/talks/assets/gal_2.png" class="plain"></img>
					<span> $\log p_\theta(x) = 3100.5 $ </span>
				</div>
					</div>
				</div>
			</section>

			<section>
				<section>
				<h3 class="slide-title">Scarlet in action</h3>

				<div class="container">
					<div class="col">
						Input blend
					<div style="position:relative; width:480px; height:480px; margin:0 auto;">
					<img data-src="/talks/assets/scar_input.png" class="plain"></img>
				</div>
					</div>

				<div class="col">
					<span class="fragment" data-fragment-index="0">Solution</span>
					<div style="position:relative; width:480px; height:480px; margin:0 auto;">
							  <img class="fragment current-visible plain" data-src="/talks/assets/old_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
							  <img class="fragment  plain" data-src="/talks/assets/pix_rec.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
					</div>
				</div>

				<div class="col">
					<span class="fragment" data-fragment-index="0">Residuals</span>
					<div style="position:relative; width:480px; height:480px; margin:0 auto;">
							  <img class="fragment current-visible plain" data-src="/talks/assets/old_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="0" />
							  <img class="fragment  plain" data-src="/talks/assets/pix_res.png" style="position:absolute;top:0;left:0;" data-fragment-index="1" />
					</div>
				</div>
				</div>

				<ul>
						<li class="fragment fade-up" data-fragment-index="0">Classic priors (monotonicity, symmetry).</li>
						<br>

						<li class="fragment fade-up" data-fragment-index="1">Deep Morphology prior.</li>
				</ul>
			</section>

			<section>
				<div class="container">
					<div class="col">
						True Galaxy
					<img data-src="/talks/assets/true_input.png" class="plain"></img>
				</div>

				<div class="col">
					Deep Morphology Prior Solution
								<img class=" plain" data-src="/talks/assets/pix_rec2.png"  />
				</div>

				<div class="col">
					Monotonicity + Symmetry Solution
								<img class=" plain" data-src="/talks/assets/scar_rec2.png" />
					</div>
				</div>
			</section>
			</section>

			
		<section class="inverted" data-background="#000">
			<h2> But what about uncertainties?</h2>
		</section>

		<section data-vertical-align-top>
			<h3 class="slide-title">What Would a Bayesian Do?</h3>
			$\boxed{y = \mathbf{A}x + n}$
			<br>

			<br>
			<div class="">
				$$ p(x | y) \propto p(y | x) \ p(x) $$
			</div>
			<br>

			<ul>
				<li class=" fade-up">$p(y | x)$ is the data likelihood, which <b class="alert">contains the
						physics</b><br>
				</li>
				<br>
				<li class=" fade-up">$p(x)$ is our <b>prior knowledge</b> on the solution.</li>
			</ul>
			<br>
			<br>
			<div class=" fade-up">
				With these concepts in hand, we can: 
				<br>
				<br>
				<ul>
					<li> Estimate the Maximum A Posteriori solution:
	
						$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
					</li>
					<li class="fragment"> Estimate the full <b class="alert">posterior p(x|y)</b> with Markov Chain Monte-Carlo or Variational Inference methods</li>
				</ul>
			</div>
			<br>
			<div class="fragment">$\Longrightarrow$ Until very recently sampling from such posteriors in high number of dimensions remained very difficult!</div>
		</section>

		<section>
			<section>
			  <h3 class="slide-title">First realization: The score is all you need!</h3>
			  <br>
			  <div class="container">
				  <div class="col">
					  <ul>
						  <li> Whether you are looking for the MAP or sampling with HMC or MALA, you
							  <b class="alert">only need access to the score</b> of the posterior:
							  $$\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x \color{orange}|\color{orange} y\color{orange})}{\color{orange}
							  d
							  \color{orange}x}$$
							  <ul>
								  <li>Gradient descent: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) $</li>
								  <li>Langevin algorithm: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) + \sqrt{2\tau} n_t$ </li>
							  </ul>
						  </li>
						  <br>
					  </ul>
				  </div>
				  <div class="col">
					  <img data-src="/talks/assets/score_two_moons.png"></img>
				  </div>
			  </div>
			  <br>
			  <br>
			  <ul class="fragment">
				  <li > The score of the full posterior is simply:
					  $$\nabla_x \log p(x |y) = \underbrace{\nabla_x \log p(y |x)}_{\mbox{known explicitly}} \quad + \quad \underbrace{\nabla_x \log p(x)}_{\mbox{known implicitly}}$$
					  $\Longrightarrow$ "all" we have to do is <b class="alert">model/learn the score of the prior</b>.
				  </li>
			  </ul> 
		  </section>
  
		  <section>
			  <h3 class="slide-title">Neural Score Estimation by Denoising Score Matching (Vincent 2011)</h3>
			  <ul>
				  <li><b>Denoising Score Matching</b>: An optimal <b class="alert">Gaussian denoiser learns the score</b> of a given distribution.
					  <ul>
						  <li class="fragment fade-up"> If $x \sim \mathbb{P}$ is corrupted by additional Gaussian noise $u \in \mathcal{N}(0, \sigma^2)$ to yield
							  $$x^\prime = x + u$$
						  </li>
						  <li class="fragment fade-up"> Let's consider a denoiser $r_\theta$ trained under an $\ell_2$ loss:
							  $$\mathcal{L}=\parallel x - r_\theta(x^\prime, \sigma) \parallel_2^2$$
						  </li>
						  <li class="fragment fade-up"> The optimal denoiser $r_{\theta^\star}$ verifies:
							  $$\boxed{\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma) = \boldsymbol{x}' + \sigma^2 \nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}$$
						  </li>
					  </ul>
				  </li>
			  </ul>
  
			  <div class="fragment fade-up">
				  <div class="container">
					  <div class="col">$\boldsymbol{x}'$
					  </div>
					  <div class="col">$\boldsymbol{x}$
					  </div>
					  <div class="col">$\boldsymbol{x}'- \boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
					  </div>
					  <div class="col">$\boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
					  </div>
				  </div>
				  <img data-src="/talks/assets/denoised_mnist.png" style='width:1200px;'></img>
			  </div>
		  </section>
		  </section>
  
		  <section>
			  <h3 class="slide-title">Second Realization: Annealing is everything!</h3>
  
			  <ul>
				  <li> Even with knowledge of the score, <b class="alert">sampling in high number of dimensions is difficult!</b><br>
				  </li>
				  <br>
				  <br>
				  <li class="fragment fade-up"> Convolving a target distribution $p$ with a noise kernel, makes $p_\sigma(x) = \int \mathcal{N}(x; x^\prime, \sigma^2) (x^\prime) d x^{\prime}$ it much better behaved
					  <div>
						  $$\sigma_1 > \sigma_2 > \sigma_3 > \sigma_4 $$
						  <img data-src="/talks/assets/annealing.png" />
					  </div>
				  </li>

			  </ul>
		  </section>
  
		  <section>
			  <h3 class="slide-title">Score-Based Generative Modeling <a href="https://arxiv.org/abs/2011.13456"> Song et al. (2021)</a></h3>
			  <img data-src="/talks/assets/diffusion.png" style="height:350px;"/><br>
			  <br>
			  <ul>
				  <li class="fragment"> The SDE defines a <b class="alert">marginal distribution $p_t(x)$</b> as the convolution of the target distribution $p(x)$ with a noise kernel $p_{t|s}(\cdot | x_s)$:
					  $$p_t(x) = \int p(x_s) p_{t|s}(x | x_s) d x_s$$
				  </li>
				  <li class="fragment"> For a given forward SDE that evolves $p(x)$ to $p_T(x)$, there exists a <b>reverse SDE</b> that evolves $p_T(x)$ back into $p(x)$. It involves having access to the <b class="alert">marginal score $\nabla_x \log_t p(x)$</b>.</li>
			  </ul>
		  </section>
  
		  <section>

			<section>
				<h3 class="slide-title">Third realization: We do not have access to the marginal posterior score...</h3>
				
				<ul>
					<li>We know the following quantities: 
						<ul>
							<li>Annealed likelihood (analytically): $p_\sigma(y | x) = \mathcal{N}(y; \mathbf{A} x, \mathbf{\Sigma} + \sigma^2 \mathbf{I})$</li>
							<li>Annealed prior score (by score matching): $\nabla_x \log p_\sigma(x)$ </li>
						</ul>
					</li>
					<li class="fragment" data-fragment-index="1">But, unfortunately: $\boxed{p_\sigma(x|y) \neq p_\sigma(y|x) \  p_\sigma(x)}$
						$\Longrightarrow$ <b class="alert">We don't know the marginal posterior score!</b>
					</li>
	
					<br>
					<li class="fragment">We cannot directly use the reverse SDE/ODE of diffusion models to sample from the posterior.
						$$\mathrm{d} x = [f(x, t) - g^2(t) \underbrace{\nabla_x \log p_t(x|y)}_{\mbox{unknown}} ] \mathrm{d}t + g(t) \mathrm{d} w$$
					</li>
				</ul> 
				<div class="r-stack">
				<div class="block fragment fade-up" >
					<div class="block-title">
					 Proposed sampling strategy (Remy et al. 2020)
					</div>
					<div class="block-content">
					  <ul>
						<li> Even if not equivalent to the marginal posterior score, $\nabla_x \log p_{\sigma^2}(y | x) + \nabla_x \log p_{\sigma^2}(x)$ still 
							has good properties:
							<ul>
								<li>Tends to an isotropic Gaussian distribution for large $\sigma$ </li>
								<li>Corresponds to the target posterior for $\sigma=0$ </li>
							</ul> 
						</li>
						<br>
						<li class="fragment"> If we <b class="alert">anneal Langevin or HMC sampling sufficiently slowly</b> (i.e. timescale of change of $\sigma$ is much larger than the timescale of the SDE)
							we can expect to sample from the target posterior. 
						</li>
					  </ul>
			</section>
		  </section>

		  <section>
			<h2>High-Dimensional Bayesian Inference for Inverse Problems With Neural Score Estimation</h2>
			<a href="https://arxiv.org/abs/2011.08271"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2011.08271-B31B1B.svg" class="plain" style="height:25px;" /></a>
			<a href="https://arxiv.org/abs/2011.08698"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
			<a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;" /></a>
			<hr>
			<div class="container">
				<div class="col">
					<div align="left" style="margin-left: 20px;">
						<h3>Work in collaboration with: <br>
							Benjamin Remy, Zaccharie Ramzi
						</h3>
						<img data-src="/talks/assets/benjamin.png" style='width:200px; height:200px;object-fit: cover;'></img>
						<img data-src="http://www.cosmostat.org/wp-content/uploads/2019/03/Portrait-2-1600x2000.jpg" style='width:200px; height:200px;object-fit: cover;'></img>

						<br>

						$\Longrightarrow$ <b class="alert">Learn complex priors</b> by Neural Score Estimation and <b class="alert">sample from posterior</b> with gradient-based MCMC.
					</div>
				</div>
				<div class="col">
					<img class="plain" data-src="/talks/assets/cropped.gif" style="width:450px;" />
				</div>
			</div>
			<br>
		</section> 
  
		  <section>
			<section data-background-image="/talks/assets/gravitational-lensing-diagram.jpg">
				<h3 class="slide-title">Let's set the stage: Gravitational lensing</h3>
				<div class="fade-up">
					<img class="plain" data-src="/talks/assets/great.jpg" />

					<div class="block ">
						<div class="block-title">
							Galaxy shapes as estimators for gravitational shear
						</div>
						<div class="block-content">
							$$ e = \gamma + e_i \qquad \mbox{ with } \qquad e_i \sim \mathcal{N}(0, I)$$
							<ul>
								<li> We are trying the measure the <b class="alert"> ellipticity $e$</b> of
									galaxies as an estimator for the <b class="alert">gravitational shear $\gamma$ </b>
								</li>
							</ul>
						</div>
					</div>
				</div>
			</section> 

			<section>
				<h3 class="slide-title">Gravitational Lensing as an Inverse Problem</h3>
				<div class="container">
					<div class="col">
						Shear <b class="alert">$\gamma$</b><br>
						<img data-src="/talks/assets/shear_cat1.png" style="width:450px;"></img>
					</div>

					<div class="col fragment fade-up">
						Convergence <b class="alert">$\kappa$</b><br>
						<img data-src="/talks/assets/kappa.png" style="width:450px;"></img>
					</div>
				</div>

				<div style="position:relative; width:1000px; height:100px; margin:0 auto;">
					<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
						$$\gamma_1 = \frac{1}{2} (\partial_1^2 - \partial_2^2) \ \Psi \quad;\quad \gamma_2 = \partial_1 \partial_2 \ \Psi \quad;\quad \kappa = \frac{1}{2} (\partial_1^2 + \partial_2^2) \ \Psi$$
					</div>
					<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
						$$\boxed{\gamma = \mathbf{P} \kappa}$$
					</div>
				</div>
			</section>

		</section> 
		
		<section>
			<h3 class="slide-title">Writing down the convergence map log posterior</h3>

				$$ \log p( \kappa | e) = \underbrace{\log p(e | \kappa)}_{\simeq -\frac{1}{2} \parallel e - P \kappa \parallel_\Sigma^2} + \log p(\kappa) +cst $$

				<ul>
					<li> The likelihood term is <b class="alert">known analytically</b>, given to us by the physics of gravitational lensing.
					</li>

					<li class="fragment fade-up"> There is <b class="alert">no close form expression for the prior</b> on dark matter maps $\kappa$.
						<br> However:
						<ul>
							<li class='fragment'> <b>We do have access to samples of full  <b class="alert">implicit</b> prior</b> through simulations: $X = \{x_0, x_1, \ldots, x_n \}$ with $x_i \sim \mathbb{P}$
								<img data-src='/talks/assets/plot_massive_nu.png' />
							</li>
						</ul>
					</li>
				</ul>
				<div class="fragment">$\Longrightarrow$ Our strategy: <b class="alert">Learn the prior from simulation</b>,
					and then <b class="alert">sample the full posterior</b>.</div>
	  </section>


		<section>  
  
			<section>
				<h3 class="slide-title">Example of one chain during annealing</h3>
				<img data-src="/talks/assets/hmc-annealing.gif"/>
		</section>

		<section>
			<h3 class="slide-title">Validating Posterior Sampling under a Gaussian prior</h3>

			<img  style="height:600px;" data-src="/talks/assets/Remy2022Wiener.png"/>

		</section>
		</section>


			  <section>
				  <section>
					  <h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>
					  <div class="container">
						  <div class="col">
								  <div style="float:right; font-size: 20px"> Remy, Lanusse, et al. (2022) <a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
						  </div>
					  </div>
					  <div class="container">
						  <div class="col">
							  <img data-src='/talks/assets/ref_ktng.png' style="width:350px; height:350px;" />
							  <br>
							  True convergence map
						  </div>
						  <div class="col">
							  <div class="block-content">
								  <div style="position:relative; width:350px; height:350px; top:10px; left:40px;">
									  <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="0">
										  <img data-src='/talks/assets/ks_ktng.png' style="width:350px; height:350px;" />
									  </div>
									  <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="1">
										  <img data-src='/talks/assets/wiener_ktng.png' style="width:350px; height:350px;" />
									  </div>
									  <div class="plain fragment" style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="2">
										  <img data-src='/talks/assets/mean_post_ktng.png' style="width:350px; height:350px;" />
									  </div>
								  </div>
								  <div class="block-content">
									  <div style="position:relative; width:350px; height:20px; top:50px; left:10px;">
										  <div class="fragment current-visible " data-fragment-index="0" style="position:absolute;top:0;left:0;width:350px;">Traditional Kaiser-Squires</div>
										  <div class="fragment current-visible " data-fragment-index="1" style="position:absolute;top:0;left:0;width:350px;">Wiener Filter</div>
										  <div class="fragment" data-fragment-index="2" style="position:absolute;top:0;left:0;width:350px;">Posterior Mean (ours)</div>
									  </div>
								  </div>
								  <br>
								  <br>
							  </div>
  
						  </div>
						  <div class="col fragment">
							  <img data-src='/talks/assets/cropped.gif' style="width:350px; height:350px;" />
							  <br>
							  Posterior samples
						  </div>
					  </div>
  
				  </section>
  
  
				  <section>
					  <h3 class="slide-title">Reconstruction of the <b class="alert">HST/ACS COSMOS field</b></h3>
  
					  <ul>
					  <li> COSMOS shear data from <a href=https://ui.adsabs.harvard.edu/abs/2010A%26A...516A..63S/abstract>Schrabback et al. 2010</a>
					  </li>
					  <li> Prior learned from $\kappa$-TNG simulation from <a href=https://arxiv.org/abs/2010.09731>Osato et al. 2021</a>.
					  </li>
				  </ul>
					  <br>
					  <div class="container">
						  <div class="col">
							  <div class="block-content">
								  <div style="position:relative; height:570px; top:0px; left:0px;">
									  Massey et al. (2007)
									  <img data-src="/talks/assets/massey.png" style="height:500px;"></img>
								  </div>
							  </div>
						  </div>
  
						  <div class="col">
							  <div class="block-content">
								  <div style="position:relative; height:570px; top:0px; left:0px;">
									  <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
										  Remy et al. (2022) <b class="alert">Posterior mean</b>
										  <img data-src='/talks/assets/remy.png' style="height:500px;" />
									  </div>
  
									  <div class="plain fragment" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
										  Remy et al. (2022) <b class="alert">Posterior samples</b>
										  <img data-src='/talks/assets/cosmos_samples.gif' style="height:500px;" />
									  </div>
  
								  </div>
							  </div>
						  </div>
  
					  </div>
				  </section>
				  </section>


				  <section>
					<h3 class="slide-title">Uncertainty quantification in Magnetic Resonance Imaging (MRI)</h3>
					<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
								src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
					</div>
					<br>
					<br>
					$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
					<div><video data-autoplay loop="loop" data-src="/talks/assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
					</div>
					<br>
	
					<br>
	
					<br>
	
					<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
				</section>
		
		<section>
			<h1>Inference over hybrid physical/data-driven models</h1>
			<hr>
		</section>

	  <section>
		<h3 class="slide-title"> Complications specific to astronomical images: spot the differences!</h3>

		<div class="container">
			<div class="col">
			  <img data-src="/talks/assets/celeba.png" class="plain" style="height: 450px;" ></img>
			  <br>
			  CelebA
			</div>
			<div class="col">
			  <img data-src="/talks/assets/hsc_images.png" class="plain"  style="height: 450px;" ></img>
			  <br>
			  HSC PDR-2 wide
			</div>
		</div>
		<br>
		<div >
		  <ul>
			<li class="fragment"> There is <b class="alert">noise</b></li>
			<li class="fragment"> We have a <b class="alert">Point Spread Function</b> (instrumental response)</li>
		  </ul>
		</div>
	  </section>
	  <section>

		<section>
		   <h3 class="slide-title" style="position:absolute;top:0;">A Physicist's approach: let's build a model</h3>
				   <div class="container">
					   <div class="col">
						   <div style="float:right; font-size: 20px"> <b>Lanusse</b> et al. (2020) <a href="https://arxiv.org/abs/2008.03833"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
					   </div>
				   </div>
		   <div class="container">
			 <div class="col">
				 <img class="plain fragment" data-src="/talks/assets/rand_z_square.png" style="height: 150px" data-fragment-index="4"/>
			 </div>
			 <div class="col">
				 <img class="plain fragment" data-src="/talks/assets/cosmos_gal.png" style="width: 200px" data-fragment-index="3"/>
			 </div>
			 <div class="col">
			   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_psf.png" style="width: 200px" data-fragment-index="2"/>
			 </div>

			 <div class="col">
			   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_pix.png" style="width: 200px" data-fragment-index="1"/>
			 </div>

			 <div class="col">
			   <img class="plain fragment" data-src="/talks/assets/cosmos_gal_ground.png" style="width: 200px" data-fragment-index="0"/>
			 </div>
		   </div>

		 <div class="container" style="position:relative; width:1000px; height:50px; margin:0 auto;">
		   <div class='col fragment' data-fragment-index='4'> <font size="10"> $\longrightarrow$ </font> <br> $g_\theta$ </div>
		   <div class='col fragment' data-fragment-index='3'> <font size="10"> $\longrightarrow$ </font> <br> PSF </div>
		   <div class='col fragment' data-fragment-index='2'> <font size="10"> $\longrightarrow$ </font> <br> Pixelation</div>
		   <div class='col fragment' data-fragment-index='1'> <font size="10"> $\longrightarrow$ </font> <br> Noise </div>
		 </div>

		 <div class="container">
			 <div class="col">
			   <div style="position:relative; width:400px; height:300px; margin:0 auto;">
			   <img data-src="/talks/assets/pgm_0.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="0"/>
			   <img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="1"/>
			   <img data-src="/talks/assets/pgm_1.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="2"/>
			   <img data-src="/talks/assets/pgm_2.png" class="plain fragment current-visible " style="position:absolute;top:0;left:0;height:350px;" data-fragment-index="3"/>
			   <img data-src="/talks/assets/pgm_3.png" class="plain fragment " style="position:absolute;top:0;left:0;height:300px;" data-fragment-index="4"/>
			   </div>
			 </div>
			 <div class=" col">
			   <div class="block fragment" data-fragment-index="0">
			   <div class="block-title">
				Probabilistic model
			   </div>
			   <div class="block-content">
			   <div style="position:relative; width:400px; height:100px; margin:0 auto;">
				 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="0"> $$ x \sim ? $$ </div>
				 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="1"> $$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br>latent $z$ is a denoised galaxy image</div>
				 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="2"> $$ x \sim \mathcal{N}( \mathbf{P} z, \Sigma) \quad z \sim ?$$<br>latent $z$ is a super-resolved and denoised galaxy image</div>
				 <div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="3"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast z), \Sigma) \quad z \sim ? $$<br>latent $z$ is a deconvolved, super-resolved, and denoised galaxy image </div>
				 <div class="plain fragment " style="position:absolute;top:0;left:0;width:400px;" data-fragment-index="4"> $$ x \sim \mathcal{N}( \mathbf{P} (\Pi \ast g_\theta(z)), \Sigma) \quad z \sim \mathcal{N}(0, \mathbf{I}) $$ <br>latent $z$ is a Gaussian sample<br> <b class="alert"> $\theta$ are parameters of the model</b> </div>
			   </div>
			   <br>
			   <br>
			   <br>
			 </div>
			 </div>
			 </div>
		 </div>
		 <div class="fragment"> $\Longrightarrow$ <b class="alert"> Decouples the morphology model from the observing conditions</b>.</div>
		</section>

	   <section>
		 <h3 class="slide-title">Bayesian Inference a.k.a. Uncertainty Quantification</h3>
		 <div class="container">
			 <div class="col">
			   <img data-src="/talks/assets/pgm.png" class="plain" style="height: 250px;" ></img>
			 </div>
			 <div class="col">
			   The Bayesian view of the problem:
					$$ p(z | x ) \propto p_\theta(x | z, \Sigma, \mathbf{\Pi}) p(z)$$
				where:
				<br>
				  <ul>
					<li>$p( z | x )$ is the <b class="alert">posterior</b></li>
					<li>$p( x | z )$ is the data likelihood, <b class="alert">contains the physics</b></li>
					<li>$p( z )$ is the <b>prior</b> </li>
				  </ul>
			 </div>
		 </div>

		 <div class="container">
			 <div class="col">
			   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
				 <img class="plain  current-visible" data-src="/talks/assets/cosmos_gal_ground.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0" />
				 <img class="plain fragment" data-src="/talks/assets/cosmos_gal.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
			   </div>
			   <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
				 <div class='col current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;"> Data<br> $x_n$</div>
				 <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Truth<br> $x_0$ </div>
			   </div>
			   <br>
			 </div>

			 <div class="col fragment" data-fragment-index='0' >
			   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
				 <div><video data-autoplay data-loop data-src="/talks/assets/rec_samples.mp4" type="video/mp4" style="height: 200px;"/>
				 </div>
			   </div>
			   <div>Posterior samples<br> $g_\theta(z)$</div>
			 </div>

			 <div class="col">
			   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
				 <div><video class="fragment current-visible" data-autoplay data-loop data-src="/talks/assets/rec_lsst.mp4" type="video/mp4" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0"/></div>
				 <img class="plain fragment " data-src="/talks/assets/rec_median.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
			   </div>

			   <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
				 <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;">  <br> $\mathbf{P} (\Pi \ast g_\theta(z))$</div>
				 <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Median </div>
			   </div>
			 </div>

			 <div class="col">
			   <div style="position:relative; width:200px; height:200px; margin:0 auto;">
				 <div><video class="fragment current-visible" data-autoplay data-loop data-src="/talks/assets/res_lsst.mp4" type="video/mp4" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="0"/></div>
				 <img class="plain fragment " data-src="/talks/assets/rec_std.png" style="position:absolute;top:0;left:0;width:200px;" data-fragment-index="1"/>
			   </div>

			   <div class="container" style="position:relative; width:200px; height:50px; margin:0 auto;">
				 <div class='col fragment current-visible' data-fragment-index='0' style="position:absolute;top:0;left:0;width:200px;"> Data residuals <br> $x_n - \mathbf{P} (\Pi \ast g_\theta(z))$</div>
				 <div class='col fragment' data-fragment-index='1' style="position:absolute;top:0;left:0;width:200px;"> Standard Deviation </div>
			   </div>
			 </div>
		 </div>
		 <div class="fragment"> $\Longrightarrow$ <b class="alert">Uncertainties are fully captured by the posterior</b>.</div>
	   </section>
	 </section>

	  <section>
<section>
<h3 class="slide-title">How to train your <s>dragon</s> model</h3>
<div class="container">
   <div class="col">
	 <img data-src="/talks/assets/pgm.png" class="plain" style="height: 300px;" ></img>
   </div>
   <div class="col">
	 <ul>
	   <li> Training the generative amounts to finding $\theta_\star$ that
		 <b>maximizes the marginal likelihood</b> of the model:
		   $$p_\theta(x | \Sigma, \Pi) = \int \mathcal{N}( \Pi \ast g_\theta(z), \Sigma) \ p(z) \ dz$$
		   <div> $\Longrightarrow$ This is <b class="alert">generally intractable</b></div>
	   </li>
	   <br>
	   <li class="fragment fade-up"> Efficient training of parameter $\theta$ is made possible by <b class="alert">Amortized Variational Inference</b>.
	   </li>
	 </ul>
   </div>
</div>

<div class="block fragment fade-up">
<div class="block-title">
Auto-Encoding Variational Bayes (Kingma & Welling, 2014)
</div>
<div class="block-content">
 <ul>
   <li class=" fade-up"> We introduce a <b>parametric distribution</b> $q_\phi(z | x, \Pi, \Sigma)$ which aims to model the
   posterior $p_{\theta}(z | x, \Pi, \Sigma)$.
   </li>
   <br>
   <li class=" fade-up"> Working out the KL divergence between these two distributions leads to:

	 $$\log p_\theta(x | \Sigma, \Pi) \quad \geq \quad - \mathbb{D}_{KL}\left( q_\phi(z | x, \Sigma, \Pi) \parallel p(z) \right) \quad + \quad \mathbb{E}_{z \sim q_{\phi}(. | x, \Sigma, \Pi)} \left[ \log p_\theta(x | z, \Sigma, \Pi)  \right]$$

	 $\Longrightarrow$ This is the <b>Evidence Lower-Bound</b>, which is differentiable with respect to $\theta$ and $\phi$.
   </li>
 </ul>
</div>
</div>

</section>
<section>
<h3 class="slide-title">The famous Variational Auto-Encoder</h3>
<img data-src="/talks/assets/vae.png" class="plain" style="height: 450px;"> </img>
<br>
<br>
$$\log p_\theta(x| \Sigma, \Pi ) \geq - \underbrace{\mathbb{D}_{KL}\left( q_\phi(z | x, \Sigma, \Pi) \parallel p(z) \right)}_{\mbox{code regularization}} + \underbrace{\mathbb{E}_{z \sim q_{\phi}(. | x, \Sigma, \Pi)} \left[ \log p_\theta(x | z, \Sigma, \Pi)  \right]}_{\mbox{reconstruction error}} $$
</section>

   <section>
			 <h3 class="slide-title"> Sampling from the model</h3>
	   <div class="container">
	   <div class="col fragment fade-up">
		 <img data-src="/talks/assets/vae_samples_bad.png" class="plain" ></img>
		 Woups... what's going on?
	   </div>
	   <div class="col">
		 <img data-src="/talks/assets/latent_space.png" class="plain fragment fade-up" ></img>
	   </div>
	 </div>
</section>

   <section>
	   <h3 class="slide-title"> Tradeoff between code regularization and image quality</h3>

 <br>
 $$\log p_\theta(x| \Sigma, \Pi ) \geq - \underbrace{\mathbb{D}_{KL}\left( q_\phi(z | x, \Sigma, \Pi) \parallel p(z) \right)}_{\mbox{code regularization}} + \underbrace{\mathbb{E}_{z \sim q_{\phi}(. | x, \Sigma, \Pi)} \left[ \log p_\theta(x | z, \Sigma, \Pi)  \right]}_{\mbox{reconstruction error}} $$

 <img data-src="/talks/assets/sdss_ae_kl.png" class="plain" ></img>

</section>

<section data-background-image=https://media.giphy.com/media/3o85xIO33l7RlmLR4I/source.gif>
</section>

   <section>
	   <h3 class="slide-title"> Latent space modeling with Normalizing Flows</h3>
 <br>
 $\Longrightarrow$ All we need to do is <b class="alert">sample from the aggregate posterior</b> of the data instead of sampling from the prior.

<br>
<br>

<div class="container">
<div class="col">
 <img data-src="/talks/assets/flow_dinh_1.png" class="plain fragment fade-up" data-fragment-index="1"></img>
 <img data-src="/talks/assets/flow_dinh_2.png" class="plain fragment fade-up" data-fragment-index="3"></img>

 <br>
		<div class="fragment fade-up" style="float:right; font-size: 20px" data-fragment-index="1">Dinh et al. 2016</div>
</div>
<div class="col">
		 <div class="block fragment fade-up" data-fragment-index="1">
		 <div class="block-title">
		  Normalizing Flows
		 </div>
		 <div class="block-content">
		   <ul>
			 <li> Assumes a <b class="alert">bijective</b> mapping between
			   data space $x$ and latent space $z$ with prior $p(z)$:
			   $$ z = f_{\theta} ( x ) \qquad \mbox{and} \qquad x = f^{-1}_{\theta}(z)$$
			 </li>
			 <li class="fragment" data-fragment-index="2"> Admits an explicit likelihood:
			   $$ \log p_\theta(x) = \log p(z) + \log \left| \frac{\partial f_\theta}{\partial x}  \right|(x)    $$
			 </li>
		   </ul>
	   </div>
	   </div>
	   <br>
		 <br>
		 <br>
		 <br>
</div>
</div>

</section>
<section>
	<h3 class="slide-title">If you want to code your own Normalizing Flow</h3>

	<br>
	<br>

	<a href="https://colab.research.google.com/github/EiffL/Tutorials/blob/master/NormalizingFlowsInJAX.ipynb">This notebook</a> to implement a Normalizing Flow in JAX+Flax+TensorFlow Probability

	<img data-src="/talks/assets/points.png"/>
	<br>
	<br>

	<br>
	<br>
</section>
</section>


<section>
	<h3 class="slide-title"> Flow-VAE samples</h3>
<br>
<br>
<img class="current-visible plain" data-src="/talks/assets/lanusse2020_figure1.png"/>
</section>

<section>
	<h2>Variational Inference over Hybrid Hierarchical Bayesian Models</h2>
	<a href="https://arxiv.org/abs/2008.03833"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg" class="plain" style="height:25px;" /></a>
	<a href="https://arxiv.org/abs/2210.16243"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2210.16243-B31B1B.svg" class="plain" style="height:25px;" /></a>
	<hr>
	<div class="container">
		<div class="col">
			<div align="left" style="margin-left: 20px;">
				<h3>Work led by Benjamin Remy
				</h3>
				<img data-src="/talks/assets/benjamin.png" style='width:200px; height:200px;object-fit: cover;'></img>

				<br>
				$\Longrightarrow$ <b class="alert">Eliminate model bias</b> in shear inference by using data-driven morphology priors.

			</div>
		</div>
		<div class="col">
			<img class="plain" data-src="/talks/assets/deepgal1.png" style="width:450px;" />
		</div>
	</div>
	<br>
</section> 

<section>
	<h3 class="slide-title"> Impact of galaxy morphology on shape measurement</h3>

		<div class="r-stack">
			<div><img class="plain" data-src="/talks/assets/great.jpg" />
				</div>
			   <div class='container fragment'>
				   <div class='col'>
		  <img class="plain" data-src="/talks/assets/real_gal-inv.png" style="height: 350px;"/>
		  <br>
		  <div style="float:left; font-size: 20px">Mandelbaum, et al. (2013), Mandelbaum, et al. (2014)</div>
		</div>

		<div class='col'>
		  <img class="plain fragment fade-up" data-src="/talks/assets/great3_calib2-inv.png" style="height: 425px;"/>
		</div>
	  </div>
	  </div>
	  <br>
  </section>

<section>
	<!-- <section>
		<h3 class="slide-title">Bayesian modeling of cosmic shear</h3>
		<div align="left">
		We aim to model the posterior distribution $p(\gamma|\mathcal{D})$ <br><br>

		<div class="fragment">
		$\begin{align}
		p(\gamma|\mathcal{D}) &= \int p(\gamma, z, \Pi|\mathcal{D}) ~dz~d\Pi \\
		\end{align}$
	</div>
	<div class="fragment">
		$\begin{align}
		~~~~~~~~~~~&= \int \color{orange}{\underbrace{p(\mathcal{D}|\gamma, z, \Pi)}_{\text{likelihood}}} \underbrace{p(\gamma)p(z)p(\Pi)}_{\text{priors}} ~dz~d\Pi
		\end{align}$
	</div>
		<br>
		
		<div class="fragment">
		The likelihood $\color{orange}{p(\mathcal{D}|\gamma, z, \Pi)}$ is naturally built from the forward model <br>
			<div align="center">
		<img class="plain" style="height:300px" data-src="/talks/assets/great.jpg" />
	</div>
	</div>
	</div>

	</section> -->

	<section>
		<h3 class="slide-title">
			Let's again think as physicists
		</h3>
		<!-- <div class="container">
			<div class="col">
				<div style="float: right; font-size: 20px">
					<b>Lanusse</b> et al. (2020)
					<a href="https://arxiv.org/abs/2008.03833"
						><img
							src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2008.03833-B31B1B.svg"
							class="plain"
							style="height: 25px; vertical-align: middle"
					/></a>
				</div>
			</div>
		</div> -->
		<div class="container">
			<div class="col">
				<img
					class="plain "
					data-src="/talks/assets/rand_z_square.png"
					style="height: 150px"
					data-fragment-index="4"
				/>
			</div>
			<div class="col">
				<img
					class="plain "
					data-src="/talks/assets/cosmos_gal_shear.png"
					style="width: 200px"
					data-fragment-index="3"
				/>
			</div>
			<div class="col">
				<img
					class="plain "
					data-src="/talks/assets/cosmos_gal.png"
					style="width: 200px"
					data-fragment-index="2"
				/>
			</div>

			<div class="col">
				<img
					class="plain "
					data-src="/talks/assets/cosmos_gal_psf.png"                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     osmos_gal_psf.png"
					style="width: 200px"
					data-fragment-index="1"
				/>
			</div>

			<div class="col">
				<img
					class="plain "
					data-src="/talks/assets/cosmos_gal_ground.png"
					style="width: 200px"
					data-fragment-index="0"
				/>
			</div>
		</div>

		<div
			class="container"
			style="
				position: relative;
				width: 1000px;
				height: 50px;
				margin: 0 auto;
			"
		>
			<div class="col " data-fragment-index="4">
				<font size="10"> $\longrightarrow$ </font> <br />
				$g_\theta$
			</div>
			<div class="col " data-fragment-index="3">
				<font size="10"> $\longrightarrow$ </font> <br />
				<b class="alert">shear $\gamma$</b>
			</div>
			<div class="col " data-fragment-index="2">
				<font size="10"> $\longrightarrow$ </font> <br />
				PSF
			</div>
			<div class="col " data-fragment-index="1">
				<font size="10"> $\longrightarrow$ </font> <br />
				Noise
			</div>
		</div>

		<div class="container">
			<div class="col">
				<div
					style="
						position: relative;
						width: 400px;
						height: 300px;
						margin: 0 auto;
					"
				>
					<!-- <img
						data-src="/talks/assets/pgm_x.png"
						class="plain  current-visible"
						style="position: absolute; top: 0; left: 0; height: 300px"
						data-fragment-index="0"
					/>
					<img
						data-src="/talks/assets/pgm_xzs.png"
						class="plain  current-visible"
						style="position: absolute; top: 0; left: 0; height: 300px"
						data-fragment-index="1"
					/>
					<img
						data-src="/talks/assets/pgm_xzsp.png"
						class="plain  current-visible"
						style="position: absolute; top: 0; left: 0; height: 350px"
						data-fragment-index="2"
					/>
					<img
						data-src="/talks/assets/pgm_xzsp_shear.png"
						class="plain  current-visible"
						style="position: absolute; top: 0; left: 0; height: 350px"
						data-fragment-index="3"
					/> -->
					<img
						data-src="/talks/assets/pgm_full.png"
						class="plain "
						style="position: absolute; top: 0; left: 0; height: 300px"
						data-fragment-index="4"
					/>
				</div>
			</div>
			<div class="col">
				<div class="block " data-fragment-index="0">
					<div class="block-title">Probabilistic model</div>
					<div class="block-content">
						<div
							style="
								position: relative;
								width: 400px;
								height: 100px;
								margin: 0 auto;
							"
						>
							<!-- <div
								class="plain fragment current-visible"
								style="
									position: absolute;
									top: 0;
									left: 0;
									width: 400px;
								"
								data-fragment-index="0"
							>
								$$ x \sim ? $$
							</div>
							<div
								class="plain fragment current-visible"
								style="
									position: absolute;
									top: 0;
									left: 0;
									width: 400px;
								"
								data-fragment-index="1"
							>
								$$ x \sim \mathcal{N}(z, \Sigma) \quad z \sim ? $$<br />latent
								$z$ is a denoised galaxy image
							</div>
							<div
								class="plain fragment current-visible"
								style="
									position: absolute;
									top: 0;
									left: 0;
									width: 400px;
								"
								data-fragment-index="2"
							>
							$$ x \sim \mathcal{N}(\Pi \ast z, \Sigma)
							\quad z \sim ? $$<br />latent $z$ is a deconvolved,
							and denoised galaxy image
							</div>
							<div
								class="plain fragment current-visible"
								style="
									position: absolute;
									top: 0;
									left: 0;
									width: 450px;
								"
								data-fragment-index="3"
							>
							$$ x \sim \mathcal{N}(\Pi \ast (z \otimes \gamma), \Sigma)
							\quad z \sim ? $$<br />latent $z$ is a unsheared deconvolved,
							and denoised galaxy image
							</div> -->
							<div
								class="plain "
								style="
									position: absolute;
									top: 0;
									left: 0;
									width: 500px;
								"
								data-fragment-index="4"
							>
								$$ x \sim \mathcal{N}(\Pi \ast
								(g_\theta(z) \otimes \gamma), \Sigma) \quad z \sim \mathcal{N}(0,
								\mathbf{I}) $$ <br />latent $z$ are morphological parameters<br />
								
									$\theta$ are global parameters of the model
								<br >
								<b class="alert" >
									$\gamma$ are shear parameters</b>
							</div>
						</div>
						<br />
						<br />
						<br />
					</div>
				</div>
			</div>
		</div>
		<div class="fragment">
			$\Longrightarrow$ We have a hybrid probabilistic model, with the <b>known physics of lensing and of the instrument</b>, and
			<b>learned morphology model</b>.
		</div>
	</section>

	<section>
		<h3 class="slide-title">Joint inference using a parametric model for the morphology</h3>
		<div align="left">
		Let's assume that $g(z)$ is a <b class="alert">sersic model</b>, i.e. $z = \{n, r_\text{hlr}, F, e_1, e_2, s_x, s_y\}$ and 
		$$g(z) = F \times I_0 \exp \left( -b_n \left[\left( \frac{r}{r_\text{hlr}}\right)^{\frac{1}{n}} -1\right] \right)$$
		
		<div class="fragment">
			The joint inference of $p(z, \gamma | \mathcal{D})$ leads to a <b class="alert">biased posterior</b>...
		
			<div class="container">
				<div class="col" align="center">
					<img src="/talks/assets/shear_estimate_bias.png" class="plain" height="300"></img>
					<br>
					Marginal shear posterior $p(\gamma|\mathcal{D})$
				</div>

				<div class="col fragment">
					<img src="/talks/assets/sersic_fit.png" class="plain" height="300"></img>
					<br>
					Maximum a posteriori fit and residuals
				</div>
			</div>
		</div>

		<div class="fragment" align="center"> <br>
			<b class="alert">We need a more realistic model of galaxy morphology</b>
		</div>
	</section>

			<section>
				<h3 class="slide-title">Joint inference using a generative model for the morpholgy</h3>
				<div class="container">
					<div class="col">
						<div style="float: right; font-size: 20px">
							Remy, <b>Lanusse</b>, Starck (2022)
							<a href="https://arxiv.org/abs/2210.16243"
								><img
									src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2210.16243-B31B1B.svg"
									class="plain"
									style="height: 25px; vertical-align: middle"
							/></a>
						</div>
					</div>
				</div>
				<div align="left">
				Let's use a learned $g_\theta(z)$ <br><br>
				
				<div class="fragment">
					The joint inference of $p(z, \gamma | \mathcal{D})$ leads to an <b class="alert">unbiased posterior</b>!<br><br>
				
					<div class="container">
						<div class="col" align="center">
							<img src="/talks/assets/shear_estimate_nobias.png" class="plain" height="300"></img>
							<br>
							Marginal shear posterior $p(\gamma|\mathcal{D})$
						</div>

						<div class="col fragment">
							<img src="/talks/assets/deepgal1.png" class="plain" height="300"></img>
							<br>
							Maximum a posteriori fit and residuals
						</div>
					</div>
				</div>
			</section>
</section>



<section>
	<h1> Conclusion </h1>
</section>

<section>
	<h3 class="slide-title"> Conclusion </h3>
	<div class="block ">
		<div class="block-title">
			Merging Deep Learning with Physical Models for Bayesian Inference
		</div>
		<div class="block-content">
			$\Longrightarrow$ Makes <b>Bayesian inference possible</b> at scale and with non-trivial models!
			<br>
			<br>
			<ul>


			<li class="fragment"> Enables inference in high dimension from numerical simulators.
					<ul>
						<li>By turning implicit physical models into usable explicit distributions.</li>
					</ul>
					</il>
					<br>

			<li class="fragment"> Complement known physical models with data-driven components
				<ul>
					<li>Use data-driven generative model as prior for solving inverse problems.</il>
				</ul>
				</il>
				<br>

				<br>
			</ul>
		</div>
	</div>
	<br>

	<br>
	<p class="fragment">Thank you ! </p>
	<br> <br> <br>
</section>

		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		}  */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
				src: 'reveal.js/plugin/markdown/marked.js'
			},
			{
				src: 'reveal.js/plugin/markdown/markdown.js'
			},
			{
				src: 'reveal.js/plugin/notes/notes.js',
				async: true
			},
			{
				src: 'reveal.js/plugin/math/math.js',
				async: true
			},
			{
				src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
			},
			{
				src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
			},
			{
				src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
			},
			{
				src: 'reveal.js/plugin/highlight/highlight.js',
				async: true
			},
			]

		});
	</script>
</body>

</html>