<!doctype html>
<html>

<head>
	<meta charset="utf-8">
	<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

	<title>Merging AI with Physical Models to Study the Universe</title>

	<meta name="description" content="ISC 24 talk, Hamburg, Germany">
	<link rel="stylesheet" href="reveal.js/dist/reset.css">
	<link rel="stylesheet" href="reveal.js/dist/reveal.css">
	<link rel="stylesheet" href="reveal.js/dist/theme/darkenergy.css" id="theme">

	<!-- Theme used for syntax highlighted code -->
	<link rel="stylesheet" href="reveal.js/plugin/highlight/monokai.css" id="highlight-theme">
</head>

<body>
	<div class="reveal">
		<div class="slides">
			<section data-background-image="ISC2024_Title-Slide.jpg" data-background-size="contain" data-vertical-align-top >
				<a href="https://eiffl.github.io/talks/ISC24" style="color:crimson">eiffl.github.io/talks/ISC24</a>
				<br> <br> <br> <br> <br> <br> <br> <br><br> <br> <br> <br><br> <br><br> <br><br> <br><br> <br><br> <br><br> <br>
			</section>

			<section data-background-image="/talks/assets/WMAP_timeline_large.jpg">
				<h3 class="slide-title" style="position: absolute; top: 0">
				  the $\Lambda$CDM view of the Universe
				</h3>
				<br />
				<br />
				<div class="container">
				  <div class="col" style="flex: 0 0 40em"></div>
				  <div class="col">
					<img
					  class="plain"
					  data-src="/talks/assets/Euclid.png"
					  style="width: 240px"
					/>
	  
					<img
					  class="plain"
					  data-src="/talks/assets/roman_logo_black_w200px.png"
					  style="width: 240px"
					/>
	  
					<img
					  class="plain"
					  data-src="/talks/assets/vrro.png"
					  style="width: 240px"
					/>
				  </div>
				</div>
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
				<br />
			  </section>

			<section>
				<section data-background-video="/talks/assets/animation-day-to-night.mov" data-background-video-muted>
					<h3 class='slide-title'>the Rubin Observatory Legacy Survey of Space and Time</h3>
					<div class="container">
						<div class="col">
							<ul>
								<li class="fragment fade-up"> 1000 images each night, 15 TB/night for 10 years</li>
								<br>
								<li class="fragment fade-up"> 18,000 square degrees, observed once every few days</li>
								<br>
								<li class="fragment fade-up"> Tens of billions of objects, each one observed $\sim1000$ times</li>
							</ul>
						</div>

						<div class="col">
							<video data-autoplay class="fragment fade-up" data-fragment-index="1" data-src="/talks/assets/obsim.mp4" type="video/mp4" />
						</div>
					</div>
				</section>

				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_sdss.png" data-vertical-align-top>
					<p>Previous generation survey: SDSS</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_des.png" data-vertical-align-top>
					<p>Current generation survey: DES</p>
					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
				<section data-transition="fade-in fade-out" data-background="/talks/assets/gal_hsc.png" data-vertical-align-top>
					<p>LSST precursor survey: HSC</p>

					<br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br> <br>
					<br> <br> <br> <br> <br> <br> <br>
					<div style="float:right; font-size: 20px">Image credit: Peter Melchior</div>
				</section>
			</section>

			<section>
				<section>
					<h3 class="slide-title">We need to rethink all stages of data analysis for modern surveys</h3>

					<div class="r-stack">
						<div class="fragment current-visible" data-fragment-index="0">
							<img data-src="/talks/assets/hsc_shredded.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="0" style="float:right; font-size: 20px">Bosch et al. 2017</div>
						</div>

						<div class="fragment" data-fragment-index="1">
							<img data-src="/talks/assets/deepmass_sims_clean.png" style="height:400px;"/><br>
							<div class="fragment" data-fragment-index="1" style="float:right; font-size: 20px">Jeffrey, <b>Lanusse</b>, et al. 2020</div>
						</div>

					</div>
					<ul>
						<li class="fragment" data-fragment-index="0">Galaxies are no longer blobs.</li>
						<li class="fragment" data-fragment-index="1">Signals are no longer Gaussian.</li>
					</ul>
					<br>
					<br>
					<div class="fragment">$\Longrightarrow$ This is the <b class="alert">end of the analytic era</b>...</div>
				</section>

				<section>
					<h3 class="slide-title">... but the <b class="alert">beginning of the data-driven era</b></h3>
						<br>
						 <div class="container">
							 <div class="col fragment" >
									 <b>Case I</b>: Examples from data, no accurate physical model<br>
									 <img data-src="/talks/assets/real_gal-inv-small.png" style="height:350px;"/><br>
										<div style="float:right; font-size: 20px">Mandelbaum et al. 2014</div>
										<br>
							 </div>

							 <div class="col fragment">
								 <b>Case II</b>: Physical model only available as a simulator<br>
								<div>
								 <video data-autoplay style="height:350px;" data-src="/talks/assets/illustris_movie_cube_sub_frame_small.mp4" type="video/mp4" />
								 </div><br> 
									 <div style="float:right; font-size: 20px">Nelson et al. 2015</div>
									  <br> 
							 </div>
						 </div>
						 <br>
						 <div class="fragment">$\Longrightarrow$ Examples of <b class="alert">implicit distributions</b>: we have access to samples $\{x_0, x_1, \ldots, x_n \}$
							 but <b>we do not know $p(x)$ analytically</b>.
						 </div>
				</section>
			</section>

			<section class="inverted" data-background="#000">
				<h2>How can we unlock these implicit distributions <br> to push back the limits of <b>physical inference</b>?</h2>
			</section>

			<section>
				<h2>High-Dimensional Bayesian Inference  Under Implicit Priors</h2>
				<a href="https://arxiv.org/abs/2011.08271"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2011.08271-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<a href="https://arxiv.org/abs/2011.08698"><img src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;" /></a>
				<hr>
				<div class="container">
					<div class="col">
						<div align="left" style="margin-left: 20px;">
							<p>Work in collaboration with: <br>
								Benjamin Remy (now in Princeton), Zaccharie Ramzi (now at Meta)
							</p>
							<img data-src="/talks/assets/benjamin.png" style='width:200px; height:200px;object-fit: cover;'></img>
							<img data-src="http://www.cosmostat.org/wp-content/uploads/2019/03/Portrait-2-1600x2000.jpg" style='width:200px; height:200px;object-fit: cover;'></img>

							<br>

							$\Longrightarrow$ <b class="alert">Learn complex priors</b> by Neural Score Estimation and <b class="alert">sample from posterior</b> with gradient-based MCMC.
						</div>
					</div>
					<div class="col">
						<img class="plain" data-src="/talks/assets/cropped.gif" style="width:450px;" />
					</div>
				</div>
				<br>
			</section> 

			<section>
				<section data-background-image="/talks/assets/gravitational-lensing-diagram.jpg">
					<h3 class="slide-title">Let's set the stage: Gravitational lensing</h3>
					<div class="fragment fade-up">
						<img class="plain" data-src="/talks/assets/great.jpg" />

						<div class="block ">
							<div class="block-title">
								Galaxy shapes as estimators for gravitational shear
							</div>
							<div class="block-content">
								$$ e = \gamma + e_i \qquad \mbox{ with } \qquad e_i \sim \mathcal{N}(0, I)$$
								<ul>
									<li> We are trying the measure the <b class="alert"> ellipticity $e$</b> of
										galaxies as an estimator for the <b class="alert">gravitational shear $\gamma$ </b>
									</li>
								</ul>
							</div>
						</div>
					</div>
				</section> 

				<section>
					<h3 class="slide-title">Gravitational Lensing as an Inverse Problem</h3>
					<div class="container">
						<div class="col">
							Shear <b class="alert">$\gamma$</b><br>
							<img data-src="/talks/assets/shear_cat1.png" style="width:450px;"></img>
						</div>

						<div class="col fragment fade-up">
							Convergence <b class="alert">$\kappa$</b><br>
							<img data-src="/talks/assets/kappa.png" style="width:450px;"></img>
						</div>
					</div>

					<div style="position:relative; width:1000px; height:100px; margin:0 auto;">
						<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
							$$\gamma_1 = \frac{1}{2} (\partial_1^2 - \partial_2^2) \ \Psi \quad;\quad \gamma_2 = \partial_1 \partial_2 \ \Psi \quad;\quad \kappa = \frac{1}{2} (\partial_1^2 + \partial_2^2) \ \Psi$$
						</div>
						<div class="fragment current-visible plain fade-up" style="position:absolute;top:0;left:0;width:1000px;">
							$$\boxed{\gamma = \mathbf{P} \kappa}$$
						</div>
					</div>
				</section>

				</section> 

					<section>
						<section data-vertical-align-top>
							<h3 class="slide-title">What Would a Bayesian Do?</h3>
							$\boxed{y = \mathbf{A}x + n}$
							<br>
							<br>
							The Bayesian view of the problem:
							<br>
							$$ p(x | y) \propto p(y | x) \ p(x) $$
							<ul>
								<li class="fragment fade-up">$p(y | x)$ is the data <b>likelihood</b>, which <b class="alert">contains the physics</b><br>
								</li>
								<br>
								<li class="fragment fade-up">$p(x)$ is the <b>prior</b> knowledge on the solution.</li>
							</ul>
							<br>
							<br>
							<div class="fragment fade-up">
								<ul>With these concepts in hand we can:
									<br>
									<li class="fragment">Estimate for instance the <b>Maximum A Posteriori</b> solution:
										<br>
										$$\hat{x} = \arg\max\limits_x \ \log p(y \ | \ x) + \log p(x)$$
									</li>
									<li class="fragment">Estimate from the <b>full posterior p(x|y)</b> with MCMC or Variational Inference methods.
									</li>
								</ul>
							</div>
							<br>
							<div class="fragment fade-up">
								<h3>How do you choose the prior ?</h3>
							</div>
						</section>

						<section>
							<h3 class="slide-title"> Classical examples of signal priors </h3>
							<div class="container">
								<div class="col">
									Sparse
									<img data-src="/talks/assets/wavelet.png" height="400" class="plain"></img><br>
									$$ \log p(x) = \parallel \mathbf{W} x \parallel_1 $$
								</div>
								<div class="col">
									Gaussian
									<img data-src="/talks/assets/zknj8.jpg" height="400" class="plain"></img>
									$$ \log p(x) = x^t \mathbf{\Sigma^{-1}} x $$
								</div>
								<div class="col">
									Total Variation
									<img data-src="/talks/assets/shepp-Logan.ppm" class="plain"></img>
									$$ \log p(x) = \parallel \nabla x \parallel_1 $$

								</div>
							</div>
						</section>

						<section data-background="/talks/assets/convergence.png">
							<h2>But what about this?</h2>
							<br>
							<br>
							<div class="fragment"> $\Longrightarrow$ <b>Implicit prior</b> $p(x)$ in the form of <b>cosmological simulations</b>.</div>

						</section>
					</section>

					<section class="inverted" data-background="#000">
						<h2> How to embed simulation-driven priors within a <b>physical Bayesian model</b>?</h2>
					</section>

					<section>
						<section>
							<h3 class="slide-title"> The answer: Deep Generative Modeling</h3>
							<br>
							<ul>
								<li>The goal of generative modeling is to <b>learn an <b class="alert">implicit</b> distribution $\mathbb{P}$</b>
									from which the <b>training set $X = \{x_0, x_1, \ldots, x_n \}$</b> is drawn.
								</li>
								<br>
								<li class='fragment'> Usually, this means building a parametric model $\mathbb{P}_\theta$
									that tries to be close to $\mathbb{P}$.
								</li>
							</ul>
		
							<br>
							<div class="container">
								<div class="col fragment fade-up">
									<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756538/pasted-from-clipboard.png" class="plain"></img>
									<br>
									True $\mathbb{P}$
								</div>
		
								<div class="col  fragment fade-up">
									<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756539/pasted-from-clipboard.png" class="plain"></img>
									<br>
									Samples $x_i \sim \mathbb{P}$
								</div>
		
								<div class="col  fragment fade-up">
									<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/7756554/pasted-from-clipboard.png" class="plain"></img>
									<br>
									Model $\mathbb{P}_\theta$
								</div>
							</div>
							<br>
							<br>
							<ul>
								<li class="fragment"> Once trained, you can typically <b>sample from $\mathbb{P}_\theta$</b> and/or <b class="alert">evaluate the likelihood $p_\theta(x)$</b>.
								</li>
							</ul>
						</section>
		
						<section>
							<h3 class="slide-title"> The evolution of generative models </h3>
			
							<br> 
							<div class='container'>
								<div class='col'>
									<div style="position:relative; width:500px; height:600px; margin:0 auto;">
										<img class="fragment current-visible plain" data-src="/talks/assets/DBN.png"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="0" />
										<img class="fragment current-visible plain" data-src="/talks/assets/vae_faces.jpg"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="1" />
										<img class="fragment current-visible plain" data-src="/talks/assets/gan-samples-1.png"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="2" />
										<img class="fragment plain" data-src="/talks/assets/karras2017.png"
											style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="3" />
										<img class="fragment plain" data-src="https://preview.redd.it/de6cf3omoqpa1.jpg?width=960&format=pjpg&auto=webp&v=enabled&s=0f4641ba72ad4c7ef106efafc539804cf7247410"
										style="position:absolute;top:0;left:0;width:500px;" data-fragment-index="4" />
									</div>
								</div>
			
								<div class='col'>
									<ul>
										<li class="fragment" data-fragment-index="0"> Deep Belief Network <br> (Hinton et al. 2006)
										</li>
										<br>
										<li class="fragment" data-fragment-index="1"> Variational AutoEncoder <br> (Kingma & Welling
											2014) </li>
										<br>
										<li class="fragment" data-fragment-index="2"> Generative Adversarial Network <br>
											(Goodfellow et al. 2014)</li>
										<br>
										<li class="fragment" data-fragment-index="3"> Wasserstein GAN <br> (Arjovsky et al. 2017)
										</li>
										<br>
										<li class="fragment" data-fragment-index="4"> Midjourney v5 Guided Latent Diffusion (2023)
										</li>
									</ul>
									<br>
									<br>
									<div class="fragment">$\Longrightarrow$ For all intents and purposes <b class="alert">we can now
										model arbitrary distributions</b> even in extremely high dimensions.
									</div>
								</div>
							</div>
							<br> <br> <br>
						</section>
					</section>

						  <section>
							<h3 class="slide-title">Writing down the convergence map log posterior</h3>
			
								$$ \log p( \kappa | e) = \underbrace{\log p(e | \kappa)}_{\simeq -\frac{1}{2} \parallel e - P \kappa \parallel_\Sigma^2} + \log p(\kappa) +cst $$
			
								<ul>
									<li> The likelihood term is <b class="alert">known analytically</b>, given to us by the physics of gravitational lensing.
									</li>
			
									<li class="fragment fade-up"> There is <b class="alert">no close form expression for the prior</b> on dark matter maps $\kappa$.
										<br> However:
										<ul>
											<li class='fragment'> <b>We do have access to samples of full  <b class="alert">implicit</b> prior</b> through simulations: $X = \{x_0, x_1, \ldots, x_n \}$ with $x_i \sim \mathbb{P}$
												<img data-src='/talks/assets/plot_massive_nu.png' />
											</li>
										</ul>
									</li>
								</ul>
								<div class="fragment">$\Longrightarrow$ Our strategy: <b class="alert">Learn the prior from simulation</b>,
									and then <b class="alert">sample the full posterior</b>.</div>
					  </section>
			
					  <section>
					  <section>
						<h3 class="slide-title">The score is all you need!</h3>
						<br>
						<div class="container">
							<div class="col">
								<ul>
									<li> Whether you are looking for the MAP or sampling with HMC or MALA, you
										<b class="alert">only need access to the score</b> of the posterior:
										$$\frac{\color{orange} d \color{orange}\log \color{orange}p\color{orange}(\color{orange}x \color{orange}|\color{orange} y\color{orange})}{\color{orange}
										d
										\color{orange}x}$$
										<ul>
											<li>Gradient descent: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) $</li>
											<li>Langevin algorithm: $x_{t+1} = x_t + \tau \nabla_x \log p(x_t | y) + \sqrt{2\tau} n_t$ </li>
										</ul>
									</li>
									<br>
								</ul>
							</div>
							<div class="col">
								<img data-src="/talks/assets/score_two_moons.png"></img>
							</div>
						</div>
						<br>
						<br>
						<ul>
							<li class="fragment"> The score of the full posterior is simply:
								$$\nabla_x \log p(x |y) = \underbrace{\nabla_x \log p(y |x)}_{\mbox{known explicitly}} \quad + \quad \underbrace{\nabla_x \log p(x)}_{\mbox{known implicitly}}$$
								$\Longrightarrow$ "all" we have to do is <b class="alert">model/learn the score of the prior</b>: This is what <b>diffusion models</b> do.
							</li>
						</ul> 
					</section>
			
					<section>
						<h3 class="slide-title">Neural Score Estimation by Denoising Score Matching (Vincent 2011)</h3>
						<ul>
							<li><b>Denoising Score Matching</b>: An optimal <b class="alert">Gaussian denoiser learns the score</b> of a given distribution.
								<ul>
									<li class="fragment fade-up"> If $x \sim \mathbb{P}$ is corrupted by additional Gaussian noise $u \in \mathcal{N}(0, \sigma^2)$ to yield
										$$x^\prime = x + u$$
									</li>
									<li class="fragment fade-up"> Let's consider a denoiser $r_\theta$ trained under an $\ell_2$ loss:
										$$\mathcal{L}=\parallel x - r_\theta(x^\prime, \sigma) \parallel_2^2$$
									</li>
									<li class="fragment fade-up"> The optimal denoiser $r_{\theta^\star}$ verifies:
										$$\boxed{\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma) = \boldsymbol{x}' + \sigma^2 \color{orange}{\nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}}$$
									</li>
								</ul>
							</li>
						</ul>
						<div class="fragment fade-up">
							<div class="container">
								
								<div class="col">$\Bigg($
								</div>
								<div class="col">$\boldsymbol{x}'$
									<img data-src="/talks/assets/noisy.png"></img>
								</div>
							
								<div class="col">$-$
								</div>
								<div class="col">$\boldsymbol{r}_{\theta^\star}(\boldsymbol{x}', \sigma)$
									<img data-src="/talks/assets/denoised-map.png"></img>
								</div>
								<div class="col">$\Bigg)~/~\sigma^2=$
								</div>
								<div class="col">$\color{orange}{\nabla_{\boldsymbol{x}} \log p_{\sigma^2}(\boldsymbol{x}')}$
									<img data-src="/talks/assets/score.png"></img>
								</div>
							</div>
							
						</div>

						<!-- <div class="fragment fade-up">
							<div class="container">
								<div class="col">$\boldsymbol{x}'$
								</div>
								<div class="col">$\boldsymbol{x}$
								</div>
								<div class="col">$\boldsymbol{x}'- \boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
								</div>
								<div class="col">$\boldsymbol{r}^\star(\boldsymbol{x}', \sigma)$
								</div>
							</div>
							<img data-src="/talks/assets/denoised_mnist.png" style='width:1200px;'></img>
						</div> -->
					</section>
					</section>

<section>
	<h3 class="slide-title">Example of Annealed Hamiltonian Monte-Carlo Sampling</h3>
	<img data-src="/talks/assets/annealing_scheme.gif" style="margin-top: -20px"/>
	<div style="margin-top: -50px;">$$\nabla_x \log q_{\sigma^2}(x |y) = \underbrace{\nabla_x \log p_{\sigma^2}(y |x)}_{\mbox{known explicitly}} \quad + \quad \underbrace{s_
		{\theta}(x, \sigma^2)}_{\mbox{learned by score matching}}$$</div>
</section>

						<section>
							<section>
								<h3 class="slide-title">Illustration on $\kappa$-TNG simulations</h3>
								<div class="container">
									<div class="col">
											<div style="float:right; font-size: 20px"> Remy, Lanusse, et al. (2023) <a href="https://arxiv.org/abs/2201.05561"><img src="https://img.shields.io/badge/astro--ph.CO-arXiv%3A2201.05561-B31B1B.svg" class="plain" style="height:25px;vertical-align:middle;" /></a></div>
									</div>
								</div>
								<div class="container">
									<div class="col">
										<img data-src='/talks/assets/ref_ktng.png' style="width:350px; height:350px;" />
										<br>
										True convergence map
									</div>
									<div class="col">
										<div class="block-content">
											<div style="position:relative; width:350px; height:350px; top:10px; left:40px;">
												<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="0">
													<img data-src='/talks/assets/ks_ktng.png' style="width:350px; height:350px;" />
												</div>
												<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="1">
													<img data-src='/talks/assets/wiener_ktng.png' style="width:350px; height:350px;" />
												</div>
												<div class="plain fragment" style="position:absolute;top:0;left:0;width:350px;" data-fragment-index="2">
													<img data-src='/talks/assets/mean_post_ktng.png' style="width:350px; height:350px;" />
												</div>
											</div>
											<div class="block-content">
												<div style="position:relative; width:350px; height:20px; top:50px; left:10px;">
													<div class="fragment current-visible " data-fragment-index="0" style="position:absolute;top:0;left:0;width:350px;">Traditional Kaiser-Squires</div>
													<div class="fragment current-visible " data-fragment-index="1" style="position:absolute;top:0;left:0;width:350px;">Wiener Filter</div>
													<div class="fragment" data-fragment-index="2" style="position:absolute;top:0;left:0;width:350px;">Posterior Mean (ours)</div>
												</div>
											</div>
											<br>
											<br>
										</div>
			
									</div>
									<div class="col fragment">
										<img data-src='/talks/assets/cropped.gif' style="width:350px; height:350px;" />
										<br>
										Posterior samples
									</div>
								</div>
		
							</section>

							<section>
								<h3 class="slide-title">Reconstruction of the <b class="alert">HST/ACS COSMOS field</b></h3>
			
								<ul>
								<li> COSMOS shear data from <a href=https://ui.adsabs.harvard.edu/abs/2010A%26A...516A..63S/abstract>Schrabback et al. 2010</a>
								</li>
								<li> Prior learned from $\kappa$-TNG simulation from <a href=https://arxiv.org/abs/2010.09731>Osato et al. 2021</a>.
								</li>
							</ul>
								<br>
								<div class="container">
									<div class="col">
										<div class="block-content">
											<div style="position:relative; height:570px; top:0px; left:0px;">
												Massey et al. (2007)
												<img data-src="/talks/assets/massey.png" style="height:500px;"></img>
											</div>
										</div>
									</div>
			
									<div class="col">
										<div class="block-content">
											<div style="position:relative; height:570px; top:0px; left:0px;">
												<div class="plain fragment current-visible " style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="0">
													Remy et al. (2023) <b class="alert">Posterior mean</b>
													<img data-src='/talks/assets/remy.png' style="height:500px;" />
												</div>
			
												<div class="plain fragment" style="position:absolute;top:0;left:0;width:600px;" data-fragment-index="1">
													Remy et al. (2023) <b class="alert">Posterior samples</b>
													<img data-src='/talks/assets/cosmos_samples.gif' style="height:500px;" />
												</div>
			
											</div>
										</div>
									</div>
			
								</div>
							</section>
						</section>
			
						<section>
							<h3 class="slide-title">Other Example of Inverse Problem under Implicit priors: MRI</h3>
							<div style="float:right; font-size: 20px">Ramzi, Remy, <b>Lanusse</b> et al. 2020 <a href="https://arxiv.org/abs/2011.08698" style='vertical-align:middle; display:inline;'><img
										src="https://img.shields.io/badge/stat.ML-arXiv%3A2011.08698-B31B1B.svg" class="plain" style="height:25px;" /></a>
							</div>
							<br>
							<br>
							$$\boxed{y = \mathbf{M} \mathbf{F} x + n}$$
							<div><video data-autoplay loop="loop" data-src="/talks/assets/knee.mp4" type="video/mp4" style="width: 1280px;" />
							</div>
							<br>
		
							<br>
		
							<br>
		
							<p class="fragment">$\Longrightarrow$ We can see which parts of the image are well constrained by data, and which regions are <b class="alert">uncertain</b>.</p>
						</section>

						<section>
							<h3 class="slide-title">takeaways</h3>
							<br>
							<br>
			
							<div class="block ">
								<div class="block-title">
									Benefits of Bayesian forward modeling for inverse problems
								</div>
								<div class="block-content">
									Using a Bayesian approach has great advantages: <b>model-based physical interpretation & uncertainty quantification</b>.
									<br>
									<br>
									<ul>
										<li class="fragment"> <b class="alert">Explicit likelihood</b>, uses of all of our physical knowledge.<br>
											$\Longrightarrow$ The method can be applied for varying noise, observing conditions, or different instruments
										</li>
										<br>
										<li class="fragment"> Deep generative models can be used to provide <b class="alert">simulation or data driven priors</b>.<br>
											$\Longrightarrow$ Embed prior only accessible from samples (e.g. numerical simulations or data).
										</li>
									</ul>
								</div>
							</div>
			
							<br>
							<br>
							<br>
						</section>

						<section>
							<h2>AstroCLIP: Cross-Modal Pretraining for <br> Astronomical Foundation Models</h2>
								<a href="https://arxiv.org/abs/2310.03024"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2310.03024-B31B1B.svg" class="plain" style="height:25px;" /></a>
							<hr>
							<div class="container">
								<div class="col">
									<div align="left" style="margin-left: 20px;">
										<div style="text-align: center;">
											<img data-src="/talks/assets/polymathic_logo.png" style="height: 100px;"><br>
											<a href="https://polymathic-ai.org" style="color: #cc0033;">polymathic-ai.org</a>
										</div>
										<br>
										<br>
										$\Longrightarrow$ Build <b class="alert">informative embeddings of multimodal data</b> in a purely self-supervised way, which can be leveraged for diverse downstream tasks.
									</div>
								</div>
								<div class="col">
									<img class="plain" data-src="/talks/assets/astroclip1.png" style="width:450px;" />
								</div>
							</div>
							<br>
						</section> 

						<section  data-background-image="/talks/assets/gal_hsc.png">
							<h3 class="slide-title">The Deep Learning Boom in Astrophysics</h3>
				  
							<div class="r-stack">
							<div class="fragment">
							<div class="container">
							  <canvas data-chart="bar" style="height: 600px;">
								<!--
										  {
										   "data": {
											  "labels": ["2012", "2013", "2014","2015", "2016" ,"2017", "2018", "2019", "2020", "2021", "2022", "2023"],
											  "datasets": [
											   {
												  "data":[ 21, 15, 17, 22, 31, 71, 113, 231, 322, 422, 537, 646 ],
												  "label":"Deep Learning || CNN || Neural Network ","backgroundColor":"#A63446"
											  }
											  ]
										   },
										   "options": { "responsive": "true",
									  "scales": {
											  "yAxes": [{
													  "type": "linear"
											  }]
									  }
										  }
										  }
										  --> </canvas
							  ><br/>
							  </div>
							  <div>
								<b>astro-ph</b> abstracts mentioning <b>Deep Learning</b>,
								<b>CNN</b>, or <b>Neural Networks</b>
							  </div>
							</div>
							<div class="block fragment" >
								<div class="block-title">
									Limits of traditional Deep Learning in Astrophysics
								</div>
								<div class="block-content">
									The <b>vast majority</b> of these results has relied on <b>supervised learning</b> and <b>networks trained from scratch</b>.
									<br>
									<br>
									<ul>
										<li class="fragment"> <b class="alert">Limited supervised training data</b><br>
											$\Longrightarrow$ Rare or novel objects have by definition few examples.
										</li>
										<br>
										<li class="fragment"><b class="alert"> Limited reusability</b><br>
											$\Longrightarrow$ Existing models are trained supervised on a specific task and specific data.
										</li>
									</ul>
								</div>
							</div>
							</div>
						  </section>
							</section>

							<section>
							<section>
								<h3 class="slide-title">The Promise of The Foundation Model Paradigm</h3>
								<br>
								<div class="container">
								<div class="col">
									<div class="sl-block" data-block-type="text" style="width: 640px; left: 0px; top: 84.0065px; height: auto;" data-block-id="492b3f915d09822a197a5b80ce451809">
										<div class="sl-block-content" data-placeholder-tag="p" data-placeholder-text="Text" style="z-index: 11;">
											<ul>
												<li>
													<strong>Foundation Model approach</strong>
													<ul>
														<li>
															<strong class="alert">Pretrain</strong>&nbsp;models on pretext tasks, without supervision, on very large scale datasets.</li>
															<br>
														<li class="fragment" data-fragment-index="1">
															<strong class="alert">Adapt</strong>&nbsp;pretrained models to downstream tasks.&nbsp;</li>
															<br>
														<li class="fragment" data-fragment-index="2">
															<strong class="alert">Combine</strong>&nbsp;pretrained modules in more complex systems.</li>
													</ul>
												</li>

												<br>

												<li class="fragment" data-fragment-index="3">This approach benefits from <b>scaling of data and compute</b>.</li>

												<br>

												<li class="fragment" data-fragment-index="4">Effective <b>self-supervised representation learning</b>.
													<br>Can be leveraged with simple <b>linear models</b> for downstream tasks: <b>zero and few-shot learning</b></li> 
											</ul>
										</div>
									</div>				
								</div>
								<div class="col">
									<div class="r-stack">
									<div class="fragment fade-out" data-fragment-index="3">
										<div class="sl-block" data-block-type="image" style="width: 640px; height: 456.013px; left: 640px; top: 84.0065px; min-width: 1px; min-height: 1px;" data-name="image-c5ccee" data-block-id="96bbfd152ee5d8305589b472d9775516">
											<div class="sl-block-content" style="z-index: 12;"><img class="" data-natural-width="1214" data-natural-height="865" data-lazy-loaded="" src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/10886697/pasted-from-clipboard.png"></div>
										</div>
										<div class="sl-block" data-block-type="text" style="height: auto; width: 300px; left: 980px; top: 576px;" data-name="text-2a10e5" data-block-id="cb4de175c77e31210c4d2f3a0ed64477">
											<div class="sl-block-content" data-placeholder-tag="p" data-placeholder-text="Text" style="z-index: 13;">
												<p><span style="font-size:0.9em"><a href="https://arxiv.org/abs/2108.07258" target="_blank">Bommasani et al. 2021</a></span></p>
											</div>
										</div>
									</div>

									<div class="fragment current-visible" data-fragment-index="3">
										<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/11185011/pasted-from-clipboard.png" style="height: 400px;"></img>
										<p><span style="font-size:0.9em"><a href="https://arxiv.org/abs/2201.03545" target="_blank">Liu et al. 2022</a></span></p>

									</div>

									<div class="fragment" data-fragment-index="4">
										<img data-src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/11185470/pasted-from-clipboard.png" style="height: 400px;"></img>
										<p><span style="font-size:0.9em"><a href="https://arxiv.org/abs/2106.04560" target="_blank">Zhai et al. 2022</a></span></p>
									</div>
								</div>
								</div>	
								<br>
							</section>
						</section>

						<section>
							<h3 class="slide-title">The data diversity challenge</h3>

							<div class="container">
								<div class="col fragment" data-fragment-index="0">
									<img data-src="/talks/assets/42254_2021_353_Fig1_HTML.webp" />
									<p style="text-align: left;"><span style="font-size:0.5em">Credit: <a href="https://www.nature.com/articles/s42254-021-00353-y" target="_blank">Melchior et al. 2021</a></span></p>
								</div>
								
								<div class="col fragment" data-fragment-index="1">
									<img  src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/10879711/pasted-from-clipboard.png"/>
									<p style="text-align:left"><span style="font-size:0.5em">Credit:DESI collaboration/DESI Legacy Imaging Surveys/LBNL/DOE &amp; KPNO/CTIO/NOIRLab/NSF/AURA/unWISE</span></p>
								</div>
							</div>

							<ul>
								<li >Success of recent foundation models is driven by large corpora of uniform data (e.g <a href="https://laion.ai/blog/laion-5b/" target="_blank">LAION 5B</a>).&nbsp;</li>
								<li class="fragment" data-fragment-index="0"><b class="alert">Scientific data comes with many additional challenges</b>:
									<ul>
										<li class="fragment" data-fragment-index="0">Metadata matters</li>
										<li class="fragment" data-fragment-index="1">Wide variety of measurements/observations</li>
									</ul>
								</li>
							</ul>
							<br>
							<br>
							<div class="fragment">
								$\Longrightarrow$ Building models that can handle this diversity is paramount for scientific foundation models.
							</div>
						</section>

						<section>
						<section>
							<h3 class="slide-title"><b>AstroCLIP</b>: Multimodal Pretraining for Astronomical Foundation Models</h3>
							<div class="container">
								<div class="col"></div>
								<div class="col">
									<a href="https://arxiv.org/abs/2310.03024"><img src="https://img.shields.io/badge/astro--ph.IM-arXiv%3A2310.03024-B31B1B.svg" class="plain" style="height:25px;" /></a>
								</div>
							</div>


							<div class="container">
								<div class="col">
									<img data-src="/talks/assets/astroclip1.png" style="width:450px;" />

									<img data-src="/talks/assets/clip_loss.png" />
									
								</div>

								<div class="col">
									<ul>
										<li> 
											<p>We adapt the Contrastive Language Image Pretraining (CLIP) strategy of 
												<a href="https://arxiv.org/abs/2103.00020" target="_blank">Radford et al. 2021</a></p>
										</li>
										
										<li>We use <b>spectra</b> and multi-band <b>images</b> as two <b class="alert">different views for the same underlying physical object</b>.</li>
									</ul>

									<div class="container">
										<div class="col fragment">
											<img data-src="/talks/assets/similarity_search.png"/>
											<br> <div style="font-size: medium;">Similarity-Based Retrieval</div>
										</div>
										
										<div class="col fragment">
											<img data-src="/talks/assets/zero_shot.png" />
										</div>
									</div>
								</div>
							</div>
						</section>

					</section>

						<section>
							<h3 class="slide-title">Connecting Embeddings to Physical Parameters by Few-Shot Learning</h3>

							<div class="container">

								<div class="col">
									<img data-src="/talks/assets/mlp_probing.png" style="height: 210px;"/>
									<img data-src="/talks/assets/supervised_baseline.png" style="height: 210px;"/>
									<div class="fragment">
									<img data-src="/talks/assets/table_predictions.png" />
									<br> <div style="font-size: medium;">$R^2$ of regression of physical galaxy properties</div>
									</div>
								</div>

								<div class="col">
									<ul>
										<li>AstroCLIP embeddings are vectors $z \in \mathbb{R}^{256}$
											<br>They possess a <b>structure that aligns with physical properties</b>, but <b>not directly physically interpretable</b>.</li>

										<br>
										<li>To perform downstream inference on physical parameters $\theta$ we build a small model $p(\theta | z )$.</li>
 									</ul>
									<div class="fragment">
									 <img data-src="/talks/assets/astroclip_classification.png" style="height: 400px;" />
									 <br> <div style="font-size: medium;">Classification accuracy on Galaxy morphology classification</div>

									 </div>
								</div>
							</div>
						</section>


						<section>
							<h3 class="slide-title">takeaways</h3>
							<div class="block" >
								<div class="block-title">
									What this new paradigm will mean for us astrophysicists
								</div>
								<div class="block-content">
									<ul>
										<li>
											<strong >Never have to retrain my own neural networks</strong>&nbsp;from scratch
											<ul>
												<li><span style="font-size:0.9em">Existing pre-trained models would already be near optimal, no matter the task at hand</span></li>
									
											</ul>
										</li>
										<br>
										<li class="fragment" data-fragment-index="0">Practical large scale Deep Learning even in <strong>very few example regime</strong>
											<ul>
												<li class="visible"><span style="font-size:0.9em">Searching for very rare objects in large surveys becomes possible</span></li>
							
											</ul>
										</li>
										<br>
										<li class="fragment" data-fragment-index="1">If the information is embedded in a space where it becomes linearly accessible, &nbsp;<strong>very simple analysis tools are enough </strong>for downstream analysis
											<ul>
												<li><span style="font-size:0.9em">In the future, survey pipelines may add vector embedding of detected objects into catalogs, these would be enough for most tasks, without the need to go back to pixels</span></li>
											</ul>
										</li>
									</ul>
								</div>
							</div>
						</section>

		<section data-background="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/10879675/bkg1.jpg">
			<div class="container">
				<div class="col">
				<img data-src="/talks/assets/polymathic_logo.png" style="width:250px;  top: 0; left: 0;"/>
				</div>
				<div class="col">
				</div>
				<div class="col">
				</div>
				</div>
				<br>
				<br>
				<br>
				<br>
				<div class="container">
					<div class="col" style="text-align: center">
						<div class="sl-block" data-block-type="shape" style="display: block; margin: auto; width:140px;" data-name="shape-903240" data-block-id="b4b819af01b2a5019b4b9dac066b1b0b"><a class="sl-block-content" data-shape-type="symbol-twitter" data-shape-fill-color="rgb(255, 255, 255)" data-shape-stretch="true" style="z-index: 12;" href="https://twitter.com/PolymathicAI" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="100%" height="100%" preserveAspectRatio="none" viewBox="-0.0010004043579101562 2.998000144958496 32.000999450683594 26.005001068115234">
							<g class="shape-element" fill="rgb(255, 255, 255)" data-viewbox="-0.0010004043579101562 2.998000144958496 32.000999450683594 26.005001068115234">
								<path d="M32 6.076c-1.177 0.522-2.443 0.875-3.771 1.034 1.355-0.813 2.396-2.099 2.887-3.632-1.269 0.752-2.674 1.299-4.169 1.593-1.198-1.276-2.904-2.073-4.792-2.073-3.626 0-6.565 2.939-6.565 6.565 0 0.515 0.058 1.016 0.17 1.496-5.456-0.274-10.294-2.888-13.532-6.86-0.565 0.97-0.889 2.097-0.889 3.301 0 2.278 1.159 4.287 2.921 5.465-1.076-0.034-2.088-0.329-2.974-0.821-0.001 0.027-0.001 0.055-0.001 0.083 0 3.181 2.263 5.834 5.266 6.437-0.551 0.15-1.131 0.23-1.73 0.23-0.423 0-0.834-0.041-1.235-0.118 0.835 2.608 3.26 4.506 6.133 4.559-2.247 1.761-5.078 2.81-8.154 2.81-0.53 0-1.052-0.031-1.566-0.092 2.905 1.863 6.356 2.95 10.064 2.95 12.076 0 18.679-10.004 18.679-18.68 0-0.285-0.006-0.568-0.019-0.849 1.283-0.926 2.396-2.082 3.276-3.398z"></path>
							</g>
						</svg></a></div>
						<a href="https://twitter.com/PolymathicAI" target="_blank">Follow us on Twitter</a>
					</div>

					<div class="col">

				<div class="sl-block" data-block-type="shape" style="display: block; margin: auto; width:140px;" data-name="shape-d3f0e3" data-block-id="9a757c824505e98de019f80ccc356f74"><a class="sl-block-content" data-shape-type="symbol-earth" data-shape-fill-color="rgb(255, 255, 255)" data-shape-stretch="true" style="z-index: 13;" href="https://polymathic-ai.org/" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" version="1.1" width="100%" height="100%" preserveAspectRatio="none" viewBox="-9.5367431640625e-7 -9.5367431640625e-7 32 32">
					<g class="shape-element" fill="rgb(255, 255, 255)" data-viewbox="-9.5367431640625e-7 -9.5367431640625e-7 32 32">
						<path d="M27.314 4.686c3.022 3.022 4.686 7.040 4.686 11.314s-1.664 8.292-4.686 11.314c-3.022 3.022-7.040 4.686-11.314 4.686s-8.292-1.664-11.314-4.686c-3.022-3.022-4.686-7.040-4.686-11.314s1.664-8.292 4.686-11.314c3.022-3.022 7.040-4.686 11.314-4.686s8.292 1.664 11.314 4.686zM25.899 25.9c1.971-1.971 3.281-4.425 3.821-7.096-0.421 0.62-0.824 0.85-1.073-0.538-0.257-2.262-2.335-0.817-3.641-1.621-1.375 0.927-4.466-1.802-3.941 1.276 0.81 1.388 4.375-1.858 2.598 1.079-1.134 2.050-4.145 6.592-3.753 8.946 0.049 3.43-3.504 0.715-4.729-0.422-0.824-2.279-0.281-6.262-2.434-7.378-2.338-0.102-4.344-0.314-5.25-2.927-0.545-1.87 0.58-4.653 2.584-5.083 2.933-1.843 3.98 2.158 6.731 2.232 0.854-0.894 3.182-1.178 3.375-2.18-1.805-0.318 2.29-1.517-0.173-2.199-1.358 0.16-2.234 1.409-1.512 2.467-2.632 0.614-2.717-3.809-5.247-2.414-0.064 2.206-4.132 0.715-1.407 0.268 0.936-0.409-1.527-1.594-0.196-1.379 0.654-0.036 2.854-0.807 2.259-1.325 1.225-0.761 2.255 1.822 3.454-0.059 0.866-1.446-0.363-1.713-1.448-0.98-0.612-0.685 1.080-2.165 2.573-2.804 0.497-0.213 0.973-0.329 1.336-0.296 0.752 0.868 2.142 1.019 2.215-0.104-1.862-0.892-3.915-1.363-6.040-1.363-3.051 0-5.952 0.969-8.353 2.762 0.645 0.296 1.012 0.664 0.39 1.134-0.483 1.439-2.443 3.371-4.163 3.098-0.893 1.54-1.482 3.238-1.733 5.017 1.441 0.477 1.773 1.42 1.464 1.736-0.734 0.64-1.185 1.548-1.418 2.541 0.469 2.87 1.818 5.515 3.915 7.612 2.644 2.644 6.16 4.1 9.899 4.1s7.255-1.456 9.899-4.1z"></path>
					</g>
				</svg></a></div>
				<a href="https://polymathic-ai.org/" target="_blank">Visit our website</a>
					</div>

					<div class="col">
						<div class="sl-block" data-block-type="image" style="display: block; margin: auto; width:120px;" data-name="image-3e0831" data-block-id="3c5ad01b9eefbfc046345bdf5bb7b699"><a class="sl-block-content" style="z-index: 14;" href="https://github.com/PolymathicAI" target="_blank"><img class="" data-natural-width="512" data-natural-height="512" data-lazy-loaded="" src="https://s3.amazonaws.com/media-p.slid.es/uploads/866922/images/10887186/github-pages-logo-repository-fork-github-86eddab19cbc3ae293ada0fe0fb9e27d.png"></a></div>
						<a href="https://github.com/PolymathicAI" target="_blank">Check out our GitHub</a>
					</div>
				</div>
				<br>
				<br>
				<div style="width: 200px; display: block; margin: auto; "><hr></div>
				<br>
				<br>
				<h2 class="fragment">Thank you!</h2>
		</section>


		</div>
	</div>

	<style>
		/* .reveal .slides {
			border: 5px solid red;
			min-height: 100%;
			width: 128mm;
			height: 96mm;
		}  */

		.reveal .block {
			background-color: #191919;
			margin-left: 20px;
			margin-right: 20px;
			text-align: left;
			padding-bottom: 0.1em;
		}

		.reveal .block-title {
			background-color: #333333;
			padding: 8px 35px 8px 14px;
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .block-content {
			padding: 8px 35px 8px 14px;
		}

		.reveal .slide-title {
			border-left: 5px solid white;
			text-align: left;
			margin-left: 20px;
			padding-left: 20px;
		}

		.reveal .alert {
			color: #FFAA7F;
			font-weight: bold;
		}

		.reveal .inverted {
			filter: invert(100%);
		}

		/*
	/* .reveal .alert {
	padding:8px 35px 8px 14px; margin-bottom:18px;
	text-shadow:0 1px 0 rgba(255,255,255,1);
	border:5px solid #FFAA7F;
	-webkit-border-radius: 14px; -moz-border-radius: 14px;
	border-radius:14px
	background-position: 10px 10px;
	background-repeat: no-repeat;
	background-size: 38px;
	padding-left: 30px; /* 55px; if icon
	}
	.reveal .alert-block {padding-top:14px; padding-bottom:14px}
	.reveal .alert-block > p, .alert-block > ul {margin-bottom:1em}
	/*.reveal .alert li {margin-top: 1em}
	.reveal .alert-block p+p {margin-top:5px} */
	</style>


	<script src="reveal.js/dist/reveal.js"></script>
	<script src="reveal.js/plugin/notes/notes.js"></script>
	<script src="reveal.js/plugin/markdown/markdown.js"></script>
	<script src="reveal.js/plugin/highlight/highlight.js"></script>
	<script src="reveal.js/plugin/math/math.js"></script>
	<script>
		// More info about initialization & config:
		// - https://revealjs.com/initialization/
		// - https://revealjs.com/config/
		Reveal.initialize({
			controls: true,

			//center: false,
			hash: true,

			// Visibility rule for backwards navigation arrows; "faded", "hidden"
			// or "visible"
			controlsBackArrows: 'hidden',

			// Display a presentation progress bar
			progress: true,

			// Display the page number of the current slide
			slideNumber: true,

			transition: 'slide', // none/fade/slide/convex/concave/zoom

			// The "normal" size of the presentation, aspect ratio will be preserved
			// when the presentation is scaled to fit different resolutions. Can be
			// specified using percentage units.
			width: 1280,
			height: 720,

			// Factor of the display size that should remain empty around the content
			margin: 0.1,

			// Bounds for smallest/largest possible scale to apply to content
			minScale: 0.2,
			maxScale: 1.5,

			autoPlayMedia: true,

			// Learn about plugins: https://revealjs.com/plugins/
			plugins: [RevealMarkdown, RevealHighlight, RevealNotes, RevealMath],

			dependencies: [{
					src: 'reveal.js/plugin/markdown/marked.js'
				},
				{
					src: 'reveal.js/plugin/markdown/markdown.js'
				},
				{
					src: 'reveal.js/plugin/notes/notes.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/math/math.js',
					async: true
				},
				{
					src: 'reveal.js/plugin/reveal.js-d3/reveald3.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/Chart.min.js'
				},
				{
					src: 'reveal.js/plugin/reveal.js-plugins/chart/csv2chart.js'
				},
				{
					src: 'reveal.js/plugin/highlight/highlight.js',
					async: true
				},
			]

		});

		// utility function that excepts a fragmentshown or fragmenthidden event and returns a boolean indicating whether or not
		// the fragment is a video
		const isVideoFragment = (event) => event.fragment.nodeName === 'VIDEO';

		// Listens for the 'fragmentshown' event; if the fragment being shown is a video, play the video
		Reveal.addEventListener('fragmentshown', (event) => {
		if (isVideoFragment(event)) {
			event.fragment.play();
		}
		});

		// Listens for the 'fragmenthidden' event; if the fragment being hidden is a video, pause the video
		Reveal.addEventListener('fragmenthidden', (event) => {
		if (isVideoFragment(event)) {
			event.fragment.pause();
		}
		});
	</script>
</body>

</html>
